{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math, time\n",
    "import sys\n",
    "\n",
    "sys.path.append('/home/boyuan/Faith-NNVerificationCompiler/')\n",
    "\n",
    "epsilon = 1e-12\n",
    "\n",
    "from HandTunedKernels.kernel_test.forward_test_bound import Bounds\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size=1;length=2;dim_in=1024;dim_out=1024;dim_y_out=1024\n",
    "\n",
    "import tvm\n",
    "import tvm.testing\n",
    "from tvm import te\n",
    "import numpy\n",
    "import timeit\n",
    "dtype=\"float32\"\n",
    "target=\"llvm\"\n",
    "dev = tvm.device(target, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mamtul_template(batch_size, length, dim_in, dim_out, dim_y_out):\n",
    "    ibound = te.placeholder((batch_size, length, dim_in*2+2, dim_out), name='ibound')\n",
    "    w = te.placeholder((dim_out, dim_y_out), name='w')\n",
    "\n",
    "    w_pos = te.compute(\n",
    "        w.shape, \n",
    "        lambda i,j: te.if_then_else(w[i,j]>0, w[i,j], 0.),\n",
    "        name='w_pos'\n",
    "    )\n",
    "    w_neg = te.compute(w.shape, lambda i,j: w[i,j]-w_pos[i,j], name='w_neg')\n",
    "\n",
    "    w_concate = te.compute(\n",
    "        [dim_out*2, dim_y_out], \n",
    "        lambda dout, dout_y: \n",
    "        te.if_then_else(\n",
    "            dout < dim_out, \n",
    "            w_pos[dout, dout_y],\n",
    "            w_neg[dout-dim_out, dout_y]\n",
    "        ),\n",
    "        name='w_concate'\n",
    "    )\n",
    "\n",
    "    ibound_concate = te.compute(\n",
    "        [batch_size, length, dim_in, dim_out*2],\n",
    "        lambda b, l, din, dout:\n",
    "        te.if_then_else(\n",
    "            dout < dim_out,\n",
    "            ibound[b, l, 2+din, dout],\n",
    "            ibound[b, l, 2+dim_in+din, dout]\n",
    "        ),\n",
    "        name='ibound_concate'\n",
    "    )\n",
    "\n",
    "    dout = te.reduce_axis((0, dim_out*2), \"dout\")\n",
    "    obound_lw = te.compute(\n",
    "        [batch_size, length, dim_in, dim_y_out],\n",
    "        lambda b, l, din, dout_y:\n",
    "        te.sum(ibound_concate[b, l, din, dout] * w_concate[dout, dout_y], axis=dout),\n",
    "        name='obound_lw'\n",
    "    )\n",
    "\n",
    "\n",
    "    s = te.create_schedule([obound_lw.op])\n",
    "    func = tvm.build(s, [ibound, w, obound_lw], target=target, name=\"certified_matmul\")\n",
    "\n",
    "    input_bound_raw = numpy.random.rand(batch_size, length, 2+2*dim_in, dim_out).astype(dtype) \n",
    "    input_bound_tvm = tvm.nd.array(input_bound_raw, dev)\n",
    "\n",
    "    input_w_raw = numpy.random.rand(dim_out, dim_y_out).astype(dtype)\n",
    "    input_w_tvm = tvm.nd.array(input_w_raw, dev)\n",
    "\n",
    "    output_bound_lw_raw = numpy.zeros((batch_size, length, dim_in, dim_y_out)).astype(dtype)\n",
    "    output_bound_lw_tvm = tvm.nd.array(output_bound_lw_raw, dev)\n",
    "\n",
    "    func(input_bound_tvm, input_w_tvm, output_bound_lw_tvm)\n",
    "    evaluator = func.time_evaluator(func.entry_name, dev, number=1)\n",
    "    baseline_latency = evaluator(input_bound_tvm, input_w_tvm, output_bound_lw_tvm).mean\n",
    "\n",
    "    bn = 32\n",
    "    kfactor = 4\n",
    "    s = te.create_schedule(obound_lw.op)\n",
    "\n",
    "    # Blocking by loop tiling\n",
    "    mo, no, mi, ni = s[obound_lw].tile(obound_lw.op.axis[2], obound_lw.op.axis[3], bn, bn)\n",
    "    (kaxis,) = s[obound_lw].op.reduce_axis\n",
    "    ko, ki = s[obound_lw].split(kaxis, factor=kfactor)\n",
    "\n",
    "    # Host reduction domain outside the blocking loop\n",
    "    s[obound_lw].reorder(mo, no, ko, ki, mi, ni)\n",
    "\n",
    "    func = tvm.build(s, [ibound, w, obound_lw], target=target, name=\"certified_matmul\")\n",
    "    func(input_bound_tvm, input_w_tvm, output_bound_lw_tvm)\n",
    "    evaluator = func.time_evaluator(func.entry_name, dev, number=1)\n",
    "    loop_tiling_latency = evaluator(input_bound_tvm, input_w_tvm, output_bound_lw_tvm).mean\n",
    "    \n",
    "    print(\"%d\\t%d\\t%d\\t%f\\t%f\" % (batch_size, length, dim_in, baseline_latency, loop_tiling_latency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch\tlength\tdim_in\ttemplate(raw)\ttemplate(tuned)\n",
      "1\t2\t64\t1.307692\t0.073274\n",
      "1\t2\t128\t2.568111\t0.161789\n",
      "1\t2\t256\t5.304649\t0.320858\n",
      "1\t2\t512\t10.823001\t0.692673\n",
      "1\t2\t1024\t20.684462\t1.112095\n",
      "1\t4\t64\t2.520366\t0.161275\n",
      "1\t4\t128\t5.043037\t0.319600\n",
      "1\t4\t256\t10.066399\t0.635296\n",
      "1\t4\t512\t20.691795\t1.275573\n",
      "1\t4\t1024\t56.506438\t2.609836\n",
      "1\t8\t64\t6.823598\t0.564768\n",
      "1\t8\t128\t12.274440\t1.162039\n",
      "1\t8\t256\t26.274846\t1.431888\n",
      "1\t16\t64\t13.702192\t0.669615\n",
      "1\t16\t128\t28.436459\t1.431849\n",
      "1\t16\t256\t41.498004\t2.229562\n",
      "1\t32\t64\t20.670114\t1.100166\n",
      "1\t32\t128\t41.519295\t2.223166\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "print(\"batch\\tlength\\tdim_in\\ttemplate(raw)\\ttemplate(tuned)\")\n",
    "for length in [2,4,8,16,32,64,128]:\n",
    "    for dim_in in [64,128,256,512,1024]: # matmul\n",
    "        if length >=8 and dim_in >=512:\n",
    "            pass\n",
    "        elif length >=64 and dim_in >=128:\n",
    "            pass\n",
    "        else:\n",
    "            mamtul_template(batch_size, length, dim_in, dim_out, dim_y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ Below are expired ==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = torch.rand(1,length,dim_out).to(device)\n",
    "ub = lb + torch.rand(1,length,dim_out).to(device)\n",
    "lw = torch.rand(1,length,dim_in,dim_out).to(device) - 0.5\n",
    "uw = torch.rand(1,length,dim_in,dim_out).to(device) - 0.5\n",
    "bound = Bounds(p=2,eps=0.5,lw=lw,lb=lb,uw=uw,ub=ub)\n",
    "W = torch.rand(dim_y_out, dim_out).to(device) - 0.5\n",
    "l, u = bound.concretize()\n",
    "bound_output = bound.matmul(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_bound_lb_raw = numpy.random.rand(batch_size, length, dim_out).astype(dtype)\n",
    "input_bound_lb_tvm = tvm.nd.array(input_bound_lb_raw, dev)\n",
    "\n",
    "input_bound_ub_raw = input_bound_lb_raw + numpy.random.rand(batch_size, length, dim_out).astype(dtype),\n",
    "input_bound_ub_tvm = tvm.nd.array(input_bound_ub_raw, dev)\n",
    "\n",
    "input_bound_lw_raw = numpy.random.rand(batch_size, length, dim_in, dim_out).astype(dtype) - 0.5\n",
    "input_bound_lw_tvm = tvm.nd.array(input_bound_lw_raw, dev)\n",
    "\n",
    "input_bound_uw_raw = numpy.random.rand(batch_size, length, dim_in, dim_out).astype(dtype) - 0.5\n",
    "input_bound_uw_tvm = tvm.nd.array(input_bound_uw_raw, dev)\n",
    "\n",
    "input_w_raw = numpy.random.rand(dim_y_out, dim_out).astype(dtype)\n",
    "input_w_tvm = tvm.nd.array(input_w_raw, dev)\n",
    "\n",
    "pos_mask_raw = numpy.zeros((dim_y_out, dim_out)).astype(dtype)\n",
    "pos_mask_tvm = tvm.nd.array(pos_mask_raw, dev)\n",
    "\n",
    "w_pos_raw = numpy.zeros((dim_y_out, dim_out)).astype(dtype)\n",
    "w_pos_tvm =  tvm.nd.array(w_pos_raw, dev)\n",
    "\n",
    "output_bound_lw_raw = numpy.random.rand(batch_size, length, dim_in, dim_y_out).astype(dtype) - 0.5\n",
    "output_bound_lw_tvm = tvm.nd.array(output_bound_lw_raw, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 64, 64]\n",
      "[1, 2, 64, 64] [1, 2, 64, 64] [64, 64] [1, 2, 64, 64]\n"
     ]
    },
    {
     "ename": "TVMError",
     "evalue": "Traceback (most recent call last):\n  6: TVMFuncCall\n  5: std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)>::AssignTypedLambda<tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}>(tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)\n  4: tvm::LowerSchedule(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<tvm::te::Tensor, tvm::tir::Buffer, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::tir::Buffer> > > const&, bool)\n  3: tvm::ScheduleToModule(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<tvm::te::Tensor, tvm::tir::Buffer, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::tir::Buffer> > > const&)\n  2: tvm::te::InferBound(tvm::te::Schedule const&)\n  1: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)\n  0: tvm::te::BaseComputeOpNode::GatherBound(tvm::te::Operation const&, std::unordered_map<tvm::te::Tensor, tvm::te::TensorDom, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::te::TensorDom> > > const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*) const\n  File \"/home/tianqi_tang/tvm/src/te/operation/compute_op.cc\", line 256\nTVMError: \n---------------------------------------------------------------\nAn error occurred during the execution of TVM.\nFor more information, please see: https://tvm.apache.org/docs/errors.html\n---------------------------------------------------------------\n  Check failed: (!out_dom_map->count(this->reduce_axis[i])) is false: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7da0ca7666e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# print(tvm.lower(s, [ibound_lw,ibound_uw, res, w], simple_mode=True))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mibound_lw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mibound_uw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"certified_matmul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/tvm/python/tvm/driver/build_module.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(inputs, args, target, target_host, name, binds)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"args must be given for build from schedule\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0minput_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mmerged_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIRModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tvm/python/tvm/driver/build_module.py\u001b[0m in \u001b[0;36mlower\u001b[0;34m(inp, args, name, binds, simple_mode)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower_primfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimple_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSchedule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower_schedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimple_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected input to be an IRModule, PrimFunc or Schedule, but got, \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tvm/python/tvm/_ffi/_ctypes/packed_func.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         ):\n\u001b[0;32m--> 237\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mget_last_ffi_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  6: TVMFuncCall\n  5: std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)>::AssignTypedLambda<tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}>(tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)\n  4: tvm::LowerSchedule(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<tvm::te::Tensor, tvm::tir::Buffer, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::tir::Buffer> > > const&, bool)\n  3: tvm::ScheduleToModule(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<tvm::te::Tensor, tvm::tir::Buffer, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::tir::Buffer> > > const&)\n  2: tvm::te::InferBound(tvm::te::Schedule const&)\n  1: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)\n  0: tvm::te::BaseComputeOpNode::GatherBound(tvm::te::Operation const&, std::unordered_map<tvm::te::Tensor, tvm::te::TensorDom, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::te::TensorDom> > > const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*) const\n  File \"/home/tianqi_tang/tvm/src/te/operation/compute_op.cc\", line 256\nTVMError: \n---------------------------------------------------------------\nAn error occurred during the execution of TVM.\nFor more information, please see: https://tvm.apache.org/docs/errors.html\n---------------------------------------------------------------\n  Check failed: (!out_dom_map->count(this->reduce_axis[i])) is false: "
     ]
    }
   ],
   "source": [
    "ibound_lb = te.placeholder((batch_size, length, dim_out))\n",
    "ibound_ub = te.placeholder((batch_size, length, dim_out))\n",
    "ibound_lw = te.placeholder((batch_size, length, dim_in, dim_out))\n",
    "ibound_uw = te.placeholder((batch_size, length, dim_in, dim_out))\n",
    "w = te.placeholder((dim_out, dim_y_out))\n",
    "obound_lb = te.placeholder((batch_size, length, dim_y_out))\n",
    "obound_ub = te.placeholder((batch_size, length, dim_y_out))\n",
    "obound_lw = te.placeholder((batch_size, length, dim_in, dim_y_out))\n",
    "obound_uw = te.placeholder((batch_size, length, dim_in, dim_y_out))\n",
    "\n",
    "pos_mask = te.compute(w.shape, \n",
    "                      lambda i,j: te.if_then_else(w[i,j]>0, 1., 0.)\n",
    "                     )\n",
    "\n",
    "# s = te.create_schedule(pos_mask.op)\n",
    "# func = tvm.build(s, [w,pos_mask], target=target, name=\"foo\")\n",
    "# func(input_w_tvm, pos_mask_tvm)\n",
    "# evaluator = func.time_evaluator(func.entry_name, dev, number=1)\n",
    "# print(\"Baseline: %f\" % evaluator(input_w_tvm, pos_mask_tvm).mean)\n",
    "# print(tvm.lower(s, [w, pos_mask], simple_mode=True))\n",
    "\n",
    "w_pos = te.compute(w.shape, lambda i,j: w[i,j]*pos_mask[i,j])\n",
    "# w_pos = te.compute(w.shape, lambda i,j: function_lambda(w, pos_mask, i, j))\n",
    "w_neg = te.compute(w.shape, lambda i,j: w[i,j]-w_pos[i,j])\n",
    "\n",
    "# s = te.create_schedule(w_pos.op)\n",
    "# func = tvm.build(s, [w, w_pos], target=target, name=\"foo\")\n",
    "# func(input_w_tvm, w_pos_tvm)\n",
    "# evaluator = func.time_evaluator(func.entry_name, dev, number=1)\n",
    "# print(\"Baseline: %f\" % evaluator(input_w_tvm, w_pos_tvm).mean)\n",
    "# print(tvm.lower(s, [w, w_pos], simple_mode=True))\n",
    "\n",
    "\n",
    "dout = te.reduce_axis((0, dim_out), \"dout\")\n",
    "res_a = te.compute(obound_lw.shape,\n",
    "                 lambda b,l,din,dout_y: \n",
    "                 te.sum(ibound_lw[b,l,din,dout]*w_pos[dout,dout_y], axis=dout)#+te.sum(ibound_uw[b,l,din,dout]*w_neg[dout_y,dout], axis=dout)\n",
    "                )\n",
    "\n",
    "print(res_a.shape)\n",
    "\n",
    "# s = te.create_schedule(res_a.op)\n",
    "# func = tvm.build(s, [w, ibound_lw, res_a], target=target, name=\"foo\")\n",
    "# func(input_w_tvm, input_bound_lw_tvm, output_bound_lw_tvm)\n",
    "# evaluator = func.time_evaluator(func.entry_name, dev, number=1)\n",
    "# print(\"Baseline: %f\" % evaluator(input_w_tvm, input_bound_lw_tvm, output_bound_lw_tvm).mean)\n",
    "# print(tvm.lower(s, [w, ibound_lw, res_a], simple_mode=True))\n",
    "\n",
    "\n",
    "res_b = te.compute(obound_lw.shape,\n",
    "                   lambda b, l, din, dout_y:\n",
    "                   te.sum(ibound_uw[b,l,din,dout]*w_neg[dout,dout_y], axis=dout)\n",
    "                  )\n",
    "# print(res_b.shape)\n",
    "# s = te.create_schedule(res_b.op)\n",
    "# func = tvm.build(s, [w, ibound_uw, res_b], target=target, name=\"foo\")\n",
    "# func(input_w_tvm, input_bound_lw_tvm, output_bound_lw_tvm)\n",
    "# evaluator = func.time_evaluator(func.entry_name, dev, number=1)\n",
    "# print(\"Baseline: %f\" % evaluator(input_w_tvm, input_bound_lw_tvm, output_bound_lw_tvm).mean)\n",
    "# print(tvm.lower(s, [w, ibound_uw, res_b], simple_mode=True))\n",
    "\n",
    "\n",
    "res = te.compute(obound_lw.shape, lambda b,l,din,dout_y:res_a[b,l,din,dout_y]+res_b[b,l,din,dout_y])\n",
    "print(ibound_uw.shape, ibound_lw.shape, w.shape, res.shape)\n",
    "s = te.create_schedule(res.op)\n",
    "# print()\n",
    "# print(tvm.lower(s, [ibound_lw,ibound_uw, res, w], simple_mode=True))\n",
    "\n",
    "func = tvm.build(s, [ibound_lw, ibound_uw, w, res], target=target, name=\"certified_matmul\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
