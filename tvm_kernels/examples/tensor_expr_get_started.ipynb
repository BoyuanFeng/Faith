{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "import tvm.testing\n",
    "from tvm import te\n",
    "import numpy as np\n",
    "\n",
    "# You will get better performance if you can identify the CPU you are targeting\n",
    "# and specify it. If you're using llvm, you can get this information from the\n",
    "# command ``llc --version`` to get the CPU type, and you can check\n",
    "# ``/proc/cpuinfo`` for additional extensions that your processor might\n",
    "# support. For example, you can use \"llvm -mcpu=skylake-avx512\" for CPUs with\n",
    "# AVX-512 instructions.\n",
    "\n",
    "tgt = tvm.target.Target(target=\"llvm\", host=\"llvm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = te.var(\"n\")\n",
    "A = te.placeholder((n,), name=\"A\")\n",
    "B = te.placeholder((n,), name=\"B\")\n",
    "C = te.compute(A.shape, lambda i: A[i] + B[i], name=\"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = te.create_schedule(C.op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fadd = tvm.build(s, [A, B, C], tgt, name=\"myadd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = tvm.device(tgt.kind.name, 0)\n",
    "\n",
    "n = 1024\n",
    "a = tvm.nd.array(np.random.uniform(size=n).astype(A.dtype), dev)\n",
    "b = tvm.nd.array(np.random.uniform(size=n).astype(B.dtype), dev)\n",
    "c = tvm.nd.array(np.zeros(n, dtype=C.dtype), dev)\n",
    "fadd(a, b, c)\n",
    "tvm.testing.assert_allclose(c.numpy(), a.numpy() + b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy running time: 0.000017\n",
      "naive: 0.000021\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "np_repeat = 100\n",
    "np_running_time = timeit.timeit(\n",
    "    setup=\"import numpy\\n\"\n",
    "    \"n = 32768\\n\"\n",
    "    'dtype = \"float32\"\\n'\n",
    "    \"a = numpy.random.rand(n, 1).astype(dtype)\\n\"\n",
    "    \"b = numpy.random.rand(n, 1).astype(dtype)\\n\",\n",
    "    stmt=\"answer = a + b\",\n",
    "    number=np_repeat,\n",
    ")\n",
    "print(\"Numpy running time: %f\" % (np_running_time / np_repeat))\n",
    "\n",
    "\n",
    "def evaluate_addition(func, target, optimization, log):\n",
    "    dev = tvm.device(target.kind.name, 0)\n",
    "    n = 32768\n",
    "    a = tvm.nd.array(np.random.uniform(size=n).astype(A.dtype), dev)\n",
    "    b = tvm.nd.array(np.random.uniform(size=n).astype(B.dtype), dev)\n",
    "    c = tvm.nd.array(np.zeros(n, dtype=C.dtype), dev)\n",
    "\n",
    "    evaluator = func.time_evaluator(func.entry_name, dev, number=10)\n",
    "    mean_time = evaluator(a, b, c).mean\n",
    "    print(\"%s: %f\" % (optimization, mean_time))\n",
    "\n",
    "    log.append((optimization, mean_time))\n",
    "\n",
    "\n",
    "log = [(\"numpy\", np_running_time / np_repeat)]\n",
    "evaluate_addition(fadd, tgt, \"naive\", log=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {C: Buffer(C_2: Pointer(float32), float32, [n: int32], [stride: int32], type=\"auto\"),\n",
      "             A: Buffer(A_2: Pointer(float32), float32, [n], [stride_1: int32], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [n], [stride_2: int32], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C} {\n",
      "  for (i: int32, 0, n) \"parallel\" {\n",
      "    C_2[(i*stride)] = ((float32*)A_2[(i*stride_1)] + (float32*)B_2[(i*stride_2)])\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s[C].parallel(C.op.axis[0])\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallel: 0.000011\n"
     ]
    }
   ],
   "source": [
    "fadd_parallel = tvm.build(s, [A, B, C], tgt, name=\"myadd_parallel\")\n",
    "fadd_parallel(a, b, c)\n",
    "\n",
    "tvm.testing.assert_allclose(c.numpy(), a.numpy() + b.numpy())\n",
    "\n",
    "evaluate_addition(fadd_parallel, tgt, \"parallel\", log=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector: 0.000021\n",
      "primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {C: Buffer(C_2: Pointer(float32), float32, [n: int32], [stride: int32], type=\"auto\"),\n",
      "             A: Buffer(A_2: Pointer(float32), float32, [n], [stride_1: int32], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [n], [stride_2: int32], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C} {\n",
      "  for (i.outer: int32, 0, floordiv((n + 3), 4)) \"parallel\" {\n",
      "    for (i.inner.s: int32, 0, 4) {\n",
      "      if @tir.likely((((i.outer*4) + i.inner.s) < n), dtype=bool) {\n",
      "        C_2[(((i.outer*4) + i.inner.s)*stride)] = ((float32*)A_2[(((i.outer*4) + i.inner.s)*stride_1)] + (float32*)B_2[(((i.outer*4) + i.inner.s)*stride_2)])\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Recreate the schedule, since we modified it with the parallel operation in\n",
    "# the previous example\n",
    "n = te.var(\"n\")\n",
    "A = te.placeholder((n,), name=\"A\")\n",
    "B = te.placeholder((n,), name=\"B\")\n",
    "C = te.compute(A.shape, lambda i: A[i] + B[i], name=\"C\")\n",
    "\n",
    "s = te.create_schedule(C.op)\n",
    "\n",
    "# This factor should be chosen to match the number of threads appropriate for\n",
    "# your CPU. This will vary depending on architecture, but a good rule is\n",
    "# setting this factor to equal the number of available CPU cores.\n",
    "factor = 4\n",
    "\n",
    "outer, inner = s[C].split(C.op.axis[0], factor=factor)\n",
    "s[C].parallel(outer)\n",
    "s[C].vectorize(inner)\n",
    "\n",
    "fadd_vector = tvm.build(s, [A, B, C], tgt, name=\"myadd_parallel\")\n",
    "\n",
    "evaluate_addition(fadd_vector, tgt, \"vector\", log=log)\n",
    "\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Operator\t              Timing\t         Performance\n",
      "               numpy\t1.65657201432623e-05\t                 1.0\n",
      "               naive\t         2.13486e-05\t  1.2887215174091313\n",
      "            parallel\t1.0666800000000001e-05\t  0.6439080165397132\n",
      "              vector\t2.0833899999999997e-05\t  1.2576513317758586\n"
     ]
    }
   ],
   "source": [
    "baseline = log[0][1]\n",
    "print(\"%s\\t%s\\t%s\" % (\"Operator\".rjust(20), \"Timing\".rjust(20), \"Performance\".rjust(20)))\n",
    "for result in log:\n",
    "    print(\n",
    "        \"%s\\t%s\\t%s\"\n",
    "        % (result[0].rjust(20), str(result[1]).rjust(20), str(result[1] / baseline).rjust(20))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tvm.te.tensor.Tensor'>\n",
      "gpu_optimized: 0.000003\n",
      "-----GPU code-----\n",
      "\n",
      "#ifdef _WIN32\n",
      "  using uint = unsigned int;\n",
      "  using uchar = unsigned char;\n",
      "  using ushort = unsigned short;\n",
      "  using int64_t = long long;\n",
      "  using uint64_t = unsigned long long;\n",
      "#else\n",
      "  #define uint unsigned int\n",
      "  #define uchar unsigned char\n",
      "  #define ushort unsigned short\n",
      "  #define int64_t long long\n",
      "  #define uint64_t unsigned long long\n",
      "#endif\n",
      "extern \"C\" __global__ void __launch_bounds__(64) myadd_kernel0(float* __restrict__ C, float* __restrict__ A, float* __restrict__ B, int n, int stride, int stride1, int stride2) {\n",
      "  if (((int)blockIdx.x) < (n >> 6)) {\n",
      "    C[((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) * stride2))] = (A[((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) * stride))] + B[((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) * stride1))]);\n",
      "  } else {\n",
      "    if (((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) < n) {\n",
      "      C[((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) * stride2))] = (A[((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) * stride))] + B[((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) * stride1))]);\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If you want to run this code, change ``run_cuda = True``\n",
    "# Note that by default this example is not run in the docs CI.\n",
    "\n",
    "from tvm.autotvm.measure.measure_methods import set_cuda_target_arch\n",
    "set_cuda_target_arch('sm_75')\n",
    "\n",
    "run_cuda = True\n",
    "if run_cuda:\n",
    "    # Change this target to the correct backend for you gpu. For example: cuda (NVIDIA GPUs),\n",
    "    # rocm (Radeon GPUS), OpenCL (opencl).\n",
    "    tgt_gpu = tvm.target.Target(target=\"cuda\", host=\"llvm\")\n",
    "\n",
    "    # Recreate the schedule\n",
    "    n = te.var(\"n\")\n",
    "    A = te.placeholder((n,), name=\"A\")\n",
    "    B = te.placeholder((n,), name=\"B\")\n",
    "    C = te.compute(A.shape, lambda i: A[i] + B[i], name=\"C\")\n",
    "    print(type(C))\n",
    "\n",
    "    s = te.create_schedule(C.op)\n",
    "\n",
    "    bx, tx = s[C].split(C.op.axis[0], factor=64)\n",
    "\n",
    "    ################################################################################\n",
    "    # Finally we must bind the iteration axis bx and tx to threads in the GPU\n",
    "    # compute grid. The naive schedule is not valid for GPUs, and these are\n",
    "    # specific constructs that allow us to generate code that runs on a GPU.\n",
    "\n",
    "    s[C].bind(bx, te.thread_axis(\"blockIdx.x\"))\n",
    "    s[C].bind(tx, te.thread_axis(\"threadIdx.x\"))\n",
    "\n",
    "    ######################################################################\n",
    "    # Compilation\n",
    "    # -----------\n",
    "    # After we have finished specifying the schedule, we can compile it\n",
    "    # into a TVM function. By default TVM compiles into a type-erased\n",
    "    # function that can be directly called from the python side.\n",
    "    #\n",
    "    # In the following line, we use tvm.build to create a function.\n",
    "    # The build function takes the schedule, the desired signature of the\n",
    "    # function (including the inputs and outputs) as well as target language\n",
    "    # we want to compile to.\n",
    "    #\n",
    "    # The result of compilation fadd is a GPU device function (if GPU is\n",
    "    # involved) as well as a host wrapper that calls into the GPU\n",
    "    # function. fadd is the generated host wrapper function, it contains\n",
    "    # a reference to the generated device function internally.\n",
    "\n",
    "    fadd = tvm.build(s, [A, B, C], target=tgt_gpu, name=\"myadd\")\n",
    "\n",
    "    ################################################################################\n",
    "    # The compiled TVM function exposes a concise C API that can be invoked from\n",
    "    # any language.\n",
    "    #\n",
    "    # We provide a minimal array API in python to aid quick testing and prototyping.\n",
    "    # The array API is based on the `DLPack <https://github.com/dmlc/dlpack>`_ standard.\n",
    "    #\n",
    "    # - We first create a GPU device.\n",
    "    # - Then tvm.nd.array copies the data to the GPU.\n",
    "    # - ``fadd`` runs the actual computation\n",
    "    # - ``numpy()`` copies the GPU array back to the CPU (so we can verify correctness).\n",
    "    #\n",
    "    # Note that copying the data to and from the memory on the GPU is a required step.\n",
    "\n",
    "    dev = tvm.device(tgt_gpu.kind.name, 0)\n",
    "\n",
    "    n = 1024\n",
    "    a = tvm.nd.array(np.random.uniform(size=n).astype(A.dtype), dev)\n",
    "    b = tvm.nd.array(np.random.uniform(size=n).astype(B.dtype), dev)\n",
    "    c = tvm.nd.array(np.zeros(n, dtype=C.dtype), dev)\n",
    "    fadd(a, b, c)\n",
    "    tvm.testing.assert_allclose(c.numpy(), a.numpy() + b.numpy())\n",
    "\n",
    "    evaluate_addition(fadd, tgt_gpu, \"gpu_optimized\", log=log)\n",
    "\n",
    "    ################################################################################\n",
    "    # Inspect the Generated GPU Code\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # You can inspect the generated code in TVM. The result of tvm.build is a TVM\n",
    "    # Module. fadd is the host module that contains the host wrapper, it also\n",
    "    # contains a device module for the CUDA (GPU) function.\n",
    "    #\n",
    "    # The following code fetches the device module and prints the content code.\n",
    "\n",
    "    if (\n",
    "        tgt_gpu.kind.name == \"cuda\"\n",
    "        or tgt_gpu.kind.name == \"rocm\"\n",
    "        or tgt_gpu.kind.name.startswith(\"opencl\")\n",
    "    ):\n",
    "        dev_module = fadd.imported_modules[0]\n",
    "        print(\"-----GPU code-----\")\n",
    "        print(dev_module.get_source())\n",
    "    else:\n",
    "        print(fadd.get_source())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tgt.kind.name.startswith(\"opencl\"):\n",
    "    fadd_cl = tvm.build(s, [A, B, C], tgt, name=\"myadd\")\n",
    "    print(\"------opencl code------\")\n",
    "    print(fadd_cl.imported_modules[0].get_source())\n",
    "    dev = tvm.cl(0)\n",
    "    n = 1024\n",
    "    a = tvm.nd.array(np.random.uniform(size=n).astype(A.dtype), dev)\n",
    "    b = tvm.nd.array(np.random.uniform(size=n).astype(B.dtype), dev)\n",
    "    c = tvm.nd.array(np.zeros(n, dtype=C.dtype), dev)\n",
    "    fadd_cl(a, b, c)\n",
    "    tvm.testing.assert_allclose(c.numpy(), a.numpy() + b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy running time: 0.012656\n"
     ]
    }
   ],
   "source": [
    "import tvm\n",
    "import tvm.testing\n",
    "from tvm import te\n",
    "import numpy\n",
    "\n",
    "# The size of the matrix\n",
    "# (M, K) x (K, N)\n",
    "# You are free to try out different shapes, sometimes TVM optimization outperforms numpy with MKL.\n",
    "M = 1024\n",
    "K = 1024\n",
    "N = 1024\n",
    "\n",
    "# The default tensor data type in tvm\n",
    "dtype = \"float32\"\n",
    "\n",
    "# You will want to adjust the target to match any CPU vector extensions you\n",
    "# might have. For example, if you're using using Intel AVX2 (Advanced Vector\n",
    "# Extensions) ISA for SIMD, you can get the best performance by changing the\n",
    "# following line to ``llvm -mcpu=core-avx2``, or specific type of CPU you use.\n",
    "# Recall that you're using llvm, you can get this information from the command\n",
    "# ``llc --version`` to get the CPU type, and you can check ``/proc/cpuinfo``\n",
    "# for additional extensions that your processor might support.\n",
    "\n",
    "target = tvm.target.Target(target=\"llvm\", host=\"llvm\")\n",
    "dev = tvm.device(target.kind.name, 0)\n",
    "\n",
    "# Random generated tensor for testing\n",
    "a = tvm.nd.array(numpy.random.rand(M, K).astype(dtype), dev)\n",
    "b = tvm.nd.array(numpy.random.rand(K, N).astype(dtype), dev)\n",
    "\n",
    "# Repeatedly perform a matrix multiplication to get a performance baseline\n",
    "# for the default numpy implementation\n",
    "np_repeat = 100\n",
    "np_running_time = timeit.timeit(\n",
    "    setup=\"import numpy\\n\"\n",
    "    \"M = \" + str(M) + \"\\n\"\n",
    "    \"K = \" + str(K) + \"\\n\"\n",
    "    \"N = \" + str(N) + \"\\n\"\n",
    "    'dtype = \"float32\"\\n'\n",
    "    \"a = numpy.random.rand(M, K).astype(dtype)\\n\"\n",
    "    \"b = numpy.random.rand(K, N).astype(dtype)\\n\",\n",
    "    stmt=\"answer = numpy.dot(a, b)\",\n",
    "    number=np_repeat,\n",
    ")\n",
    "print(\"Numpy running time: %f\" % (np_running_time / np_repeat))\n",
    "\n",
    "answer = numpy.dot(a.numpy(), b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none: 3.747287\n"
     ]
    }
   ],
   "source": [
    "# TVM Matrix Multiplication using TE\n",
    "k = te.reduce_axis((0, K), \"k\")\n",
    "A = te.placeholder((M, K), name=\"A\")\n",
    "B = te.placeholder((K, N), name=\"B\")\n",
    "C = te.compute((M, N), lambda x, y: te.sum(A[x, k] * B[k, y], axis=k), name=\"C\")\n",
    "\n",
    "# Default schedule\n",
    "s = te.create_schedule(C.op)\n",
    "func = tvm.build(s, [A, B, C], target=target, name=\"mmult\")\n",
    "\n",
    "c = tvm.nd.array(numpy.zeros((M, N), dtype=dtype), dev)\n",
    "func(a, b, c)\n",
    "tvm.testing.assert_allclose(c.numpy(), answer, rtol=1e-5)\n",
    "\n",
    "\n",
    "def evaluate_operation(s, vars, target, name, optimization, log):\n",
    "    func = tvm.build(s, [A, B, C], target=target, name=\"mmult\")\n",
    "    assert func\n",
    "\n",
    "    c = tvm.nd.array(numpy.zeros((M, N), dtype=dtype), dev)\n",
    "    func(a, b, c)\n",
    "    tvm.testing.assert_allclose(c.numpy(), answer, rtol=1e-5)\n",
    "\n",
    "    evaluator = func.time_evaluator(func.entry_name, dev, number=10)\n",
    "    mean_time = evaluator(a, b, c).mean\n",
    "    print(\"%s: %f\" % (optimization, mean_time))\n",
    "    log.append((optimization, mean_time))\n",
    "\n",
    "\n",
    "log = []\n",
    "\n",
    "evaluate_operation(s, [A, B, C], target=target, name=\"mmult\", optimization=\"none\", log=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocking: 0.318817\n"
     ]
    }
   ],
   "source": [
    "bn = 32\n",
    "\n",
    "# Blocking by loop tiling\n",
    "xo, yo, xi, yi = s[C].tile(C.op.axis[0], C.op.axis[1], bn, bn)\n",
    "(k,) = s[C].op.reduce_axis\n",
    "ko, ki = s[C].split(k, factor=4)\n",
    "\n",
    "# Hoist reduction domain outside the blocking loop\n",
    "s[C].reorder(xo, yo, ko, ki, xi, yi)\n",
    "\n",
    "evaluate_operation(s, [A, B, C], target=target, name=\"mmult\", optimization=\"blocking\", log=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorization: 0.344938\n",
      "primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [1024, 1024], []),\n",
      "             C: Buffer(C_2: Pointer(float32), float32, [1024, 1024], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [1024, 1024], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C} {\n",
      "  for (x.outer: int32, 0, 32) {\n",
      "    for (y.outer: int32, 0, 32) {\n",
      "      for (x.inner.init: int32, 0, 32) {\n",
      "        C_2[ramp((((x.outer*32768) + (x.inner.init*1024)) + (y.outer*32)), 1, 32)] = broadcast(0f32, 32)\n",
      "      }\n",
      "      for (k.outer: int32, 0, 256) {\n",
      "        for (k.inner: int32, 0, 4) {\n",
      "          for (x.inner: int32, 0, 32) {\n",
      "            C_2[ramp((((x.outer*32768) + (x.inner*1024)) + (y.outer*32)), 1, 32)] = ((float32x32*)C_2[ramp((((x.outer*32768) + (x.inner*1024)) + (y.outer*32)), 1, 32)] + (broadcast((float32*)A_2[((((x.outer*32768) + (x.inner*1024)) + (k.outer*4)) + k.inner)], 32)*(float32x32*)B_2[ramp((((k.outer*4096) + (k.inner*1024)) + (y.outer*32)), 1, 32)]))\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply the vectorization optimization\n",
    "s[C].vectorize(yi)\n",
    "\n",
    "evaluate_operation(s, [A, B, C], target=target, name=\"mmult\", optimization=\"vectorization\", log=log)\n",
    "\n",
    "# The generalized IR after vectorization\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop permutation: 0.120492\n",
      "primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {C: Buffer(C_2: Pointer(float32), float32, [1024, 1024], []),\n",
      "             A: Buffer(A_2: Pointer(float32), float32, [1024, 1024], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [1024, 1024], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C} {\n",
      "  for (x.outer: int32, 0, 32) {\n",
      "    for (y.outer: int32, 0, 32) {\n",
      "      for (x.inner.init: int32, 0, 32) {\n",
      "        C_2[ramp((((x.outer*32768) + (x.inner.init*1024)) + (y.outer*32)), 1, 32)] = broadcast(0f32, 32)\n",
      "      }\n",
      "      for (k.outer: int32, 0, 256) {\n",
      "        for (x.inner: int32, 0, 32) {\n",
      "          for (k.inner: int32, 0, 4) {\n",
      "            C_2[ramp((((x.outer*32768) + (x.inner*1024)) + (y.outer*32)), 1, 32)] = ((float32x32*)C_2[ramp((((x.outer*32768) + (x.inner*1024)) + (y.outer*32)), 1, 32)] + (broadcast((float32*)A_2[((((x.outer*32768) + (x.inner*1024)) + (k.outer*4)) + k.inner)], 32)*(float32x32*)B_2[ramp((((k.outer*4096) + (k.inner*1024)) + (y.outer*32)), 1, 32)]))\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = te.create_schedule(C.op)\n",
    "xo, yo, xi, yi = s[C].tile(C.op.axis[0], C.op.axis[1], bn, bn)\n",
    "(k,) = s[C].op.reduce_axis\n",
    "ko, ki = s[C].split(k, factor=4)\n",
    "\n",
    "# re-ordering\n",
    "s[C].reorder(xo, yo, ko, xi, ki, yi)\n",
    "s[C].vectorize(yi)\n",
    "\n",
    "evaluate_operation(\n",
    "    s, [A, B, C], target=target, name=\"mmult\", optimization=\"loop permutation\", log=log\n",
    ")\n",
    "\n",
    "# Again, print the new generalized IR\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array packing: 0.144356\n",
      "primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [1024, 1024], []),\n",
      "             C: Buffer(C_2: Pointer(float32), float32, [1024, 1024], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [1024, 1024], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C} {\n",
      "  allocate(packedB: Pointer(global float32x32), float32x32, [32768]), storage_scope = global {\n",
      "    for (x: int32, 0, 32) \"parallel\" {\n",
      "      for (y: int32, 0, 1024) {\n",
      "        packedB[ramp(((x*32768) + (y*32)), 1, 32)] = (float32x32*)B_2[ramp(((y*1024) + (x*32)), 1, 32)]\n",
      "      }\n",
      "    }\n",
      "    for (x.outer: int32, 0, 32) {\n",
      "      for (y.outer: int32, 0, 32) {\n",
      "        for (x.inner.init: int32, 0, 32) {\n",
      "          C_2[ramp((((x.outer*32768) + (x.inner.init*1024)) + (y.outer*32)), 1, 32)] = broadcast(0f32, 32)\n",
      "        }\n",
      "        for (k.outer: int32, 0, 256) {\n",
      "          for (x.inner: int32, 0, 32) {\n",
      "            for (k.inner: int32, 0, 4) {\n",
      "              C_2[ramp((((x.outer*32768) + (x.inner*1024)) + (y.outer*32)), 1, 32)] = ((float32x32*)C_2[ramp((((x.outer*32768) + (x.inner*1024)) + (y.outer*32)), 1, 32)] + (broadcast((float32*)A_2[((((x.outer*32768) + (x.inner*1024)) + (k.outer*4)) + k.inner)], 32)*(float32x32*)packedB[ramp((((y.outer*32768) + (k.outer*128)) + (k.inner*32)), 1, 32)]))\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We have to re-write the algorithm slightly.\n",
    "packedB = te.compute((N / bn, K, bn), lambda x, y, z: B[y, x * bn + z], name=\"packedB\")\n",
    "C = te.compute(\n",
    "    (M, N),\n",
    "    lambda x, y: te.sum(A[x, k] * packedB[y // bn, k, tvm.tir.indexmod(y, bn)], axis=k),\n",
    "    name=\"C\",\n",
    ")\n",
    "\n",
    "s = te.create_schedule(C.op)\n",
    "\n",
    "xo, yo, xi, yi = s[C].tile(C.op.axis[0], C.op.axis[1], bn, bn)\n",
    "(k,) = s[C].op.reduce_axis\n",
    "ko, ki = s[C].split(k, factor=4)\n",
    "\n",
    "s[C].reorder(xo, yo, ko, xi, ki, yi)\n",
    "s[C].vectorize(yi)\n",
    "\n",
    "x, y, z = s[packedB].op.axis\n",
    "s[packedB].vectorize(z)\n",
    "s[packedB].parallel(x)\n",
    "\n",
    "evaluate_operation(s, [A, B, C], target=target, name=\"mmult\", optimization=\"array packing\", log=log)\n",
    "\n",
    "# Here is the generated IR after array packing.\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block caching: 0.144224\n",
      "primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [1024, 1024], []),\n",
      "             C: Buffer(C_2: Pointer(float32), float32, [1024, 1024], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [1024, 1024], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C} {\n",
      "  allocate(packedB: Pointer(global float32x32), float32x32, [32768]), storage_scope = global;\n",
      "  allocate(C.global: Pointer(global float32), float32, [1024]), storage_scope = global {\n",
      "    for (x: int32, 0, 32) \"parallel\" {\n",
      "      for (y: int32, 0, 1024) {\n",
      "        packedB[ramp(((x*32768) + (y*32)), 1, 32)] = (float32x32*)B_2[ramp(((y*1024) + (x*32)), 1, 32)]\n",
      "      }\n",
      "    }\n",
      "    for (x.outer: int32, 0, 32) {\n",
      "      for (y.outer: int32, 0, 32) {\n",
      "        for (x.c.init: int32, 0, 32) {\n",
      "          C.global[ramp((x.c.init*32), 1, 32)] = broadcast(0f32, 32)\n",
      "        }\n",
      "        for (k.outer: int32, 0, 256) {\n",
      "          for (x.c: int32, 0, 32) {\n",
      "            C.global[ramp((x.c*32), 1, 32)] = ((float32x32*)C.global[ramp((x.c*32), 1, 32)] + (broadcast((float32*)A_2[(((x.outer*32768) + (x.c*1024)) + (k.outer*4))], 32)*(float32x32*)packedB[ramp(((y.outer*32768) + (k.outer*128)), 1, 32)]))\n",
      "            C.global[ramp((x.c*32), 1, 32)] = ((float32x32*)C.global[ramp((x.c*32), 1, 32)] + (broadcast((float32*)A_2[((((x.outer*32768) + (x.c*1024)) + (k.outer*4)) + 1)], 32)*(float32x32*)packedB[ramp((((y.outer*32768) + (k.outer*128)) + 32), 1, 32)]))\n",
      "            C.global[ramp((x.c*32), 1, 32)] = ((float32x32*)C.global[ramp((x.c*32), 1, 32)] + (broadcast((float32*)A_2[((((x.outer*32768) + (x.c*1024)) + (k.outer*4)) + 2)], 32)*(float32x32*)packedB[ramp((((y.outer*32768) + (k.outer*128)) + 64), 1, 32)]))\n",
      "            C.global[ramp((x.c*32), 1, 32)] = ((float32x32*)C.global[ramp((x.c*32), 1, 32)] + (broadcast((float32*)A_2[((((x.outer*32768) + (x.c*1024)) + (k.outer*4)) + 3)], 32)*(float32x32*)packedB[ramp((((y.outer*32768) + (k.outer*128)) + 96), 1, 32)]))\n",
      "          }\n",
      "        }\n",
      "        for (x.inner: int32, 0, 32) {\n",
      "          for (y.inner: int32, 0, 32) {\n",
      "            C_2[((((x.outer*32768) + (x.inner*1024)) + (y.outer*32)) + y.inner)] = (float32*)C.global[((x.inner*32) + y.inner)]\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = te.create_schedule(C.op)\n",
    "\n",
    "# Allocate write cache\n",
    "CC = s.cache_write(C, \"global\")\n",
    "\n",
    "xo, yo, xi, yi = s[C].tile(C.op.axis[0], C.op.axis[1], bn, bn)\n",
    "\n",
    "# Write cache is computed at yo\n",
    "s[CC].compute_at(s[C], yo)\n",
    "\n",
    "# New inner axes\n",
    "xc, yc = s[CC].op.axis\n",
    "\n",
    "(k,) = s[CC].op.reduce_axis\n",
    "ko, ki = s[CC].split(k, factor=4)\n",
    "s[CC].reorder(ko, xc, ki, yc)\n",
    "s[CC].unroll(ki)\n",
    "s[CC].vectorize(yc)\n",
    "\n",
    "x, y, z = s[packedB].op.axis\n",
    "s[packedB].vectorize(z)\n",
    "s[packedB].parallel(x)\n",
    "\n",
    "evaluate_operation(s, [A, B, C], target=target, name=\"mmult\", optimization=\"block caching\", log=log)\n",
    "\n",
    "# Here is the generated IR after write cache blocking.\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallelization: 0.023709\n",
      "primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {C: Buffer(C_2: Pointer(float32), float32, [1024, 1024], []),\n",
      "             A: Buffer(A_2: Pointer(float32), float32, [1024, 1024], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [1024, 1024], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C} {\n",
      "  allocate(packedB: Pointer(global float32x32), float32x32, [32768]), storage_scope = global {\n",
      "    for (x: int32, 0, 32) \"parallel\" {\n",
      "      for (y: int32, 0, 1024) {\n",
      "        packedB[ramp(((x*32768) + (y*32)), 1, 32)] = (float32x32*)B_2[ramp(((y*1024) + (x*32)), 1, 32)]\n",
      "      }\n",
      "    }\n",
      "    for (x.outer: int32, 0, 32) \"parallel\" {\n",
      "      allocate(C.global: Pointer(global float32), float32, [1024]), storage_scope = global;\n",
      "      for (y.outer: int32, 0, 32) {\n",
      "        for (x.c.init: int32, 0, 32) {\n",
      "          C.global[ramp((x.c.init*32), 1, 32)] = broadcast(0f32, 32)\n",
      "        }\n",
      "        for (k.outer: int32, 0, 256) {\n",
      "          for (x.c: int32, 0, 32) {\n",
      "            C.global[ramp((x.c*32), 1, 32)] = ((float32x32*)C.global[ramp((x.c*32), 1, 32)] + (broadcast((float32*)A_2[(((x.outer*32768) + (x.c*1024)) + (k.outer*4))], 32)*(float32x32*)packedB[ramp(((y.outer*32768) + (k.outer*128)), 1, 32)]))\n",
      "            C.global[ramp((x.c*32), 1, 32)] = ((float32x32*)C.global[ramp((x.c*32), 1, 32)] + (broadcast((float32*)A_2[((((x.outer*32768) + (x.c*1024)) + (k.outer*4)) + 1)], 32)*(float32x32*)packedB[ramp((((y.outer*32768) + (k.outer*128)) + 32), 1, 32)]))\n",
      "            C.global[ramp((x.c*32), 1, 32)] = ((float32x32*)C.global[ramp((x.c*32), 1, 32)] + (broadcast((float32*)A_2[((((x.outer*32768) + (x.c*1024)) + (k.outer*4)) + 2)], 32)*(float32x32*)packedB[ramp((((y.outer*32768) + (k.outer*128)) + 64), 1, 32)]))\n",
      "            C.global[ramp((x.c*32), 1, 32)] = ((float32x32*)C.global[ramp((x.c*32), 1, 32)] + (broadcast((float32*)A_2[((((x.outer*32768) + (x.c*1024)) + (k.outer*4)) + 3)], 32)*(float32x32*)packedB[ramp((((y.outer*32768) + (k.outer*128)) + 96), 1, 32)]))\n",
      "          }\n",
      "        }\n",
      "        for (x.inner: int32, 0, 32) {\n",
      "          for (y.inner: int32, 0, 32) {\n",
      "            C_2[((((x.outer*32768) + (x.inner*1024)) + (y.outer*32)) + y.inner)] = (float32*)C.global[((x.inner*32) + y.inner)]\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# parallel\n",
    "s[C].parallel(xo)\n",
    "\n",
    "x, y, z = s[packedB].op.axis\n",
    "s[packedB].vectorize(z)\n",
    "s[packedB].parallel(x)\n",
    "\n",
    "evaluate_operation(\n",
    "    s, [A, B, C], target=target, name=\"mmult\", optimization=\"parallelization\", log=log\n",
    ")\n",
    "\n",
    "# Here is the generated IR after parallelization.\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Operator\t              Timing\t         Performance\n",
      "                none\t  3.7472870649999996\t                 1.0\n",
      "            blocking\t        0.3188166245\t   0.085079317108576\n",
      "       vectorization\t 0.34493826699999997\t 0.09205013147291399\n",
      "    loop permutation\t        0.1204922964\t0.032154541221410274\n",
      "       array packing\t 0.14435580669999998\t  0.0385227510452285\n",
      "       block caching\t        0.1442240137\t 0.03848758080133901\n",
      "     parallelization\t         0.023708579\t0.0063268648995269865\n"
     ]
    }
   ],
   "source": [
    "baseline = log[0][1]\n",
    "print(\"%s\\t%s\\t%s\" % (\"Operator\".rjust(20), \"Timing\".rjust(20), \"Performance\".rjust(20)))\n",
    "for result in log:\n",
    "    print(\n",
    "        \"%s\\t%s\\t%s\"\n",
    "        % (result[0].rjust(20), str(result[1]).rjust(20), str(result[1] / baseline).rjust(20))\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d6993cb2f9ce9a59d5d7380609d9cb5192a9dedd2735a011418ad9e827eb538"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
