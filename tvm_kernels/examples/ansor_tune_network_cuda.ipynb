{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tvm\n",
    "from tvm import relay, auto_scheduler\n",
    "import tvm.relay.testing\n",
    "from tvm.contrib import graph_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network(name, batch_size, layout=\"NHWC\", dtype=\"float32\"):\n",
    "\n",
    "    if layout == \"NHWC\":\n",
    "        image_shape = (224, 224, 3)\n",
    "    elif layout == \"NCHW\":\n",
    "        image_shape = (3, 224, 224)\n",
    "    else:\n",
    "        raise ValueError(\"Invlida layout: \"+ layout)\n",
    "\n",
    "    input_shape = (batch_size,) + image_shape\n",
    "    output_shape = (batch_size, 1000)\n",
    "\n",
    "    if name.startswith(\"resnet-\"):\n",
    "        n_layer = int(name.split(\"-\")[1])\n",
    "        mod, params = relay.testing.resnet.get_workload(\n",
    "            num_layers=n_layer,\n",
    "            batch_size=batch_size,\n",
    "            layout=layout,\n",
    "            dtype=dtype,\n",
    "            image_shape=image_shape,\n",
    "        )\n",
    "    elif name.startswith(\"resnet3d-\"):\n",
    "        n_layer = int(name.split(\"-\")[1])\n",
    "        mod, params = relay.testing.resnet.get_workload(\n",
    "            num_layers=n_layer,\n",
    "            batch_size=batch_size,\n",
    "            layout=layout,\n",
    "            dtype=dtype,\n",
    "            image_shape=image_shape,\n",
    "        )\n",
    "    elif name == \"mobilenet\":\n",
    "        mod, params = relay.testing.mobilenet.get_workload(\n",
    "            batch_size=batch_size, layout=layout, dtype=dtype, image_shape=image_shape\n",
    "        )\n",
    "    elif name == \"squeezenet_v1.1\":\n",
    "        assert layout == \"NCHW\", \"squeezenet_v1.1 only supports NCHW layout\"\n",
    "        mod, params = relay.testing.squeezenet.get_workload(\n",
    "            version=\"1.1\",\n",
    "            batch_size=batch_size,\n",
    "            dtype=dtype,\n",
    "            image_shape=image_shape,\n",
    "        )\n",
    "    elif name == \"inception_v3\":\n",
    "        input_shape = (batch_size, 3, 299, 299) if layout == \"NCHW\" else (batch_size, 299, 299, 3)\n",
    "        mod, params = relay.testing.inception_v3.get_workload(batch_size=batch_size, dtype=dtype)\n",
    "\n",
    "    return mod, params, input_shape, output_shape\n",
    "    \n",
    "# Define the neural network and compilation target\n",
    "network = \"resnet-18\"\n",
    "batch_size = 1\n",
    "layout = \"NHWC\"\n",
    "target = tvm.target.Target(\"cuda\")\n",
    "dtype = \"float32\"\n",
    "log_file = \"%s-%s-B%d-%s.json\" % (network, layout, batch_size, target.kind.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract tasks...\n",
      "========== Task 0  (workload key: [\"12b88bedece6984af589a28b43e0f3c4\", [1, 56, 56, 64], [3, 3, 64, 128], [1, 1, 1, 128], [1, 28, 28, 128]]) ==========\n",
      "placeholder = PLACEHOLDER [1, 56, 56, 64]\n",
      "PaddedInput(i0, i1, i2, i3) = tir.if_then_else(((((i1 >= 1) && (i1 < 57)) && (i2 >= 1)) && (i2 < 57)), placeholder[i0, (i1 - 1), (i2 - 1), i3], 0f)\n",
      "placeholder = PLACEHOLDER [3, 3, 64, 128]\n",
      "Conv2dOutput(nn, yy, xx, ff) += (PaddedInput[nn, ((yy*2) + ry), ((xx*2) + rx), rc]*placeholder[ry, rx, rc, ff])\n",
      "placeholder = PLACEHOLDER [1, 1, 1, 128]\n",
      "T_add(ax0, ax1, ax2, ax3) = (Conv2dOutput[ax0, ax1, ax2, ax3] + placeholder[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 1  (workload key: [\"12b88bedece6984af589a28b43e0f3c4\", [1, 14, 14, 256], [3, 3, 256, 512], [1, 1, 1, 512], [1, 7, 7, 512]]) ==========\n",
      "placeholder = PLACEHOLDER [1, 14, 14, 256]\n",
      "PaddedInput(i0, i1, i2, i3) = tir.if_then_else(((((i1 >= 1) && (i1 < 15)) && (i2 >= 1)) && (i2 < 15)), placeholder[i0, (i1 - 1), (i2 - 1), i3], 0f)\n",
      "placeholder = PLACEHOLDER [3, 3, 256, 512]\n",
      "Conv2dOutput(nn, yy, xx, ff) += (PaddedInput[nn, ((yy*2) + ry), ((xx*2) + rx), rc]*placeholder[ry, rx, rc, ff])\n",
      "placeholder = PLACEHOLDER [1, 1, 1, 512]\n",
      "T_add(ax0, ax1, ax2, ax3) = (Conv2dOutput[ax0, ax1, ax2, ax3] + placeholder[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 2  (workload key: [\"e4cdf917b876dbdd64488c3818d9c141\", [1, 28, 28, 128], [4, 4, 128, 128], [1, 1, 1, 128], [1, 28, 28, 128]]) ==========\n",
      "placeholder = PLACEHOLDER [1, 28, 28, 128]\n",
      "data_pad(i0, i1, i2, i3) = tir.if_then_else(((((i1 >= 1) && (i1 < 29)) && (i2 >= 1)) && (i2 < 29)), placeholder[i0, (i1 - 1), (i2 - 1), i3], 0f)\n",
      "input_tile(eps, nu, p, ci) = data_pad[floordiv(p, 196), ((floormod(floordiv(p, 14), 14)*2) + eps), ((floormod(p, 14)*2) + nu), ci]\n",
      "B(i, j) = select(((floormod(i, 4) == 3) && (floormod(j, 4) == 3)), 1f, select(((floormod(i, 4) == 3) && (floormod(j, 4) == 2)),  ..(OMITTED).. ormod(i, 4) == 0) && (floormod(j, 4) == 1)), 0f, select(((floormod(i, 4) == 0) && (floormod(j, 4) == 0)), 1f, 0f))))))))))))))))\n",
      "data_pack(eps, nu, p, ci) += ((input_tile[r_a, r_b, p, ci]*B[r_a, eps])*B[r_b, nu])\n",
      "placeholder = PLACEHOLDER [4, 4, 128, 128]\n",
      "bgemm(eps, nu, p, co) += (data_pack[eps, nu, p, ci]*placeholder[eps, nu, co, ci])\n",
      "A(i, j) = select(((floormod(i, 4) == 3) && (floormod(j, 2) == 1)), 1f, select(((floormod(i, 4) == 3) && (floormod(j, 2) == 0)),  ..(OMITTED).. ct(((floormod(i, 4) == 0) && (floormod(j, 2) == 1)), 0f, select(((floormod(i, 4) == 0) && (floormod(j, 2) == 0)), 1f, 0f))))))))\n",
      "inverse(vh, vw, p, co) += ((bgemm[r_a, r_b, p, co]*A[r_a, vh])*A[r_b, vw])\n",
      "conv2d_winograd(n, h, w, co) = inverse[floormod(h, 2), floormod(w, 2), ((((n*14)*14) + (floordiv(h, 2)*14)) + floordiv(w, 2)), co]\n",
      "placeholder = PLACEHOLDER [1, 1, 1, 128]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_winograd[ax0, ax1, ax2, ax3] + placeholder[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 3  (workload key: [\"c4500b4e2fd04e695c32d2f31bbdc14a\", [1, 28, 28, 128], [4, 4, 128, 128], [1, 28, 28, 128], [1, 1, 1, 128], [1, 28, 28, 128]]) ==========\n",
      "placeholder = PLACEHOLDER [1, 28, 28, 128]\n",
      "data_pad(i0, i1, i2, i3) = tir.if_then_else(((((i1 >= 1) && (i1 < 29)) && (i2 >= 1)) && (i2 < 29)), placeholder[i0, (i1 - 1), (i2 - 1), i3], 0f)\n",
      "input_tile(eps, nu, p, ci) = data_pad[floordiv(p, 196), ((floormod(floordiv(p, 14), 14)*2) + eps), ((floormod(p, 14)*2) + nu), ci]\n",
      "B(i, j) = select(((floormod(i, 4) == 3) && (floormod(j, 4) == 3)), 1f, select(((floormod(i, 4) == 3) && (floormod(j, 4) == 2)),  ..(OMITTED).. ormod(i, 4) == 0) && (floormod(j, 4) == 1)), 0f, select(((floormod(i, 4) == 0) && (floormod(j, 4) == 0)), 1f, 0f))))))))))))))))\n",
      "data_pack(eps, nu, p, ci) += ((input_tile[r_a, r_b, p, ci]*B[r_a, eps])*B[r_b, nu])\n",
      "placeholder = PLACEHOLDER [4, 4, 128, 128]\n",
      "bgemm(eps, nu, p, co) += (data_pack[eps, nu, p, ci]*placeholder[eps, nu, co, ci])\n",
      "A(i, j) = select(((floormod(i, 4) == 3) && (floormod(j, 2) == 1)), 1f, select(((floormod(i, 4) == 3) && (floormod(j, 2) == 0)),  ..(OMITTED).. ct(((floormod(i, 4) == 0) && (floormod(j, 2) == 1)), 0f, select(((floormod(i, 4) == 0) && (floormod(j, 2) == 0)), 1f, 0f))))))))\n",
      "inverse(vh, vw, p, co) += ((bgemm[r_a, r_b, p, co]*A[r_a, vh])*A[r_b, vw])\n",
      "conv2d_winograd(n, h, w, co) = inverse[floormod(h, 2), floormod(w, 2), ((((n*14)*14) + (floordiv(h, 2)*14)) + floordiv(w, 2)), co]\n",
      "placeholder = PLACEHOLDER [1, 28, 28, 128]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_winograd[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, ax2, ax3])\n",
      "placeholder = PLACEHOLDER [1, 1, 1, 128]\n",
      "T_add(ax0, ax1, ax2, ax3) = (T_add[ax0, ax1, ax2, ax3] + placeholder[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 4  (workload key: [\"69115f188984ae34ede37c3b8ca40b43\", [1, 7, 7, 512], [1, 1, 1, 512]]) ==========\n",
      "placeholder = PLACEHOLDER [1, 7, 7, 512]\n",
      "tensor(ax0, ax1, ax2, ax3) += placeholder[ax0, ((ax1*7) + rv0), ((ax2*7) + rv1), ax3]\n",
      "tensor(ax0, ax1, ax2, ax3) = (tensor[ax0, ax1, ax2, ax3]/(float32((select((bool)1, ((ax1 + 1)*7), (((ax1 + 1)*7) + 1)) - (ax1*7)))*float32((select((bool)1, ((ax2 + 1)*7), (((ax2 + 1)*7) + 1)) - (ax2*7)))))\n",
      "\n",
      "========== Task 5  (workload key: [\"3a69f9fbc63760d99e36b4c17b3bfc57\", [1, 7, 7, 512], [4, 4, 512, 512], [1, 1, 1, 512], [1, 7, 7, 512]]) ==========\n",
      "placeholder = PLACEHOLDER [1, 7, 7, 512]\n",
      "data_pad(i0, i1, i2, i3) = tir.if_then_else(((((i1 >= 1) && (i1 < 8)) && (i2 >= 1)) && (i2 < 8)), placeholder[i0, (i1 - 1), (i2 - 1), i3], 0f)\n",
      "input_tile(eps, nu, p, ci) = data_pad[floordiv(p, 16), ((floormod(floordiv(p, 4), 4)*2) + eps), ((floormod(p, 4)*2) + nu), ci]\n",
      "B(i, j) = select(((floormod(i, 4) == 3) && (floormod(j, 4) == 3)), 1f, select(((floormod(i, 4) == 3) && (floormod(j, 4) == 2)),  ..(OMITTED).. ormod(i, 4) == 0) && (floormod(j, 4) == 1)), 0f, select(((floormod(i, 4) == 0) && (floormod(j, 4) == 0)), 1f, 0f))))))))))))))))\n",
      "data_pack(eps, nu, p, ci) += ((input_tile[r_a, r_b, p, ci]*B[r_a, eps])*B[r_b, nu])\n",
      "placeholder = PLACEHOLDER [4, 4, 512, 512]\n",
      "bgemm(eps, nu, p, co) += (data_pack[eps, nu, p, ci]*placeholder[eps, nu, co, ci])\n",
      "A(i, j) = select(((floormod(i, 4) == 3) && (floormod(j, 2) == 1)), 1f, select(((floormod(i, 4) == 3) && (floormod(j, 2) == 0)),  ..(OMITTED).. ct(((floormod(i, 4) == 0) && (floormod(j, 2) == 1)), 0f, select(((floormod(i, 4) == 0) && (floormod(j, 2) == 0)), 1f, 0f))))))))\n",
      "inverse(vh, vw, p, co) += ((bgemm[r_a, r_b, p, co]*A[r_a, vh])*A[r_b, vw])\n",
      "conv2d_winograd(n, h, w, co) = inverse[floormod(h, 2), floormod(w, 2), ((((n*4)*4) + (floordiv(h, 2)*4)) + floordiv(w, 2)), co]\n",
      "placeholder = PLACEHOLDER [1, 1, 1, 512]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_winograd[ax0, ax1, ax2, ax3] + placeholder[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 6  (workload key: [\"b818b53148cd450f86569dfc3e04cb8a\", [1, 56, 56, 64], [6, 6, 64, 64], [1, 1, 1, 64], [1, 56, 56, 64]]) ==========\n",
      "placeholder = PLACEHOLDER [1, 56, 56, 64]\n",
      "data_pad(i0, i1, i2, i3) = tir.if_then_else(((((i1 >= 1) && (i1 < 57)) && (i2 >= 1)) && (i2 < 57)), placeholder[i0, (i1 - 1), (i2 - 1), i3], 0f)\n",
      "input_tile(eps, nu, p, ci) = data_pad[floordiv(p, 196), ((floormod(floordiv(p, 14), 14)*4) + eps), ((floormod(p, 14)*4) + nu), ci]\n",
      "B(i, j) = select(((floormod(i, 6) == 5) && (floormod(j, 6) == 5)), 1f, select(((floormod(i, 6) == 5) && (floormod(j, 6) == 4)),  ..(OMITTED)..  (floormod(j, 6) == 1)), 0f, select(((floormod(i, 6) == 0) && (floormod(j, 6) == 0)), 1f, 0f))))))))))))))))))))))))))))))))))))\n",
      "data_pack(eps, nu, p, ci) += ((input_tile[r_a, r_b, p, ci]*B[r_a, eps])*B[r_b, nu])\n",
      "placeholder = PLACEHOLDER [6, 6, 64, 64]\n",
      "bgemm(eps, nu, p, co) += (data_pack[eps, nu, p, ci]*placeholder[eps, nu, co, ci])\n",
      "A(i, j) = select(((floormod(i, 6) == 5) && (floormod(j, 4) == 3)), 1f, select(((floormod(i, 6) == 5) && (floormod(j, 4) == 2)),  ..(OMITTED)..  6) == 0) && (floormod(j, 4) == 1)), 0f, select(((floormod(i, 6) == 0) && (floormod(j, 4) == 0)), 1f, 0f))))))))))))))))))))))))\n",
      "inverse(vh, vw, p, co) += ((bgemm[r_a, r_b, p, co]*A[r_a, vh])*A[r_b, vw])\n",
      "conv2d_winograd(n, h, w, co) = inverse[floormod(h, 4), floormod(w, 4), ((((n*14)*14) + (floordiv(h, 4)*14)) + floordiv(w, 4)), co]\n",
      "placeholder = PLACEHOLDER [1, 1, 1, 64]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_winograd[ax0, ax1, ax2, ax3] + placeholder[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 7  (workload key: [\"dac19035dd5fe9424ee8617421b9c817\", [1, 28, 28, 128], [4, 4, 128, 128], [1, 28, 28, 128], [1, 28, 28, 128]]) ==========\n",
      "placeholder = PLACEHOLDER [1, 28, 28, 128]\n",
      "data_pad(i0, i1, i2, i3) = tir.if_then_else(((((i1 >= 1) && (i1 < 29)) && (i2 >= 1)) && (i2 < 29)), placeholder[i0, (i1 - 1), (i2 - 1), i3], 0f)\n",
      "input_tile(eps, nu, p, ci) = data_pad[floordiv(p, 196), ((floormod(floordiv(p, 14), 14)*2) + eps), ((floormod(p, 14)*2) + nu), ci]\n",
      "B(i, j) = select(((floormod(i, 4) == 3) && (floormod(j, 4) == 3)), 1f, select(((floormod(i, 4) == 3) && (floormod(j, 4) == 2)),  ..(OMITTED).. ormod(i, 4) == 0) && (floormod(j, 4) == 1)), 0f, select(((floormod(i, 4) == 0) && (floormod(j, 4) == 0)), 1f, 0f))))))))))))))))\n",
      "data_pack(eps, nu, p, ci) += ((input_tile[r_a, r_b, p, ci]*B[r_a, eps])*B[r_b, nu])\n",
      "placeholder = PLACEHOLDER [4, 4, 128, 128]\n",
      "bgemm(eps, nu, p, co) += (data_pack[eps, nu, p, ci]*placeholder[eps, nu, co, ci])\n",
      "A(i, j) = select(((floormod(i, 4) == 3) && (floormod(j, 2) == 1)), 1f, select(((floormod(i, 4) == 3) && (floormod(j, 2) == 0)),  ..(OMITTED).. ct(((floormod(i, 4) == 0) && (floormod(j, 2) == 1)), 0f, select(((floormod(i, 4) == 0) && (floormod(j, 2) == 0)), 1f, 0f))))))))\n",
      "inverse(vh, vw, p, co) += ((bgemm[r_a, r_b, p, co]*A[r_a, vh])*A[r_b, vw])\n",
      "conv2d_winograd(n, h, w, co) = inverse[floormod(h, 2), floormod(w, 2), ((((n*14)*14) + (floordiv(h, 2)*14)) + floordiv(w, 2)), co]\n",
      "placeholder = PLACEHOLDER [1, 28, 28, 128]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_winograd[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, ax2, ax3])\n",
      "\n",
      "========== Task 8  (workload key: [\"7006235cfc29b73be524cf390ed5a977\", [1, 56, 56, 64], [1, 1, 64, 64], [1, 56, 56, 64]]) ==========\n",
      "placeholder = PLACEHOLDER [1, 56, 56, 64]\n",
      "PaddedInput(i0, i1, i2, i3) = placeholder[i0, i1, i2, i3]\n",
      "placeholder = PLACEHOLDER [1, 1, 64, 64]\n",
      "Conv2dOutput(nn, yy, xx, ff) += (PaddedInput[nn, (yy + ry), (xx + rx), rc]*placeholder[ry, rx, rc, ff])\n",
      "\n",
      "========== Task 9  (workload key: [\"1e3c4211ffd2f2db91078ae4d04b779d\", [1, 56, 56, 64], [6, 6, 64, 64], [1, 56, 56, 64], [1, 1, 1, 64], [1, 56, 56, 64]]) ==========\n",
      "placeholder = PLACEHOLDER [1, 56, 56, 64]\n",
      "data_pad(i0, i1, i2, i3) = tir.if_then_else(((((i1 >= 1) && (i1 < 57)) && (i2 >= 1)) && (i2 < 57)), placeholder[i0, (i1 - 1), (i2 - 1), i3], 0f)\n",
      "input_tile(eps, nu, p, ci) = data_pad[floordiv(p, 196), ((floormod(floordiv(p, 14), 14)*4) + eps), ((floormod(p, 14)*4) + nu), ci]\n",
      "B(i, j) = select(((floormod(i, 6) == 5) && (floormod(j, 6) == 5)), 1f, select(((floormod(i, 6) == 5) && (floormod(j, 6) == 4)),  ..(OMITTED)..  (floormod(j, 6) == 1)), 0f, select(((floormod(i, 6) == 0) && (floormod(j, 6) == 0)), 1f, 0f))))))))))))))))))))))))))))))))))))\n",
      "data_pack(eps, nu, p, ci) += ((input_tile[r_a, r_b, p, ci]*B[r_a, eps])*B[r_b, nu])\n",
      "placeholder = PLACEHOLDER [6, 6, 64, 64]\n",
      "bgemm(eps, nu, p, co) += (data_pack[eps, nu, p, ci]*placeholder[eps, nu, co, ci])\n",
      "A(i, j) = select(((floormod(i, 6) == 5) && (floormod(j, 4) == 3)), 1f, select(((floormod(i, 6) == 5) && (floormod(j, 4) == 2)),  ..(OMITTED)..  6) == 0) && (floormod(j, 4) == 1)), 0f, select(((floormod(i, 6) == 0) && (floormod(j, 4) == 0)), 1f, 0f))))))))))))))))))))))))\n",
      "inverse(vh, vw, p, co) += ((bgemm[r_a, r_b, p, co]*A[r_a, vh])*A[r_b, vw])\n",
      "conv2d_winograd(n, h, w, co) = inverse[floormod(h, 4), floormod(w, 4), ((((n*14)*14) + (floordiv(h, 4)*14)) + floordiv(w, 4)), co]\n",
      "placeholder = PLACEHOLDER [1, 56, 56, 64]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_winograd[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, ax2, ax3])\n",
      "placeholder = PLACEHOLDER [1, 1, 1, 64]\n",
      "T_add(ax0, ax1, ax2, ax3) = (T_add[ax0, ax1, ax2, ax3] + placeholder[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 10  (workload key: [\"12b88bedece6984af589a28b43e0f3c4\", [1, 28, 28, 128], [3, 3, 128, 256], [1, 1, 1, 256], [1, 14, 14, 256]]) ==========\n",
      "placeholder = PLACEHOLDER [1, 28, 28, 128]\n",
      "PaddedInput(i0, i1, i2, i3) = tir.if_then_else(((((i1 >= 1) && (i1 < 29)) && (i2 >= 1)) && (i2 < 29)), placeholder[i0, (i1 - 1), (i2 - 1), i3], 0f)\n",
      "placeholder = PLACEHOLDER [3, 3, 128, 256]\n",
      "Conv2dOutput(nn, yy, xx, ff) += (PaddedInput[nn, ((yy*2) + ry), ((xx*2) + rx), rc]*placeholder[ry, rx, rc, ff])\n",
      "placeholder = PLACEHOLDER [1, 1, 1, 256]\n",
      "T_add(ax0, ax1, ax2, ax3) = (Conv2dOutput[ax0, ax1, ax2, ax3] + placeholder[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 11  (workload key: [\"f3b6c10fcc6ce01ff01add933e4d21e9\", [1, 14, 14, 256], [4, 4, 256, 256], [1, 14, 14, 256], [1, 1, 1, 256], [1, 14, 14, 256]]) ==========\n",
      "placeholder = PLACEHOLDER [1, 14, 14, 256]\n",
      "data_pad(i0, i1, i2, i3) = tir.if_then_else(((((i1 >= 1) && (i1 < 15)) && (i2 >= 1)) && (i2 < 15)), placeholder[i0, (i1 - 1), (i2 - 1), i3], 0f)\n",
      "input_tile(eps, nu, p, ci) = data_pad[floordiv(p, 49), ((floormod(floordiv(p, 7), 7)*2) + eps), ((floormod(p, 7)*2) + nu), ci]\n",
      "B(i, j) = select(((floormod(i, 4) == 3) && (floormod(j, 4) == 3)), 1f, select(((floormod(i, 4) == 3) && (floormod(j, 4) == 2)),  ..(OMITTED).. ormod(i, 4) == 0) && (floormod(j, 4) == 1)), 0f, select(((floormod(i, 4) == 0) && (floormod(j, 4) == 0)), 1f, 0f))))))))))))))))\n",
      "data_pack(eps, nu, p, ci) += ((input_tile[r_a, r_b, p, ci]*B[r_a, eps])*B[r_b, nu])\n",
      "placeholder = PLACEHOLDER [4, 4, 256, 256]\n",
      "bgemm(eps, nu, p, co) += (data_pack[eps, nu, p, ci]*placeholder[eps, nu, co, ci])\n",
      "A(i, j) = select(((floormod(i, 4) == 3) && (floormod(j, 2) == 1)), 1f, select(((floormod(i, 4) == 3) && (floormod(j, 2) == 0)),  ..(OMITTED).. ct(((floormod(i, 4) == 0) && (floormod(j, 2) == 1)), 0f, select(((floormod(i, 4) == 0) && (floormod(j, 2) == 0)), 1f, 0f))))))))\n",
      "inverse(vh, vw, p, co) += ((bgemm[r_a, r_b, p, co]*A[r_a, vh])*A[r_b, vw])\n",
      "conv2d_winograd(n, h, w, co) = inverse[floormod(h, 2), floormod(w, 2), ((((n*7)*7) + (floordiv(h, 2)*7)) + floordiv(w, 2)), co]\n",
      "placeholder = PLACEHOLDER [1, 14, 14, 256]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_winograd[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, ax2, ax3])\n",
      "placeholder = PLACEHOLDER [1, 1, 1, 256]\n",
      "T_add(ax0, ax1, ax2, ax3) = (T_add[ax0, ax1, ax2, ax3] + placeholder[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 12  (workload key: [\"b8b52b9be9df6102466a22a014c44c1f\", [1, 14, 14, 256], [4, 4, 256, 256], [1, 1, 1, 256], [1, 14, 14, 256]]) ==========\n",
      "placeholder = PLACEHOLDER [1, 14, 14, 256]\n",
      "data_pad(i0, i1, i2, i3) = tir.if_then_else(((((i1 >= 1) && (i1 < 15)) && (i2 >= 1)) && (i2 < 15)), placeholder[i0, (i1 - 1), (i2 - 1), i3], 0f)\n",
      "input_tile(eps, nu, p, ci) = data_pad[floordiv(p, 49), ((floormod(floordiv(p, 7), 7)*2) + eps), ((floormod(p, 7)*2) + nu), ci]\n",
      "B(i, j) = select(((floormod(i, 4) == 3) && (floormod(j, 4) == 3)), 1f, select(((floormod(i, 4) == 3) && (floormod(j, 4) == 2)),  ..(OMITTED).. ormod(i, 4) == 0) && (floormod(j, 4) == 1)), 0f, select(((floormod(i, 4) == 0) && (floormod(j, 4) == 0)), 1f, 0f))))))))))))))))\n",
      "data_pack(eps, nu, p, ci) += ((input_tile[r_a, r_b, p, ci]*B[r_a, eps])*B[r_b, nu])\n",
      "placeholder = PLACEHOLDER [4, 4, 256, 256]\n",
      "bgemm(eps, nu, p, co) += (data_pack[eps, nu, p, ci]*placeholder[eps, nu, co, ci])\n",
      "A(i, j) = select(((floormod(i, 4) == 3) && (floormod(j, 2) == 1)), 1f, select(((floormod(i, 4) == 3) && (floormod(j, 2) == 0)),  ..(OMITTED).. ct(((floormod(i, 4) == 0) && (floormod(j, 2) == 1)), 0f, select(((floormod(i, 4) == 0) && (floormod(j, 2) == 0)), 1f, 0f))))))))\n",
      "inverse(vh, vw, p, co) += ((bgemm[r_a, r_b, p, co]*A[r_a, vh])*A[r_b, vw])\n",
      "conv2d_winograd(n, h, w, co) = inverse[floormod(h, 2), floormod(w, 2), ((((n*7)*7) + (floordiv(h, 2)*7)) + floordiv(w, 2)), co]\n",
      "placeholder = PLACEHOLDER [1, 1, 1, 256]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_winograd[ax0, ax1, ax2, ax3] + placeholder[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 13  (workload key: [\"f4380bb1dc62422a69ad4a1a9771f927\", [1, 14, 14, 256], [1, 1, 256, 512], [1, 7, 7, 512]]) ==========\n",
      "placeholder = PLACEHOLDER [1, 14, 14, 256]\n",
      "PaddedInput(i0, i1, i2, i3) = placeholder[i0, i1, i2, i3]\n",
      "placeholder = PLACEHOLDER [1, 1, 256, 512]\n",
      "Conv2dOutput(nn, yy, xx, ff) += (PaddedInput[nn, ((yy*2) + ry), ((xx*2) + rx), rc]*placeholder[ry, rx, rc, ff])\n",
      "\n",
      "========== Task 14  (workload key: [\"f4380bb1dc62422a69ad4a1a9771f927\", [1, 56, 56, 64], [1, 1, 64, 128], [1, 28, 28, 128]]) ==========\n",
      "placeholder = PLACEHOLDER [1, 56, 56, 64]\n",
      "PaddedInput(i0, i1, i2, i3) = placeholder[i0, i1, i2, i3]\n",
      "placeholder = PLACEHOLDER [1, 1, 64, 128]\n",
      "Conv2dOutput(nn, yy, xx, ff) += (PaddedInput[nn, ((yy*2) + ry), ((xx*2) + rx), rc]*placeholder[ry, rx, rc, ff])\n",
      "\n",
      "========== Task 15  (workload key: [\"7d44c6e3c81cd80f61ff2265b2bae89a\", [1, 512], [1000, 512], [1, 1000], [1, 1000]]) ==========\n",
      "placeholder = PLACEHOLDER [1, 512]\n",
      "placeholder = PLACEHOLDER [1000, 512]\n",
      "T_matmul_NT(i, j) += (placeholder[i, k]*placeholder[j, k])\n",
      "placeholder = PLACEHOLDER [1, 1000]\n",
      "T_add(ax0, ax1) = (T_matmul_NT[ax0, ax1] + placeholder[ax0, ax1])\n",
      "\n",
      "========== Task 16  (workload key: [\"d7b65649a4dd54becea0a52aabbc5af5\", [1, 1000], [1, 1000]]) ==========\n",
      "placeholder = PLACEHOLDER [1, 1000]\n",
      "T_softmax_maxelem(i0) max= placeholder[i0, k]\n",
      "T_softmax_exp(i0, i1) = tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))\n",
      "T_softmax_expsum(i0) += T_softmax_exp[i0, k]\n",
      "T_softmax_norm(i0, i1) = (T_softmax_exp[i0, i1]/T_softmax_expsum[i0])\n",
      "\n",
      "========== Task 17  (workload key: [\"3ea73fb9b0364374730d09e068821f95\", [1, 56, 56, 64], [6, 6, 64, 64], [1, 56, 56, 64], [1, 56, 56, 64]]) ==========\n",
      "placeholder = PLACEHOLDER [1, 56, 56, 64]\n",
      "data_pad(i0, i1, i2, i3) = tir.if_then_else(((((i1 >= 1) && (i1 < 57)) && (i2 >= 1)) && (i2 < 57)), placeholder[i0, (i1 - 1), (i2 - 1), i3], 0f)\n",
      "input_tile(eps, nu, p, ci) = data_pad[floordiv(p, 196), ((floormod(floordiv(p, 14), 14)*4) + eps), ((floormod(p, 14)*4) + nu), ci]\n",
      "B(i, j) = select(((floormod(i, 6) == 5) && (floormod(j, 6) == 5)), 1f, select(((floormod(i, 6) == 5) && (floormod(j, 6) == 4)),  ..(OMITTED)..  (floormod(j, 6) == 1)), 0f, select(((floormod(i, 6) == 0) && (floormod(j, 6) == 0)), 1f, 0f))))))))))))))))))))))))))))))))))))\n",
      "data_pack(eps, nu, p, ci) += ((input_tile[r_a, r_b, p, ci]*B[r_a, eps])*B[r_b, nu])\n",
      "placeholder = PLACEHOLDER [6, 6, 64, 64]\n",
      "bgemm(eps, nu, p, co) += (data_pack[eps, nu, p, ci]*placeholder[eps, nu, co, ci])\n",
      "A(i, j) = select(((floormod(i, 6) == 5) && (floormod(j, 4) == 3)), 1f, select(((floormod(i, 6) == 5) && (floormod(j, 4) == 2)),  ..(OMITTED)..  6) == 0) && (floormod(j, 4) == 1)), 0f, select(((floormod(i, 6) == 0) && (floormod(j, 4) == 0)), 1f, 0f))))))))))))))))))))))))\n",
      "inverse(vh, vw, p, co) += ((bgemm[r_a, r_b, p, co]*A[r_a, vh])*A[r_b, vw])\n",
      "conv2d_winograd(n, h, w, co) = inverse[floormod(h, 4), floormod(w, 4), ((((n*14)*14) + (floordiv(h, 4)*14)) + floordiv(w, 4)), co]\n",
      "placeholder = PLACEHOLDER [1, 56, 56, 64]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_winograd[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, ax2, ax3])\n",
      "\n",
      "========== Task 18  (workload key: [\"64b98c71af70a904fdbb81d7d4188d84\", [1, 112, 112, 64], [1, 1, 1, 64], [1, 56, 56, 64]]) ==========\n",
      "placeholder = PLACEHOLDER [1, 112, 112, 64]\n",
      "pad_temp(ax0, ax1, ax2, ax3) = tir.if_then_else(((((ax1 >= 1) && (ax1 < 113)) && (ax2 >= 1)) && (ax2 < 113)), placeholder[ax0, (ax1 - 1), (ax2 - 1), ax3], -3.40282e+38f)\n",
      "tensor(ax0, ax1, ax2, ax3) max= pad_temp[ax0, ((ax1*2) + rv0), ((ax2*2) + rv1), ax3]\n",
      "placeholder = PLACEHOLDER [1, 1, 1, 64]\n",
      "T_add(ax0, ax1, ax2, ax3) = (tensor[ax0, ax1, ax2, ax3] + placeholder[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 19  (workload key: [\"ad6cecbf5d85cb1cda3c2bb7af170211\", [1, 7, 7, 512], [4, 4, 512, 512], [1, 7, 7, 512], [1, 1, 1, 512], [1, 1, 1, 512], [1, 7, 7, 512]]) ==========\n",
      "placeholder = PLACEHOLDER [1, 7, 7, 512]\n",
      "data_pad(i0, i1, i2, i3) = tir.if_then_else(((((i1 >= 1) && (i1 < 8)) && (i2 >= 1)) && (i2 < 8)), placeholder[i0, (i1 - 1), (i2 - 1), i3], 0f)\n",
      "input_tile(eps, nu, p, ci) = data_pad[floordiv(p, 16), ((floormod(floordiv(p, 4), 4)*2) + eps), ((floormod(p, 4)*2) + nu), ci]\n",
      "B(i, j) = select(((floormod(i, 4) == 3) && (floormod(j, 4) == 3)), 1f, select(((floormod(i, 4) == 3) && (floormod(j, 4) == 2)),  ..(OMITTED).. ormod(i, 4) == 0) && (floormod(j, 4) == 1)), 0f, select(((floormod(i, 4) == 0) && (floormod(j, 4) == 0)), 1f, 0f))))))))))))))))\n",
      "data_pack(eps, nu, p, ci) += ((input_tile[r_a, r_b, p, ci]*B[r_a, eps])*B[r_b, nu])\n",
      "placeholder = PLACEHOLDER [4, 4, 512, 512]\n",
      "bgemm(eps, nu, p, co) += (data_pack[eps, nu, p, ci]*placeholder[eps, nu, co, ci])\n",
      "A(i, j) = select(((floormod(i, 4) == 3) && (floormod(j, 2) == 1)), 1f, select(((floormod(i, 4) == 3) && (floormod(j, 2) == 0)),  ..(OMITTED).. ct(((floormod(i, 4) == 0) && (floormod(j, 2) == 1)), 0f, select(((floormod(i, 4) == 0) && (floormod(j, 2) == 0)), 1f, 0f))))))))\n",
      "inverse(vh, vw, p, co) += ((bgemm[r_a, r_b, p, co]*A[r_a, vh])*A[r_b, vw])\n",
      "conv2d_winograd(n, h, w, co) = inverse[floormod(h, 2), floormod(w, 2), ((((n*4)*4) + (floordiv(h, 2)*4)) + floordiv(w, 2)), co]\n",
      "placeholder = PLACEHOLDER [1, 7, 7, 512]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_winograd[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, ax2, ax3])\n",
      "placeholder = PLACEHOLDER [1, 1, 1, 512]\n",
      "T_multiply(ax0, ax1, ax2, ax3) = (T_add[ax0, ax1, ax2, ax3]*placeholder[ax0, 0, 0, ax3])\n",
      "placeholder = PLACEHOLDER [1, 1, 1, 512]\n",
      "T_add(ax0, ax1, ax2, ax3) = (T_multiply[ax0, ax1, ax2, ax3] + placeholder[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "========== Task 20  (workload key: [\"d730bcd28f0920f6b97245e2a11bd8d6\", [1, 7, 7, 512], [4, 4, 512, 512], [1, 7, 7, 512], [1, 7, 7, 512]]) ==========\n",
      "placeholder = PLACEHOLDER [1, 7, 7, 512]\n",
      "data_pad(i0, i1, i2, i3) = tir.if_then_else(((((i1 >= 1) && (i1 < 8)) && (i2 >= 1)) && (i2 < 8)), placeholder[i0, (i1 - 1), (i2 - 1), i3], 0f)\n",
      "input_tile(eps, nu, p, ci) = data_pad[floordiv(p, 16), ((floormod(floordiv(p, 4), 4)*2) + eps), ((floormod(p, 4)*2) + nu), ci]\n",
      "B(i, j) = select(((floormod(i, 4) == 3) && (floormod(j, 4) == 3)), 1f, select(((floormod(i, 4) == 3) && (floormod(j, 4) == 2)),  ..(OMITTED).. ormod(i, 4) == 0) && (floormod(j, 4) == 1)), 0f, select(((floormod(i, 4) == 0) && (floormod(j, 4) == 0)), 1f, 0f))))))))))))))))\n",
      "data_pack(eps, nu, p, ci) += ((input_tile[r_a, r_b, p, ci]*B[r_a, eps])*B[r_b, nu])\n",
      "placeholder = PLACEHOLDER [4, 4, 512, 512]\n",
      "bgemm(eps, nu, p, co) += (data_pack[eps, nu, p, ci]*placeholder[eps, nu, co, ci])\n",
      "A(i, j) = select(((floormod(i, 4) == 3) && (floormod(j, 2) == 1)), 1f, select(((floormod(i, 4) == 3) && (floormod(j, 2) == 0)),  ..(OMITTED).. ct(((floormod(i, 4) == 0) && (floormod(j, 2) == 1)), 0f, select(((floormod(i, 4) == 0) && (floormod(j, 2) == 0)), 1f, 0f))))))))\n",
      "inverse(vh, vw, p, co) += ((bgemm[r_a, r_b, p, co]*A[r_a, vh])*A[r_b, vw])\n",
      "conv2d_winograd(n, h, w, co) = inverse[floormod(h, 2), floormod(w, 2), ((((n*4)*4) + (floordiv(h, 2)*4)) + floordiv(w, 2)), co]\n",
      "placeholder = PLACEHOLDER [1, 7, 7, 512]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_winograd[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, ax2, ax3])\n",
      "\n",
      "========== Task 21  (workload key: [\"d374e472bd9d8164892b9e28a0a8cb59\", [1, 14, 14, 256], [4, 4, 256, 256], [1, 14, 14, 256], [1, 14, 14, 256]]) ==========\n",
      "placeholder = PLACEHOLDER [1, 14, 14, 256]\n",
      "data_pad(i0, i1, i2, i3) = tir.if_then_else(((((i1 >= 1) && (i1 < 15)) && (i2 >= 1)) && (i2 < 15)), placeholder[i0, (i1 - 1), (i2 - 1), i3], 0f)\n",
      "input_tile(eps, nu, p, ci) = data_pad[floordiv(p, 49), ((floormod(floordiv(p, 7), 7)*2) + eps), ((floormod(p, 7)*2) + nu), ci]\n",
      "B(i, j) = select(((floormod(i, 4) == 3) && (floormod(j, 4) == 3)), 1f, select(((floormod(i, 4) == 3) && (floormod(j, 4) == 2)),  ..(OMITTED).. ormod(i, 4) == 0) && (floormod(j, 4) == 1)), 0f, select(((floormod(i, 4) == 0) && (floormod(j, 4) == 0)), 1f, 0f))))))))))))))))\n",
      "data_pack(eps, nu, p, ci) += ((input_tile[r_a, r_b, p, ci]*B[r_a, eps])*B[r_b, nu])\n",
      "placeholder = PLACEHOLDER [4, 4, 256, 256]\n",
      "bgemm(eps, nu, p, co) += (data_pack[eps, nu, p, ci]*placeholder[eps, nu, co, ci])\n",
      "A(i, j) = select(((floormod(i, 4) == 3) && (floormod(j, 2) == 1)), 1f, select(((floormod(i, 4) == 3) && (floormod(j, 2) == 0)),  ..(OMITTED).. ct(((floormod(i, 4) == 0) && (floormod(j, 2) == 1)), 0f, select(((floormod(i, 4) == 0) && (floormod(j, 2) == 0)), 1f, 0f))))))))\n",
      "inverse(vh, vw, p, co) += ((bgemm[r_a, r_b, p, co]*A[r_a, vh])*A[r_b, vw])\n",
      "conv2d_winograd(n, h, w, co) = inverse[floormod(h, 2), floormod(w, 2), ((((n*7)*7) + (floordiv(h, 2)*7)) + floordiv(w, 2)), co]\n",
      "placeholder = PLACEHOLDER [1, 14, 14, 256]\n",
      "T_add(ax0, ax1, ax2, ax3) = (conv2d_winograd[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, ax2, ax3])\n",
      "\n",
      "========== Task 22  (workload key: [\"f4380bb1dc62422a69ad4a1a9771f927\", [1, 28, 28, 128], [1, 1, 128, 256], [1, 14, 14, 256]]) ==========\n",
      "placeholder = PLACEHOLDER [1, 28, 28, 128]\n",
      "PaddedInput(i0, i1, i2, i3) = placeholder[i0, i1, i2, i3]\n",
      "placeholder = PLACEHOLDER [1, 1, 128, 256]\n",
      "Conv2dOutput(nn, yy, xx, ff) += (PaddedInput[nn, ((yy*2) + ry), ((xx*2) + rx), rc]*placeholder[ry, rx, rc, ff])\n",
      "\n",
      "========== Task 23  (workload key: [\"12b88bedece6984af589a28b43e0f3c4\", [1, 224, 224, 3], [7, 7, 3, 64], [1, 1, 1, 64], [1, 112, 112, 64]]) ==========\n",
      "placeholder = PLACEHOLDER [1, 224, 224, 3]\n",
      "PaddedInput(i0, i1, i2, i3) = tir.if_then_else(((((i1 >= 3) && (i1 < 227)) && (i2 >= 3)) && (i2 < 227)), placeholder[i0, (i1 - 3), (i2 - 3), i3], 0f)\n",
      "placeholder = PLACEHOLDER [7, 7, 3, 64]\n",
      "Conv2dOutput(nn, yy, xx, ff) += (PaddedInput[nn, ((yy*2) + ry), ((xx*2) + rx), rc]*placeholder[ry, rx, rc, ff])\n",
      "placeholder = PLACEHOLDER [1, 1, 1, 64]\n",
      "T_add(ax0, ax1, ax2, ax3) = (Conv2dOutput[ax0, ax1, ax2, ax3] + placeholder[ax0, 0, 0, ax3])\n",
      "T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Extract tasks from the network\n",
    "print(\"Extract tasks...\")\n",
    "mod, params, input_shape, output_shape = get_network(network, batch_size, layout, dtype=dtype)\n",
    "tasks, task_weights = auto_scheduler.extract_tasks(mod[\"main\"], params, target)\n",
    "\n",
    "for idx, task in enumerate(tasks):\n",
    "    print(\"========== Task %d  (workload key: %s) ==========\" % (idx, task.workload_key))\n",
    "    print(task.compute_dag)\n",
    "print(task_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin tuning...\n",
      "Get devices for measurement successfully!\n",
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |----------------------------------------------------------------------\n",
      "\n",
      "-------------------------------------------------\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "|    0 |            - |              - |      0 |\n",
      "|    1 |            - |              - |      0 |\n",
      "|    2 |            - |              - |      0 |\n",
      "|    3 |            - |              - |      0 |\n",
      "|    4 |            - |              - |      0 |\n",
      "|    5 |            - |              - |      0 |\n",
      "|    6 |            - |              - |      0 |\n",
      "|    7 |            - |              - |      0 |\n",
      "|    8 |            - |              - |      0 |\n",
      "|    9 |            - |              - |      0 |\n",
      "|   10 |            - |              - |      0 |\n",
      "|   11 |            - |              - |      0 |\n",
      "|   12 |            - |              - |      0 |\n",
      "|   13 |            - |              - |      0 |\n",
      "|   14 |            - |              - |      0 |\n",
      "|   15 |            - |              - |      0 |\n",
      "|   16 |            - |              - |      0 |\n",
      "|   17 |            - |              - |      0 |\n",
      "|   18 |            - |              - |      0 |\n",
      "|   19 |            - |              - |      0 |\n",
      "|   20 |            - |              - |      0 |\n",
      "|   21 |            - |              - |      0 |\n",
      "|   22 |            - |              - |      0 |\n",
      "|   23 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 0\tUsed time : 0 s\tNext ID: 0\t\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 1\n",
      "Sample Initial Population\t#s: 97\tfail_ct: 1951\tTime elapsed: 1.08\n",
      "GA Iter: 0\tMax score: 0.9975\tMin score: 0.7503\t#Pop: 16\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 0.9996\tMin score: 0.9970\t#Pop: 16\t#M+: 1398\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 16\tTime elapsed: 14.78\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 8 programs to measure:\n",
      ".E.E.E.E.E.E.E.E\n",
      "Time elapsed for measurement: 1.50 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n",
      "Time elapsed for training|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |: 0.10 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "-------------------------------------------------\n",
      "|    0 |            - |              - |      8 |\n",
      "|    1 |            - |              - |      0 |\n",
      "|    2 |            - |              - |      0 |\n",
      "|    3 |            - |              - |      0 |\n",
      "|    4 |            - |              - |      0 |\n",
      "|    5 |            - |              - |      0 |\n",
      "|    6 |            - |              - |      0 |\n",
      "|    7 |            - |              - |      0 |\n",
      "|    8 |            - |              - |      0 |\n",
      "|    9 |            - |              - |      0 |\n",
      "|   10 |            - |              - |      0 |\n",
      "|   11 |            - |              - |      0 |\n",
      "|   12 |            - |              - |      0 |\n",
      "|   13 |            - |              - |      0 |\n",
      "|   14 |            - |              - |      0 |\n",
      "|   15 |            - |              - |      0 |\n",
      "|   16 |            - |              - |      0 |\n",
      "|   17 |            - |              - |      0 |\n",
      "|   18 |            - |              - |      0 |\n",
      "|   19 |            - |              - |      0 |\n",
      "|   20 |            - |              - |      0 |\n",
      "|   21 |            - |              - |      0 |\n",
      "|   22 |            - |              - |      0 |\n",
      "|   23 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 8\tUsed time : 18 s\tNext ID: 1\t\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 1\n",
      "Sample Initial Population\t#s: 86\tfail_ct: 1962\tTime elapsed: 0.89\n",
      "GA Iter: 0\tMax score: 0.9899\tMin score: 0.8054\t#Pop: 16\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 1.0000\tMin score: 0.9981\t#Pop: 16\t#M+: 1389\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 16\tTime elapsed: 12.59\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 8 programs to measure:\n",
      ".E.E.E.E.E.E.E.E\n",
      "Time elapsed for measurement: 1.54 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n",
      "Time elapsed for training: 0.10 s\n",
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |----------------------------------------------------------------------\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "-------------------------------------------------\n",
      "|    0 |            - |              - |      8 |\n",
      "|    1 |            - |              - |      8 |\n",
      "|    2 |            - |              - |      0 |\n",
      "|    3 |            - |              - |      0 |\n",
      "|    4 |            - |              - |      0 |\n",
      "|    5 |            - |              - |      0 |\n",
      "|    6 |            - |              - |      0 |\n",
      "|    7 |            - |              - |      0 |\n",
      "|    8 |            - |              - |      0 |\n",
      "|    9 |            - |              - |      0 |\n",
      "|   10 |            - |              - |      0 |\n",
      "|   11 |            - |              - |      0 |\n",
      "|   12 |            - |              - |      0 |\n",
      "|   13 |            - |              - |      0 |\n",
      "|   14 |            - |              - |      0 |\n",
      "|   15 |            - |              - |      0 |\n",
      "|   16 |            - |              - |      0 |\n",
      "|   17 |            - |              - |      0 |\n",
      "|   18 |            - |              - |      0 |\n",
      "|   19 |            - |              - |      0 |\n",
      "|   20 |            - |              - |      0 |\n",
      "|   21 |            - |              - |      0 |\n",
      "|   22 |            - |              - |      0 |\n",
      "|   23 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 16\tUsed time : 33 s\tNext ID: 2\t\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 1\n",
      "Sample Initial Population\t#s: 77\tfail_ct: 1971\tTime elapsed: 2.03\n",
      "GA Iter: 0\tMax score: 0.9819\tMin score: 0.7866\t#Pop: 16\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 0.9999\tMin score: 0.9977\t#Pop: 16\t#M+: 1390\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 16\tTime elapsed: 31.81\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 8 programs to measure:\n",
      ".E.E.E.E.E.E.E.E\n",
      "Time elapsed for measurement: 1.86 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n",
      "Time elapsed for training: 0.20 s\n",
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |\n",
      "-------------------------------------------------\n",
      "|    0 |            - |              - |      8 |\n",
      "|    1 |            - |              - |      8 |\n",
      "|    2 |            - |              - |      8 |\n",
      "|    3 |            - |              - |      0 |\n",
      "|    4 |            - |              - |      0 |\n",
      "|    5 |            - |              - |      0 |\n",
      "|    6 |            - |              - |      0 |\n",
      "|    7 |            - |              - |      0 |\n",
      "|    8 |            - |              - |      0 |\n",
      "|    9 |            - |              - |      0 |\n",
      "|   10 |            - |              - |      0 |\n",
      "|   11 |            - |              - |      0 |\n",
      "|   12 |            - |              - |      0 |\n",
      "|   13 |            - |              - |      0 |\n",
      "|   14 |            - |              - |      0 |\n",
      "|   15 |            - |              - |      0 |\n",
      "|   16 |            - |              - |      0 |\n",
      "|   17 |            - |              - |      0 |\n",
      "|   18 |            - |              - |      0 |\n",
      "|   19 |            - |              - |      0 |\n",
      "|   20 |            - |              - |      0 |\n",
      "|   21 |            - |              - |      0 |\n",
      "|   22 |            - |              - |      0 |\n",
      "|   23 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 24\tUsed time : 69 s\tNext ID: 3\t\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 1\n",
      "Sample Initial Population\t#s: 85\tfail_ct: 1963\tTime elapsed: 2.55\n",
      "GA Iter: 0\tMax score: 0.9871\tMin score: 0.8308\t#Pop: 16\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 0.9997\tMin score: 0.9980\t#Pop: 16\t#M+: 1389\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 16\tTime elapsed: 33.16\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 8 programs to measure:\n",
      ".E.E.E.E.E.E.E.E\n",
      "Time elapsed for measurement: 1.88 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n",
      "Time elapsed for training: |  ID  | Latency (ms) | Speed (GFLOPS) | Trials |\n",
      "-------------------------------------------------\n",
      "|    0 |            - |              - |      8 |\n",
      "|    1 |            - |              - |      8 |\n",
      "|    2 |            - |              - |      8 |\n",
      "|    3 |            - |              - |      8 |\n",
      "|    4 |            - |              - |      0 |\n",
      "|    5 |            - |              - |      0 |\n",
      "|    6 |            - |              - |      0 |\n",
      "|    7 |            - |              - |      0 |\n",
      "|    8 |            - |              - |      0 |\n",
      "|    9 |            - |              - |      0 |\n",
      "|   10 |            - |              - |      0 |\n",
      "|   11 |            - |              - |      0 |\n",
      "|   12 |            - |              - |      0 |\n",
      "|   13 |            - |              - |      0 |\n",
      "|   14 |            - |              - |      0 |\n",
      "|   15 |            - |              - |      0 |\n",
      "|   16 |            - |              - |      0 |\n",
      "|   17 |            - |              - |      0 |\n",
      "|   18 |            - |              - |      0 |\n",
      "|   19 |            - |              - |      0 |\n",
      "|   20 |            - |              - |      0 |\n",
      "|   21 |            - |              - |      0 |\n",
      "|   22 |            - |              - |      0 |\n",
      "|   23 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 32\tUsed time : 107 s\tNext ID: 4\t\n",
      "0.22 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 1\n",
      "Sample Iter: 5\t#Pop: 5\t#Target: 50\tfail_ct: 10235\tTime elapsed: 3.54\n",
      "#Target has been reduced to 25 due to too many failures or duplications\n",
      "Sample Iter: 10\t#Pop: 5\t#Target: 25\tfail_ct: 20475\tTime elapsed: 7.04\n",
      "#Target has been reduced to 12 due to too many failures or duplications\n",
      "Sample Iter: 15\t#Pop: 5\t#Target: 12\tfail_ct: 30715\tTime elapsed: 10.54\n",
      "#Target has been reduced to 6 due to too many failures or duplications\n",
      "Sample Iter: 20\t#Pop: 5\t#Target: 6\tfail_ct: 40955\tTime elapsed: 14.06\n",
      "#Target has been reduced to 3 due to too many failures or duplications\n",
      "Sample Initial Population\t#s: 5\tfail_ct: 43003\tTime elapsed: 14.79\n",
      "GA Iter: 0\tMax score: 0.9178\tMin score: 0.0275\t#Pop: 5\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 0.9999\tMin score: 0.9977\t#Pop: 16\t#M+: 1398\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 16\tTime elapsed: 2.52\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 8 programs to measure:\n",
      ".E.E.E.E.E.E.E.E\n",
      "Time elapsed for measurement: 1.41 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n",
      "Time elapsed for training: 0.19 s\n",
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |----------------------------------------------------------------------\n",
      "------------------------------\n",
      "-------------------------------------------------\n",
      "|    0 |            - |              - |      8 |\n",
      "|    1 |            - |              - |      8 |\n",
      "|    2 |            - |              - |      8 |\n",
      "|    3 |            - |              - |      8 |\n",
      "|    4 |            - |              - |      8 |\n",
      "|    5 |            - |              - |      0 |  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "|    6 |            - |              - |      0 |\n",
      "|    7 |            - |              - |      0 |\n",
      "|    8 |            - |              - |      0 |\n",
      "|    9 |            - |              - |      0 |\n",
      "|   10 |            - |              - |      0 |\n",
      "|   11 |            - |              - |      0 |\n",
      "|   12 |            - |              - |      0 |\n",
      "|   13 |            - |              - |      0 |\n",
      "|   14 |            - |              - |      0 |\n",
      "|   15 |            - |              - |      0 |\n",
      "|   16 |            - |              - |      0 |\n",
      "|   17 |            - |              - |      0 |\n",
      "|   18 |            - |              - |      0 |\n",
      "|   19 |            - |              - |      0 |\n",
      "|   20 |            - |              - |      0 |\n",
      "|   21 |            - |              - |      0 |\n",
      "|   22 |            - |              - |      0 |\n",
      "|   23 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 40\tUsed time : 126 s\tNext ID: 5\t\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 1\n",
      "Sample Initial Population\t#s: 72\tfail_ct: 1976\tTime elapsed: 2.12\n",
      "GA Iter: 0\tMax score: 0.9967\tMin score: 0.7632\t#Pop: 16\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 1.0000\tMin score: 0.9980\t#Pop: 16\t#M+: 1397\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 16\tTime elapsed: 27.82\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 8 programs to measure:\n",
      ".E.E.E.E.E.E.E.E\n",
      "Time elapsed for measurement: 1.77 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n",
      "Time elapsed for training: 0.27 s\n",
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |\n",
      "-------------------------------------------------\n",
      "|    0 |            - |              - |      8 |\n",
      "|    1 |            - |              - |      8 |\n",
      "|    2 |            - |              - |      8 |\n",
      "|    3 |            - |              - |      8 |\n",
      "|    4 |            - |              - |      8 |\n",
      "|    5 |            - |              - |      8 |\n",
      "|    6 |            - |              - |      0 |\n",
      "|    7 |            - |              - |      0 |\n",
      "|    8 |            - |              - |      0 |\n",
      "|    9 |            - |              - |      0 |\n",
      "|   10 |            - |              - |      0 |\n",
      "|   11 |            - |              - |      0 |\n",
      "|   12 |            - |              - |      0 |\n",
      "|   13 |            - |              - |      0 |\n",
      "|   14 |            - |              - |      0 |\n",
      "|   15 |            - |              - |      0 |\n",
      "|   16 |            - |              - |      0 |\n",
      "|   17 |            - |              - |      0 |\n",
      "|   18 |            - |              - |      0 |\n",
      "|   19 |            - |              - |      0 |\n",
      "|   20 |            - |              - |      0 |\n",
      "|   21 |            - |              - |      0 |\n",
      "|   22 |            - |              - |      0 |\n",
      "|   23 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 48\tUsed time : 158 s\tNext ID: 6\t\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 1\n",
      "Sample Initial Population\t#s: 70\tfail_ct: 1978\tTime elapsed: 2.00\n",
      "GA Iter: 0\tMax score: 0.9900\tMin score: 0.8397\t#Pop: 16\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 0.9999\tMin score: 0.9973\t#Pop: 16\t#M+: 1388\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 16\tTime elapsed: 31.83\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 8 programs to measure:\n",
      ".E.E.E.E.E.E.E.E\n",
      "Time elapsed for measurement: 3.17 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n",
      "Time elapsed for training: 0.30 s\n",
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |----------------------------------------------------------------------\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "-------------------------------------------------\n",
      "|    0 |            - |              - |      8 |\n",
      "|    1 |            - |              - |      8 |\n",
      "|    2 |            - |              - |      8 |\n",
      "|    3 |            - |              - |      8 |\n",
      "|    4 |            - |              - |      8 |\n",
      "|    5 |            - |              - |      8 |\n",
      "|    6 |            - |              - |      8 |\n",
      "|    7 |            - |              - |      0 |\n",
      "|    8 |            - |              - |      0 |\n",
      "|    9 |            - |              - |      0 |\n",
      "|   10 |            - |              - |      0 |\n",
      "|   11 |            - |              - |      0 |\n",
      "|   12 |            - |              - |      0 |\n",
      "|   13 |            - |              - |      0 |\n",
      "|   14 |            - |              - |      0 |\n",
      "|   15 |            - |              - |      0 |\n",
      "|   16 |            - |              - |      0 |\n",
      "|   17 |            - |              - |      0 |\n",
      "|   18 |            - |              - |      0 |\n",
      "|   19 |            - |              - |      0 |\n",
      "|   20 |            - |              - |      0 |\n",
      "|   21 |            - |              - |      0 |\n",
      "|   22 |            - |              - |      0 |\n",
      "|   23 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 56\tUsed time : 195 s\tNext ID: 7\t\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 1\n",
      "Sample Initial Population\t#s: 92\tfail_ct: 1956\tTime elapsed: 2.27\n",
      "GA Iter: 0\tMax score: 0.9934\tMin score: 0.8884\t#Pop: 16\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 0.9999\tMin score: 0.9978\t#Pop: 16\t#M+: 1398\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 16\tTime elapsed: 32.86\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 8 programs to measure:\n",
      ".E.E.E.E.E.E.E.E\n",
      "Time elapsed for measurement: 1.87 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n",
      "Time elapsed for training: 0.32 s\n",
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |\n",
      "-------------------------------------------------\n",
      "|    0 |            - |              - |      8 |\n",
      "|    1 |            - |              - |      8 |\n",
      "|    2 |            - |              - |      8 |\n",
      "|    3 |            - |              - |      8 |\n",
      "|    4 |            - |              - |      8 |\n",
      "|    5 |            - |              - |      8 |\n",
      "|    6 |            - |              - |      8 |\n",
      "|    7 |            - |              - |      8 |\n",
      "|    8 |            - |              - |      0 |\n",
      "|    9 |            - |              - |      0 |\n",
      "|   10 |            - |              - |      0 |\n",
      "|   11 |            - |              - |      0 |\n",
      "|   12 |            - |              - |      0 |\n",
      "|   13 |            - |              - |      0 |\n",
      "|   14 |            - |              - |      0 |\n",
      "|   15 |            - |              - |      0 |\n",
      "|   16 |            - |              - |      0 |\n",
      "|   17 |            - |              - |      0 |\n",
      "|   18 |            - |              - |      0 |\n",
      "|   19 |            - |              - |      0 |\n",
      "|   20 |            - |              - |      0 |\n",
      "|   21 |            - |              - |      0 |\n",
      "|   22 |            - |              - |      0 |\n",
      "|   23 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 64\tUsed time : 233 s\tNext ID: 8\t\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 1\n",
      "Sample Initial Population\t#s: 145\tfail_ct: 1903\tTime elapsed: 0.95\n",
      "GA Iter: 0\tMax score: 0.9941\tMin score: 0.8481\t#Pop: 16\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 0.9999\tMin score: 0.9978\t#Pop: 16\t#M+: 1390\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 16\tTime elapsed: 12.00\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 8 programs to measure:\n",
      ".E.E.E.E.E.E.E.E\n",
      "Time elapsed for measurement: 1.61 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n",
      "Time elapsed for training: 0.31 s\n",
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |\n",
      "-------------------------------------------------\n",
      "|    0 |            - |              - |      8 |\n",
      "|    1 |            - |              - |      8 |\n",
      "|    2 |            - |              - |      8 |\n",
      "|    3 |            - |              - |      8 |\n",
      "|    4 |            - |              - |      8 |\n",
      "|    5 |            - |              - |      8 |\n",
      "|    6 |            - |              - |      8 |\n",
      "|    7 |            - |              - |      8 |\n",
      "|    8 |            - |              - |      8 |\n",
      "|    9 |            - |              - |      0 |\n",
      "|   10 |            - |              - |      0 |\n",
      "|   11 |            - |              - |      0 |\n",
      "|   12 |            - |              - |      0 |\n",
      "|   13 |            - |              - |      0 |\n",
      "|   14 |            - |              - |      0 |\n",
      "|   15 |            - |              - |      0 |\n",
      "|   16 |            - |              - |      0 |\n",
      "|   17 |            - |              - |      0 |\n",
      "|   18 |            - |              - |      0 |\n",
      "|   19 |            - |              - |      0 |\n",
      "|   20 |            - |              - |      0 |\n",
      "|   21 |            - |              - |      0 |\n",
      "|   22 |            - |              - |      0 |\n",
      "|   23 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 72\tUsed time : 247 s\tNext ID: 9\t\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 1\n",
      "Sample Initial Population\t#s: 62\tfail_ct: 1986\tTime elapsed: 2.26\n",
      "GA Iter: 0\tMax score: 0.9888\tMin score: 0.8234\t#Pop: 16\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 0.9999\tMin score: 0.9980\t#Pop: 16\t#M+: 1401\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 16\tTime elapsed: 32.51\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 8 programs to measure:\n",
      ".E.E.E.E.E.E.E.E\n",
      "Time elapsed for measurement: 2.86 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n",
      "Time elapsed for training: 0.36 s\n",
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |\n",
      "-------------------------------------------------\n",
      "|    0 |            - |              - |      8 |\n",
      "|    1 |            - |              - |      8 |\n",
      "|    2 |            - |              - |      8 |\n",
      "|    3 |            - |              - |      8 |\n",
      "|    4 |            - |              - |      8 |\n",
      "|    5 |            - |              - |      8 |\n",
      "|    6 |            - |              - |      8 |\n",
      "|    7 |            - |              - |      8 |\n",
      "|    8 |            - |              - |      8 |\n",
      "|    9 |            - |              - |      8 |\n",
      "|   10 |            - |              - |      0 |\n",
      "|   11 |            - |              - |      0 |\n",
      "|   12 |            - |              - |      0 |\n",
      "|   13 |            - |              - |      0 |\n",
      "|   14 |            - |              - |      0 |\n",
      "|   15 |            - |              - |      0 |\n",
      "|   16 |            - |              - |      0 |\n",
      "|   17 |            - |              - |      0 |\n",
      "|   18 |            - |              - |      0 |\n",
      "|   19 |            - |              - |      0 |\n",
      "|   20 |            - |              - |      0 |\n",
      "|   21 |            - |              - |      0 |\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "|   22 |            - |              - |      0 |\n",
      "|   23 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 80\tUsed time : 286 s\tNext ID: 10\t\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 1\n",
      "Sample Initial Population\t#s: 100\tfail_ct: 1948\tTime elapsed: 0.93\n",
      "GA Iter: 0\tMax score: 0.9949\tMin score: 0.8656\t#Pop: 16\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 0.9999\tMin score: 0.9976\t#Pop: 16\t#M+: 1373\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 16\tTime elapsed: 13.75\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 8 programs to measure:\n",
      ".E.E.E.E.E.E.E.E\n",
      "Time elapsed for measurement: 1.58 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n",
      "Time elapsed for training: 0.34 s\n",
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |\n",
      "-------------------------------------------------\n",
      "|    0 |            - |              - |      8 |\n",
      "|    1 |            - |              - |      8 |\n",
      "|    2 |            - |              - |      8 |\n",
      "|    3 |            - |              - |      8 |\n",
      "|    4 |            - |              - |      8 |\n",
      "|    5 |            - |              - |      8 |\n",
      "|    6 |            - |              - |      8 |\n",
      "|    7 |            - |              - |      8 |\n",
      "|    8 |            - |              - |      8 |\n",
      "|    9 |            - |              - |      8 |\n",
      "|   10 |            - |              - |      8 |\n",
      "|   11 |            - |              - |      0 |\n",
      "|   12 |            - |              - |      0 |\n",
      "|   13 |            - |              - |      0 |\n",
      "|   14 |            - |              - |      0 |\n",
      "|   15 |            - |              - |      0 |\n",
      "|   16 |            - |              - |      0 |\n",
      "|   17 |            - |              - |      0 |\n",
      "|   18 |            - |              - |      0 |\n",
      "|   19 |            - |              - |      0 |\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "|   20 |            - |              - |      0 |\n",
      "|   21 |            - |              - |      0 |\n",
      "|   22 |            - |              - |      0 |\n",
      "|   23 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 88\tUsed time : 302 s\tNext ID: 11\t\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 1\n",
      "Sample Initial Population\t#s: 77\tfail_ct: 1971\tTime elapsed: 1.85\n",
      "GA Iter: 0\tMax score: 0.9829\tMin score: 0.7834\t#Pop: 16\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 0.9995\tMin score: 0.9968\t#Pop: 16\t#M+: 1400\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 16\tTime elapsed: 28.42\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 8 programs to measure:\n",
      ".E.E.E.E.E.E.E.E\n",
      "Time elapsed for measurement: 1.77 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n",
      "Time elapsed for training: |  ID  | Latency (ms) | Speed (GFLOPS) | Trials |\n",
      "0.39 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "-------------------------------------------------\n",
      "|    0 |            - |              - |      8 |\n",
      "|    1 |            - |              - |      8 |\n",
      "|    2 |            - |              - |      8 |\n",
      "|    3 |            - |              - |      8 |\n",
      "|    4 |            - |              - |      8 |\n",
      "|    5 |            - |              - |      8 |\n",
      "|    6 |            - |              - |      8 |\n",
      "|    7 |            - |              - |      8 |\n",
      "|    8 |            - |              - |      8 |\n",
      "|    9 |            - |              - |      8 |\n",
      "|   10 |            - |              - |      8 |\n",
      "|   11 |            - |              - |      8 |\n",
      "|   12 |            - |              - |      0 |\n",
      "|   13 |            - |              - |      0 |\n",
      "|   14 |            - |              - |      0 |\n",
      "|   15 |            - |              - |      0 |\n",
      "|   16 |            - |              - |      0 |\n",
      "|   17 |            - |              - |      0 |\n",
      "|   18 |            - |              - |      0 |\n",
      "|   19 |            - |              - |      0 |\n",
      "|   20 |            - |              - |      0 |\n",
      "|   21 |            - |              - |      0 |\n",
      "|   22 |            - |              - |      0 |\n",
      "|   23 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 96\tUsed time : 335 s\tNext ID: 12\t\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 1\n",
      "Sample Initial Population\t#s: 86\tfail_ct: 1962\tTime elapsed: 1.92\n",
      "GA Iter: 0\tMax score: 0.9965\tMin score: 0.8003\t#Pop: 16\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 1.0000\tMin score: 0.9977\t#Pop: 16\t#M+: 1395\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 16\tTime elapsed: 28.41\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 8 programs to measure:\n",
      ".E.E.E.E.E.E.E.E\n",
      "Time elapsed for measurement: 1.84 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n",
      "Time elapsed for training: 0.38 s\n",
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "-------------------------------------------------\n",
      "|    0 |            - |              - |      8 |\n",
      "|    1 |            - |              - |      8 |\n",
      "|    2 |            - |              - |      8 |\n",
      "|    3 |            - |              - |      8 |\n",
      "|    4 |            - |              - |      8 |\n",
      "|    5 |            - |              - |      8 |\n",
      "|    6 |            - |              - |      8 |\n",
      "|    7 |            - |              - |      8 |\n",
      "|    8 |            - |              - |      8 |\n",
      "|    9 |            - |              - |      8 |\n",
      "|   10 |            - |              - |      8 |\n",
      "|   11 |            - |              - |      8 |\n",
      "|   12 |            - |              - |      8 |\n",
      "|   13 |            - |              - |      0 |\n",
      "|   14 |            - |              - |      0 |\n",
      "|   15 |            - |              - |      0 |\n",
      "|   16 |            - |              - |      0 |\n",
      "|   17 |            - |              - |      0 |\n",
      "|   18 |            - |              - |      0 |\n",
      "|   19 |            - |              - |      0 |\n",
      "|   20 |            - |              - |      0 |\n",
      "|   21 |            - |              - |      0 |\n",
      "|   22 |            - |              - |      0 |\n",
      "|   23 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 104\tUsed time : 367 s\tNext ID: 13\t\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 1\n",
      "Sample Initial Population\t#s: 130\tfail_ct: 1918\tTime elapsed: 0.80\n",
      "GA Iter: 0\tMax score: 0.9906\tMin score: 0.9208\t#Pop: 16\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 1.0000\tMin score: 0.9978\t#Pop: 16\t#M+: 1390\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 16\tTime elapsed: 10.12\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 8 programs to measure:\n",
      ".E.E.E.E.E.E.E.E\n",
      "Time elapsed for measurement: 1.45 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n",
      "Time elapsed for training: 0.42 s\n",
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |\n",
      "-------------------------------------------------\n",
      "|    0 |            - |              - |      8 |\n",
      "|    1 |            - |              - |      8 |\n",
      "|    2 |            - |              - |      8 |\n",
      "|    3 |            - |              - |      8 |\n",
      "|    4 |            - |              - |      8 |\n",
      "|    5 |            - |              - |      8 |\n",
      "|    6 |            - |              - |      8 |\n",
      "|    7 |            - |              - |      8 |\n",
      "|    8 |            - |              - |      8 |\n",
      "|    9 |            - |              - |      8 |\n",
      "|   10 |            - |              - |      8 |\n",
      "|   11 |            - |              - |      8 |\n",
      "|   12 |            - |              - |      8 |\n",
      "|   13 |            - |              - |      8 |\n",
      "|   14 |            - |              - |      0 |\n",
      "|   15 |            - |              - |      0 |\n",
      "|   16 |            - |              - |      0 |\n",
      "|   17 |            - |              - |      0 |\n",
      "|   18 |            - |              - |      0 |\n",
      "|   19 |            - |              - |      0 |\n",
      "|   20 |            - |              - |      0 |\n",
      "|   21 |            - |              - |      0 |\n",
      "|   22 |            - |              - |      0 |\n",
      "|   23 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 112\tUsed time : 380 s\tNext ID: 14\t\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 1\n",
      "Sample Initial Population\t#s: 120\tfail_ct: 1928\tTime elapsed: 0.88\n",
      "GA Iter: 0\tMax score: 0.9789\tMin score: 0.8615\t#Pop: 16\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 0.9997\tMin score: 0.9974\t#Pop: 16\t#M+: 1395\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 16\tTime elapsed: 11.62\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 8 programs to measure:\n",
      ".E.E.E.E.E.E.E.E\n",
      "Time elapsed for measurement: 1.41 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n",
      "Time elapsed for training: 0.39 s\n",
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |----------------------------------------------------------------------\n",
      "\n",
      "-------------------------------------------------\n",
      "|    0 |            - |              - |      8 |\n",
      "|    1 |            - |              - |      8 |\n",
      "|    2 |            - |              - |      8 |\n",
      "|    3 |            - |              - |      8 |\n",
      "|    4 |            - |              - |      8 |\n",
      "|    5 |            - |              - |      8 |\n",
      "|    6 |            - |              - |      8 |\n",
      "|    7 |            - |              - |      8 |\n",
      "|    8 |            - |              - |      8 |\n",
      "|    9 |            - |              - |      8 |\n",
      "|   10 |            - |              - |      8 |\n",
      "|   11 |            - |              - |      8 |\n",
      "|   12 |            - |              - |      8 |\n",
      "|   13 |            - |              - |      8 |\n",
      "|   14 |            - |              - |      8 |\n",
      "|   15 |            - |              - |      0 |\n",
      "|   16 |            - |              - |      0 |\n",
      "|   17 |            - |              - |      0 |\n",
      "|   18 |            - |              - |      0 |\n",
      "|   19 |            - |              - |      0 |\n",
      "|   20 |            - |              - |      0 |\n",
      "|   21 |            - |              - |      0 |\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "|   22 |            - |              - |      0 |\n",
      "|   23 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 120\tUsed time : 395 s\tNext ID: 15\t\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 1\n",
      "Sample Initial Population\t#s: 98\tfail_ct: 3998\tTime elapsed: 0.91\n",
      "GA Iter: 0\tMax score: 0.9998\tMin score: 0.8620\t#Pop: 16\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 1.0000\tMin score: 0.9978\t#Pop: 16\t#M+: 1389\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 16\tTime elapsed: 5.41\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 8 programs to measure:\n",
      ".E.E.E.E.E.E.E.E\n",
      "Time elapsed for measurement: 1.41 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n",
      "Time elapsed for training: 0.37 s\n",
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |----------------------------------------------------------------------\n",
      "\n",
      "-------------------------------------------------\n",
      "|    0 |            - |              - |      8 |\n",
      "|    1 |            - |              - |      8 |\n",
      "|    2 |            - |              - |      8 |\n",
      "|    3 |            - |              - |      8 |\n",
      "|    4 |            - |              - |      8 |\n",
      "|    5 |            - |              - |      8 |\n",
      "|    6 |            - |              - |      8 |\n",
      "|    7 |            - |              - |      8 |\n",
      "|    8 |            - |              - |      8 |\n",
      "|    9 |            - |              - |      8 |\n",
      "|   10 |            - |              - |      8 |\n",
      "|   11 |            - |              - |      8 |\n",
      "|   12 |            - |              - |      8 |\n",
      "|   13 |            - |              - |      8 |\n",
      "|   14 |            - |              - |      8 |\n",
      "|   15 |            - |              - |      8 |\n",
      "|   16 |            - |              - |      0 |\n",
      "|   17 |            - |              - |      0 |\n",
      "|   18 |            - |              - |      0 |\n",
      "|   19 |            - |              - |      0 |\n",
      "|   20 |            - |              - |      0 |\n",
      "|   21 |            - |              - |      0 |\n",
      "|   22 |            - |              - |      0 |\n",
      "|   23 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 128\tUsed time : 403 s\tNext ID: 16\t\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 4\n",
      "Sample Initial Population\t#s: 100\tfail_ct: 1948\tTime elapsed: 0.73\n",
      "GA Iter: 0\tMax score: 0.9833\tMin score: 0.8609\t#Pop: 16\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 1.0000\tMin score: 0.9974\t#Pop: 16\t#M+: 1404\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 16\tTime elapsed: 2.46\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 8 programs to measure:\n",
      ".E.E.E.E.E.E.E.E\n",
      "Time elapsed for measurement: 1.32 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n",
      "Time elapsed for training: 0.39 s\n",
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |\n",
      "-------------------------------------------------\n",
      "|    0 |            - |              - |      8 |\n",
      "|    1 |            - |              - |      8 |\n",
      "|    2 |            - |              - |      8 |\n",
      "|    3 |            - |              - |      8 |\n",
      "|    4 |            - |              - |      8 |\n",
      "|    5 |            - |              - |      8 |\n",
      "|    6 |            - |              - |      8 |\n",
      "|    7 |            - |              - |      8 |\n",
      "|    8 |            - |              - |      8 |\n",
      "|    9 |            - |              - |      8 |\n",
      "|   10 |            - |              - |      8 |\n",
      "|   11 |            - |              - |      8 |\n",
      "|   12 |            - |              - |      8 |\n",
      "|   13 |            - |              - |      8 |\n",
      "|   14 |            - |              - |      8 |\n",
      "|   15 |            - |              - |      8 |\n",
      "|   16 |            - |              - |      8 |\n",
      "|   17 |            - |              - |      0 |\n",
      "|   18 |            - |              - |      0 |\n",
      "|   19 |            - |              - |      0 |\n",
      "|   20 |            - |              - |      0 |\n",
      "|   21 |            - |              - |      0 |\n",
      "|   22 |            - |              - |      0 |\n",
      "|   23 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 136\tUsed time : 408 s\tNext ID: 17\t\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 1\n",
      "Sample Initial Population\t#s: 74\tfail_ct: 1974\tTime elapsed: 2.12\n",
      "GA Iter: 0\tMax score: 0.9867\tMin score: 0.7510\t#Pop: 16\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 0.9996\tMin score: 0.9976\t#Pop: 16\t#M+: 1385\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 16\tTime elapsed: 30.55\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 8 programs to measure:\n",
      ".E.E.E.E.E.E.E.E\n",
      "Time elapsed for measurement: 2.99 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n",
      "Time elapsed for training: 0.48 s\n",
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |\n",
      "-------------------------------------------------\n",
      "|    0 |            - |              - |      8 |\n",
      "|    1 |            - |              - |      8 |\n",
      "|    2 |            - |              - |      8 |\n",
      "|    3 |            - |              - |      8 |\n",
      "|    4 |            - |              - |      8 |\n",
      "|    5 |            - |              - |      8 |\n",
      "|    6 |            - |              - |      8 |\n",
      "|    7 |            - |              - |      8 |\n",
      "|    8 |            - |              - |      8 |\n",
      "|    9 |            - |              - |      8 |\n",
      "|   10 |            - |              - |      8 |\n",
      "|   11 |            - |              - |      8 |\n",
      "|   12 |            - |              - |      8 |\n",
      "|   13 |            - |              - |      8 |\n",
      "|   14 |            - |              - |      8 |\n",
      "|   15 |            - |              - |      8 |\n",
      "|   16 |            - |              - |      8 |\n",
      "|   17 |            - |              - |      8 |\n",
      "|   18 |            - |              - |      0 |\n",
      "|   19 |            - |              - |      0 |\n",
      "|   20 |            - |              - |      0 |\n",
      "|   21 |            - |              - |      0 |\n",
      "|   22 |            - |              - |      0 |\n",
      "|   23 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 144\tUsed time : 444 s\tNext ID: 18\t\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 1\n",
      "Sample Iter: 5\t#Pop: 5\t#Target: 50\tfail_ct: 10235\tTime elapsed: 7.61\n",
      "#Target has been reduced to 25 due to too many failures or duplications\n",
      "Sample Iter: 10\t#Pop: 5\t#Target: 25\tfail_ct: 20475\tTime elapsed: 15.35\n",
      "#Target has been reduced to 12 due to too many failures or duplications\n",
      "Sample Iter: 15\t#Pop: 5\t#Target: 12\tfail_ct: 30715\tTime elapsed: 23.05\n",
      "#Target has been reduced to 6 due to too many failures or duplications\n",
      "Sample Iter: 20\t#Pop: 5\t#Target: 6\tfail_ct: 40955\tTime elapsed: 30.81\n",
      "#Target has been reduced to 3 due to too many failures or duplications\n",
      "Sample Initial Population\t#s: 5\tfail_ct: 43003\tTime elapsed: 32.34\n",
      "GA Iter: 0\tMax score: 0.5665\tMin score: 0.2045\t#Pop: 5\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 0.9999\tMin score: 0.9972\t#Pop: 16\t#M+: 1384\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 16\tTime elapsed: 5.81\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 8 programs to measure:\n",
      ".E.E.E.E.E.E.E.E\n",
      "Time elapsed for measurement: 1.38 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:31:50] /home/boyuan/.anaconda3/envs/tvm-build/conda-bld/tvm-cu102-package_1636400145961/work/src/auto_scheduler/measure.cc:299: Warning: Too many errors happened during tuning. Switching to debug mode.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for training: |  ID  | Latency (ms) | Speed (GFLOPS) | Trials |\n",
      "0.42 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "-------------------------------------------------\n",
      "|    0 |            - |              - |      8 |\n",
      "|    1 |            - |              - |      8 |\n",
      "|    2 |            - |              - |      8 |\n",
      "|    3 |            - |              - |      8 |\n",
      "|    4 |            - |              - |      8 |\n",
      "|    5 |            - |              - |      8 |\n",
      "|    6 |            - |              - |      8 |\n",
      "|    7 |            - |              - |      8 |\n",
      "|    8 |            - |              - |      8 |\n",
      "|    9 |            - |              - |      8 |\n",
      "|   10 |            - |              - |      8 |\n",
      "|   11 |            - |              - |      8 |\n",
      "|   12 |            - |              - |      8 |\n",
      "|   13 |            - |              - |      8 |\n",
      "|   14 |            - |              - |      8 |\n",
      "|   15 |            - |              - |      8 |\n",
      "|   16 |            - |              - |      8 |\n",
      "|   17 |            - |              - |      8 |\n",
      "|   18 |            - |              - |      8 |\n",
      "|   19 |            - |              - |      0 |\n",
      "|   20 |            - |              - |      0 |\n",
      "|   21 |            - |              - |      0 |\n",
      "|   22 |            - |              - |      0 |\n",
      "|   23 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 152\tUsed time : 484 s\tNext ID: 19\t\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 1\n",
      "Sample Initial Population\t#s: 56\tfail_ct: 1992\tTime elapsed: 2.21\n",
      "GA Iter: 0\tMax score: 0.9868\tMin score: 0.7901\t#Pop: 16\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 0.9997\tMin score: 0.9986\t#Pop: 16\t#M+: 1395\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 16\tTime elapsed: 28.64\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 8 programs to measure:\n",
      ".E.E.E.E.E.E.E.E\n",
      "==================================================\n",
      "No: 153\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      "Idx.x) & 7) >> 1) * 512)) + ((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) & 255) * 2)) + (((int)threadIdx.x) & 1)) + 122880))] + input_tile[(15)]);\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.25, Tstamp:1637015543.02)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder, placeholder, placeholder\n",
      "data_pack auto_unroll: 512\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,128)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,64)\n",
      "    for eps (0,4)\n",
      "      for nu (0,4)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,4)\n",
      "      unroll nu (0,4)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,256)\n",
      "  threadIdx.x eps.2@nu.2@p.2@co.2@ (0,64)\n",
      "    for ci.0 (0,32)\n",
      "      for ax0@ax1@ax2@ax3@.0.0 (0,64)\n",
      "        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)\n",
      "          placeholder.shared = ...\n",
      "      for ax0@ax1@ax2@ax3@.0.0 (0,4)\n",
      "        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)\n",
      "          data_pack.shared = ...\n",
      "      for ci.1 (0,2)\n",
      "        for nu_c.3 (0,2)\n",
      "          for p_c.3 (0,2)\n",
      "            for ci.2 (0,8)\n",
      "              for co_c.4 (0,2)\n",
      "                bgemm.local = ...\n",
      "    for nu.3 (0,2)\n",
      "      for p.3 (0,2)\n",
      "        for co.3 (0,2)\n",
      "          bgemm = ...\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,256)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,2)\n",
      "      unroll vw (0,2)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,784)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_relu = ...\n",
      "\n",
      "==================================================\n",
      "No: 154\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      ") & 31) * 8)) + co_inner) + 6400))] = bgemm_local[((((((eps_inner * 64) + (nu_inner * 32)) + (p_inner * 8)) + co_inner) + 896))];\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.22, Tstamp:1637015543.05)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder, placeholder, placeholder\n",
      "data_pack auto_unroll: 64\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,256)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,4)\n",
      "      for nu (0,4)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,4)\n",
      "      unroll nu (0,4)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,2)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,8)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,64)\n",
      "      for ci.0 (0,256)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,128)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)\n",
      "            placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,4)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)\n",
      "            data_pack.shared = ...\n",
      "        for ci.1 (0,2)\n",
      "          for p_c.3 (0,2)\n",
      "            for eps_c.4 (0,2)\n",
      "              for nu_c.4 (0,2)\n",
      "                for p_c.4 (0,2)\n",
      "                  for co_c.4 (0,8)\n",
      "                    bgemm.local = ...\n",
      "      for eps.3 (0,2)\n",
      "        for nu.3 (0,2)\n",
      "          for p.3 (0,4)\n",
      "            for co.3 (0,8)\n",
      "              bgemm = ...\n",
      "inverse auto_unroll: 16\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,256)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,2)\n",
      "      unroll vw (0,2)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,784)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_relu = ...\n",
      "\n",
      "==================================================\n",
      "No: 155\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      "x.x) & 63) * 2)) + co_inner) + 2048))] = bgemm_local[((((((eps_inner * 8) + (nu_inner * 4)) + (p_inner * 2)) + co_inner) + 32))];\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.33, Tstamp:1637015543.08)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder, placeholder, placeholder\n",
      "data_pack auto_unroll: 512\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,1024)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,8)\n",
      "    for eps (0,4)\n",
      "      for nu (0,4)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,4)\n",
      "      unroll nu (0,4)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,16)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,2)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,128)\n",
      "      bgemm.local auto_unroll: 1024\n",
      "      for ci.0 (0,64)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,64)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)\n",
      "            placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,4)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)\n",
      "            data_pack.shared = ...\n",
      "        for p_c.3 (0,2)\n",
      "          for ci.2 (0,8)\n",
      "            for eps_c.4 (0,4)\n",
      "              for nu_c.4 (0,2)\n",
      "                for co_c.4 (0,2)\n",
      "                  bgemm.local = ...\n",
      "      for eps.3 (0,4)\n",
      "        for nu.3 (0,2)\n",
      "          for p.3 (0,2)\n",
      "            for co.3 (0,2)\n",
      "              bgemm = ...\n",
      "inverse auto_unroll: 512\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,256)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,2)\n",
      "      unroll vw (0,2)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,784)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_relu = ...\n",
      "\n",
      "==================================================\n",
      "No: 156\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      "lder1[((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) & 511))]) + placeholder2[((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) & 511))]), 0.000000e+00f);\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.21, Tstamp:1637015543.11)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder, placeholder, placeholder\n",
      "data_pack auto_unroll: 16\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,256)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,4)\n",
      "      for nu (0,4)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,4)\n",
      "      unroll nu (0,4)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,64)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,2)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,128)\n",
      "      for ci.0 (0,256)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,32)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)\n",
      "            placeholder.shared = ...\n",
      "        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)\n",
      "          data_pack.shared = ...\n",
      "        for ci.1 (0,2)\n",
      "          for nu_c.3 (0,2)\n",
      "            for co_c.3 (0,4)\n",
      "              bgemm.local = ...\n",
      "      for nu.3 (0,2)\n",
      "        for co.3 (0,4)\n",
      "          bgemm = ...\n",
      "inverse auto_unroll: 1024\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,128)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,64)\n",
      "    unroll vh (0,2)\n",
      "      unroll vw (0,2)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,784)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_relu = ...\n",
      "\n",
      "==================================================\n",
      "No: 157\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      "4)) & 7) * 512)) + (((((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) & 255) >> 3) * 16)) + (((int)threadIdx.x) & 15)) + 122880))] + input_tile[(15)]);\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.25, Tstamp:1637015543.14)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder, placeholder, placeholder\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,256)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,4)\n",
      "      for nu (0,4)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,4)\n",
      "      unroll nu (0,4)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,8)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,2)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,64)\n",
      "      for ci.0 (0,128)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,128)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)\n",
      "            placeholder.shared = ...\n",
      "        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)\n",
      "          vectorize ax0@ax1@ax2@ax3@.1 (0,8)\n",
      "            data_pack.shared = ...\n",
      "        for eps_c.3 (0,2)\n",
      "          for nu_c.3 (0,2)\n",
      "            for p_c.3 (0,2)\n",
      "              for ci.2 (0,4)\n",
      "                for nu_c.4 (0,2)\n",
      "                  for co_c.4 (0,8)\n",
      "                    bgemm.local = ...\n",
      "      for eps.3 (0,2)\n",
      "        for nu.3 (0,4)\n",
      "          for p.3 (0,2)\n",
      "            for co.3 (0,8)\n",
      "              bgemm = ...\n",
      "inverse auto_unroll: 16\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,256)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,2)\n",
      "      unroll vw (0,2)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,784)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_relu = ...\n",
      "\n",
      "==================================================\n",
      "No: 158\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      "((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) + 122880))] = (data_pack[((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) + 122880))] + input_tile[(15)]);\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.21, Tstamp:1637015543.17)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder, placeholder, placeholder\n",
      "data_pack auto_unroll: 16\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,256)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,4)\n",
      "      for nu (0,4)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,4)\n",
      "      unroll nu (0,4)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,128)\n",
      "  threadIdx.x eps.2@nu.2@p.2@co.2@ (0,128)\n",
      "    for ci.0 (0,32)\n",
      "      for ax0@ax1@ax2@ax3@.0.0 (0,8)\n",
      "        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)\n",
      "          placeholder.shared = ...\n",
      "      for ax0@ax1@ax2@ax3@.0.0 (0,16)\n",
      "        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)\n",
      "          data_pack.shared = ...\n",
      "      for ci.1 (0,8)\n",
      "        for ci.2 (0,2)\n",
      "          for nu_c.4 (0,2)\n",
      "            for p_c.4 (0,4)\n",
      "              bgemm.local = ...\n",
      "    for nu.3 (0,2)\n",
      "      for p.3 (0,4)\n",
      "        bgemm = ...\n",
      "inverse auto_unroll: 1024\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,256)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,2)\n",
      "      unroll vw (0,2)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,784)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_relu = ...\n",
      "\n",
      "==================================================\n",
      "No: 159\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      "(int)threadIdx.x) >> 3)) & 7) * 512)) + (((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) & 511) >> 3) * 8)) + (((int)threadIdx.x) & 7)) + 122880))]);\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.24, Tstamp:1637015543.20)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder, placeholder, placeholder\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,8192)\n",
      "  for eps (0,4)\n",
      "    for nu (0,4)\n",
      "      input_tile = ...\n",
      "  unroll eps (0,4)\n",
      "    unroll nu (0,4)\n",
      "      unroll r_a (0,4)\n",
      "        unroll r_b (0,4)\n",
      "          data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,16)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,8)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,64)\n",
      "      for ci.0 (0,128)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,128)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)\n",
      "            placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,4)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)\n",
      "            data_pack.shared = ...\n",
      "        for ci.1 (0,4)\n",
      "          for nu_c.3 (0,2)\n",
      "            for co_c.3 (0,8)\n",
      "              bgemm.local = ...\n",
      "      for nu.3 (0,2)\n",
      "        for co.3 (0,8)\n",
      "          bgemm = ...\n",
      "inverse auto_unroll: 64\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,256)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,2)\n",
      "      unroll vw (0,2)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,392)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,64)\n",
      "    T_relu = ...\n",
      "\n",
      "==================================================\n",
      "No: 160\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      "lder1[((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) & 511))]) + placeholder2[((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) & 511))]), 0.000000e+00f);\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.24, Tstamp:1637015543.23)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder, placeholder, placeholder\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,256)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,4)\n",
      "      for nu (0,4)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,4)\n",
      "      unroll nu (0,4)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,128)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,4)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,32)\n",
      "      bgemm.local auto_unroll: 1024\n",
      "      for ci.0 (0,256)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,8)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "            placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,2)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "            data_pack.shared = ...\n",
      "        for nu_c.3 (0,2)\n",
      "          for p_c.3 (0,2)\n",
      "            for ci.2 (0,2)\n",
      "              for co_c.4 (0,2)\n",
      "                bgemm.local = ...\n",
      "      for nu.3 (0,2)\n",
      "        for p.3 (0,2)\n",
      "          for co.3 (0,2)\n",
      "            bgemm = ...\n",
      "inverse auto_unroll: 1024\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,128)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,64)\n",
      "    unroll vh (0,2)\n",
      "      unroll vw (0,2)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,784)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_relu = ...\n",
      "\n",
      "Time elapsed for measurement: 1.80 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:32:23] /home/boyuan/.anaconda3/envs/tvm-build/conda-bld/tvm-cu102-package_1636400145961/work/src/auto_scheduler/measure.cc:299: Warning: Too many errors happened during tuning. Switching to debug mode.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for training: |  ID  | Latency (ms) | Speed (GFLOPS) | Trials |\n",
      "-------------------------------------------------\n",
      "|    0 |            - |              - |      8 |\n",
      "|    1 |            - |              - |      8 |\n",
      "|    2 |            - |              - |      8 |\n",
      "0.49 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "|    3 |            - |              - |      8 |\n",
      "|    4 |            - |              - |      8 |\n",
      "|    5 |            - |              - |      8 |\n",
      "|    6 |            - |              - |      8 |\n",
      "|    7 |            - |              - |      8 |\n",
      "|    8 |            - |              - |      8 |\n",
      "|    9 |            - |              - |      8 |\n",
      "|   10 |            - |              - |      8 |\n",
      "|   11 |            - |              - |      8 |\n",
      "|   12 |            - |              - |      8 |\n",
      "|   13 |            - |              - |      8 |\n",
      "|   14 |            - |              - |      8 |\n",
      "|   15 |            - |              - |      8 |\n",
      "|   16 |            - |              - |      8 |\n",
      "|   17 |            - |              - |      8 |\n",
      "|   18 |            - |              - |      8 |\n",
      "|   19 |            - |              - |      8 |\n",
      "|   20 |            - |              - |      0 |\n",
      "|   21 |            - |              - |      0 |\n",
      "|   22 |            - |              - |      0 |\n",
      "|   23 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 160\tUsed time : 517 s\tNext ID: 20\t\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 1\n",
      "Sample Initial Population\t#s: 71\tfail_ct: 1977\tTime elapsed: 2.09\n",
      "GA Iter: 0\tMax score: 0.9987\tMin score: 0.7998\t#Pop: 16\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 0.9999\tMin score: 0.9972\t#Pop: 16\t#M+: 1388\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 16\tTime elapsed: 28.16\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 8 programs to measure:\n",
      ".E.E.E.E.E.E.E.E\n",
      "==================================================\n",
      "No: 161\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      "nt)threadIdx.x) >> 4)) & 3) * 512)) + (((((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) & 127) >> 2) * 16)) + (((int)threadIdx.x) & 15)) + 122880))]);\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.27, Tstamp:1637015575.75)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,256)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,4)\n",
      "      for nu (0,4)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,4)\n",
      "      unroll nu (0,4)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,64)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,4)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,32)\n",
      "      for ci.0 (0,256)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,16)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "            placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,4)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "            data_pack.shared = ...\n",
      "        for ci.1 (0,2)\n",
      "          for eps_c.3 (0,2)\n",
      "            for co_c.3 (0,2)\n",
      "              for nu_c.4 (0,2)\n",
      "                for p_c.4 (0,2)\n",
      "                  bgemm.local = ...\n",
      "      for eps.3 (0,2)\n",
      "        for nu.3 (0,2)\n",
      "          for p.3 (0,2)\n",
      "            for co.3 (0,2)\n",
      "              bgemm = ...\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,256)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,2)\n",
      "      unroll vw (0,2)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,1568)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,16)\n",
      "    T_add = ...\n",
      "\n",
      "==================================================\n",
      "No: 162\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      "3) * 128)) + ((((int)threadIdx.x) & 15) * 4)) + co_inner) + 32832))] = bgemm_local[(((((nu_inner * 8) + (p_inner * 4)) + co_inner) + 48))];\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.35, Tstamp:1637015575.78)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,256)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,4)\n",
      "      for nu (0,4)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,4)\n",
      "      unroll nu (0,4)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,64)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,4)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,32)\n",
      "      bgemm.local auto_unroll: 512\n",
      "      for ci.0 (0,64)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,256)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "            placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,4)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "            data_pack.shared = ...\n",
      "        for ci.1 (0,4)\n",
      "          for nu_c.3 (0,2)\n",
      "            for co_c.3 (0,2)\n",
      "              for ci.2 (0,2)\n",
      "                for p_c.4 (0,2)\n",
      "                  for co_c.4 (0,2)\n",
      "                    bgemm.local = ...\n",
      "      for nu.3 (0,2)\n",
      "        for p.3 (0,2)\n",
      "          for co.3 (0,4)\n",
      "            bgemm = ...\n",
      "inverse auto_unroll: 512\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,256)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,2)\n",
      "      unroll vw (0,2)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,784)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_add = ...\n",
      "\n",
      "==================================================\n",
      "No: 163\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      "* 128)) + ((((int)threadIdx.x) & 15) * 4)) + co_inner) + 81984))] = bgemm_local[(((((eps_inner * 8) + (nu_inner * 4)) + co_inner) + 112))];\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.29, Tstamp:1637015575.81)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "data_pack auto_unroll: 16\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,256)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,4)\n",
      "      for nu (0,4)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,4)\n",
      "      unroll nu (0,4)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,8)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,8)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,128)\n",
      "      bgemm.local auto_unroll: 64\n",
      "      for ci.0 (0,256)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,32)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)\n",
      "            placeholder.shared = ...\n",
      "        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)\n",
      "          vectorize ax0@ax1@ax2@ax3@.1 (0,2)\n",
      "            data_pack.shared = ...\n",
      "        for co_c.3 (0,2)\n",
      "          for ci.2 (0,2)\n",
      "            for eps_c.4 (0,2)\n",
      "              for nu_c.4 (0,2)\n",
      "                for co_c.4 (0,2)\n",
      "                  bgemm.local = ...\n",
      "      for eps.3 (0,2)\n",
      "        for nu.3 (0,2)\n",
      "          for co.3 (0,4)\n",
      "            bgemm = ...\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,256)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,2)\n",
      "      unroll vw (0,2)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,784)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_add = ...\n",
      "\n",
      "==================================================\n",
      "No: 164\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      "dx.x)) % 3584) >> 10) * 512)) + (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) & 511)))] + placeholder[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)))]);\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.30, Tstamp:1637015575.84)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "data_pack auto_unroll: 1024\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,256)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,4)\n",
      "      for nu (0,4)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,4)\n",
      "      unroll nu (0,4)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,32)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,4)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,16)\n",
      "      for ci.0 (0,256)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,128)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,16)\n",
      "            placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,8)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,16)\n",
      "            data_pack.shared = ...\n",
      "        for eps_c.3 (0,2)\n",
      "          for p_c.3 (0,2)\n",
      "            for co_c.3 (0,2)\n",
      "              for ci.2 (0,2)\n",
      "                for nu_c.4 (0,4)\n",
      "                  for p_c.4 (0,2)\n",
      "                    bgemm.local = ...\n",
      "      for eps.3 (0,2)\n",
      "        for nu.3 (0,4)\n",
      "          for p.3 (0,4)\n",
      "            for co.3 (0,2)\n",
      "              bgemm = ...\n",
      "inverse auto_unroll: 64\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,256)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,2)\n",
      "      unroll vw (0,2)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,784)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_add = ...\n",
      "\n",
      "==================================================\n",
      "No: 165\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      "dx.x)) % 3584) >> 10) * 512)) + (((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) & 511)))] + placeholder[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)))]);\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.31, Tstamp:1637015575.87)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,256)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,4)\n",
      "      for nu (0,4)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,4)\n",
      "      unroll nu (0,4)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,64)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,2)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,32)\n",
      "      bgemm.local auto_unroll: 512\n",
      "      for ci.0 (0,128)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,32)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "            placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,16)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "            data_pack.shared = ...\n",
      "        for ci.1 (0,4)\n",
      "          for eps_c.3 (0,2)\n",
      "            for nu_c.3 (0,2)\n",
      "              for nu_c.4 (0,2)\n",
      "                for p_c.4 (0,2)\n",
      "                  for co_c.4 (0,2)\n",
      "                    bgemm.local = ...\n",
      "      for eps.3 (0,2)\n",
      "        for nu.3 (0,4)\n",
      "          for p.3 (0,2)\n",
      "            for co.3 (0,2)\n",
      "              bgemm = ...\n",
      "inverse auto_unroll: 16\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,256)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,2)\n",
      "      unroll vw (0,2)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,392)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,64)\n",
      "    T_add = ...\n",
      "\n",
      "==================================================\n",
      "No: 166\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      "((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) + 122880))] = (data_pack[((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) + 122880))] + input_tile[(15)]);\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.28, Tstamp:1637015575.90)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "data_pack auto_unroll: 16\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,256)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,4)\n",
      "      for nu (0,4)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,4)\n",
      "      unroll nu (0,4)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,8)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,4)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,64)\n",
      "      for ci.0 (0,256)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,128)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)\n",
      "            placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,2)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)\n",
      "            data_pack.shared = ...\n",
      "        for ci.1 (0,2)\n",
      "          for co_c.3 (0,32)\n",
      "            for p_c.4 (0,2)\n",
      "              bgemm.local = ...\n",
      "      for p.3 (0,2)\n",
      "        for co.3 (0,32)\n",
      "          bgemm = ...\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,256)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,2)\n",
      "      unroll vw (0,2)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,784)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_add = ...\n",
      "\n",
      "==================================================\n",
      "No: 167\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      " 6) * 512)) + (((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) & 1023) >> 7) * 64)) + (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) & 63)) + 122880))]);\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.35, Tstamp:1637015575.93)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "data_pack auto_unroll: 512\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,256)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,4)\n",
      "      for nu (0,4)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,4)\n",
      "      unroll nu (0,4)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,512)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,2)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,32)\n",
      "      for ci.0 (0,2)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,128)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "            placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,128)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "            data_pack.shared = ...\n",
      "        for ci.1 (0,32)\n",
      "          for p_c.3 (0,2)\n",
      "            for ci.2 (0,8)\n",
      "              for co_c.4 (0,2)\n",
      "                bgemm.local = ...\n",
      "      for p.3 (0,2)\n",
      "        for co.3 (0,2)\n",
      "          bgemm = ...\n",
      "inverse auto_unroll: 512\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,256)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,2)\n",
      "      unroll vw (0,2)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,784)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_add = ...\n",
      "\n",
      "==================================================\n",
      "No: 168\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      " (((int)threadIdx.x) >> 1)) >> 9) * 1024) + ((((int)threadIdx.x) & 1) * 512)) + (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) & 511)) + 122880))]);\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.21, Tstamp:1637015575.96)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "data_pack auto_unroll: 16\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,256)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,4)\n",
      "      for nu (0,4)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,4)\n",
      "      unroll nu (0,4)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,2)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,4)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,32)\n",
      "      bgemm.local auto_unroll: 16\n",
      "      for ci.0 (0,256)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,256)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "            placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,8)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "            data_pack.shared = ...\n",
      "        for co_c.3 (0,128)\n",
      "          for ci.2 (0,2)\n",
      "            for p_c.4 (0,2)\n",
      "              for co_c.4 (0,2)\n",
      "                bgemm.local = ...\n",
      "      for p.3 (0,2)\n",
      "        for co.3 (0,256)\n",
      "          bgemm = ...\n",
      "inverse auto_unroll: 1024\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,256)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,2)\n",
      "      unroll vw (0,2)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,784)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_add = ...\n",
      "\n",
      "Time elapsed for measurement: 1.88 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:32:55] /home/boyuan/.anaconda3/envs/tvm-build/conda-bld/tvm-cu102-package_1636400145961/work/src/auto_scheduler/measure.cc:299: Warning: Too many errors happened during tuning. Switching to debug mode.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for training: 0.53 s\n",
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |\n",
      "-------------------------------------------------\n",
      "|    0 |            - |              - |      8 |\n",
      "|    1 |            - |              - |      8 |\n",
      "|    2 |            - |              - |      8 |\n",
      "|    3 |            - |              - |      8 |\n",
      "|    4 |            - |              - |      8 |\n",
      "|    5 |            - |              - |      8 |\n",
      "|    6 |            - |              - |      8 |\n",
      "|    7 |            - |              - |      8 |\n",
      "|    8 |            - |              - |      8 |\n",
      "|    9 |            - |              - |      8 |\n",
      "|   10 |            - |              - |      8 |\n",
      "|   11 |            - |              - |      8 |\n",
      "|   12 |            - |              - |      8 |\n",
      "|   13 |            - |              - |      8 |\n",
      "|   14 |            - |              - |      8 |\n",
      "|   15 |            - |              - |      8 |\n",
      "|   16 |            - |              - |      8 |\n",
      "|   17 |            - |              - |      8 |\n",
      "|   18 |            - |              - |      8 |\n",
      "|   19 |            - |              - |      8 |\n",
      "|   20 |            - |              - |      8 |\n",
      "|   21 |            - |              - |      0 |\n",
      "|   22 |            - |              - |      0 |\n",
      "|   23 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 168\tUsed time : 550 s\tNext ID: 21\t\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 1\n",
      "Sample Initial Population\t#s: 64\tfail_ct: 1984\tTime elapsed: 1.89\n",
      "GA Iter: 0\tMax score: 0.9910\tMin score: 0.7239\t#Pop: 16\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 1.0000\tMin score: 0.9980\t#Pop: 16\t#M+: 1393\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 16\tTime elapsed: 27.73\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 8 programs to measure:\n",
      ".E.E.E.E.E.E.E.E\n",
      "==================================================\n",
      "No: 169\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      " 4)) % 7) * 256)) + (((((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) % 112) / 7) * 16)) + (((int)threadIdx.x) & 15)) + 188160))] + input_tile[(15)]);\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.27, Tstamp:1637015607.80)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "data_pack auto_unroll: 64\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,392)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,4)\n",
      "      for nu (0,4)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,4)\n",
      "      unroll nu (0,4)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,2)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,2)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,112)\n",
      "      bgemm.local auto_unroll: 16\n",
      "      for ci.0 (0,64)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,74)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)\n",
      "            placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,28)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)\n",
      "            data_pack.shared = ...\n",
      "        for ci.1 (0,2)\n",
      "          for eps_c.3 (0,2)\n",
      "            for nu_c.3 (0,2)\n",
      "              for co_c.3 (0,4)\n",
      "                for ci.2 (0,2)\n",
      "                  for eps_c.4 (0,2)\n",
      "                    for p_c.4 (0,7)\n",
      "                      for co_c.4 (0,2)\n",
      "                        bgemm.local = ...\n",
      "      for eps.3 (0,4)\n",
      "        for nu.3 (0,2)\n",
      "          for p.3 (0,7)\n",
      "            for co.3 (0,8)\n",
      "              bgemm = ...\n",
      "inverse auto_unroll: 16\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,392)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,2)\n",
      "      unroll vw (0,2)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,50176)\n",
      "  T_add = ...\n",
      "\n",
      "==================================================\n",
      "No: 170\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      "Idx.x)) % 3584) >> 9) * 256)) + (((((int)blockIdx.x) * 16) + ((int)threadIdx.x)) & 255)))] + placeholder[(((((int)blockIdx.x) * 16) + ((int)threadIdx.x)))]);\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.25, Tstamp:1637015607.83)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "data_pack auto_unroll: 1024\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,392)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,4)\n",
      "      for nu (0,4)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,4)\n",
      "      unroll nu (0,4)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,896)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,2)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,56)\n",
      "      bgemm.local auto_unroll: 64\n",
      "      for ci.0 (0,16)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,10)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)\n",
      "            placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,16)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)\n",
      "            data_pack.shared = ...\n",
      "        for ci.2 (0,16)\n",
      "          for nu_c.4 (0,2)\n",
      "            bgemm.local = ...\n",
      "      for nu.3 (0,2)\n",
      "        bgemm = ...\n",
      "inverse auto_unroll: 16\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,392)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,2)\n",
      "      unroll vw (0,2)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,3136)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,16)\n",
      "    T_add = ...\n",
      "\n",
      "==================================================\n",
      "No: 171\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      ") + (((int)threadIdx.x) >> 4)) % 49) * 256) + ((((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) / 49) * 16)) + (((int)threadIdx.x) & 15)) + 188160))]);\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.20, Tstamp:1637015607.86)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "data_pack auto_unroll: 1024\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,392)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,4)\n",
      "      for nu (0,4)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,4)\n",
      "      unroll nu (0,4)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,2)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,2)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,98)\n",
      "      for ci.0 (0,256)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,21)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,98)\n",
      "            placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,4)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,98)\n",
      "            data_pack.shared = ...\n",
      "        for co_c.3 (0,8)\n",
      "          for nu_c.4 (0,4)\n",
      "            for co_c.4 (0,16)\n",
      "              bgemm.local = ...\n",
      "      for nu.3 (0,4)\n",
      "        for co.3 (0,128)\n",
      "          bgemm = ...\n",
      "inverse auto_unroll: 64\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,392)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,2)\n",
      "      unroll vw (0,2)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,1568)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_add = ...\n",
      "\n",
      "==================================================\n",
      "No: 172\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      ") * 32)) + ((int)threadIdx.x)) + 37632))] + bgemm[((((((((int)blockIdx.x) % 49) * 256) + ((((int)blockIdx.x) / 49) * 32)) + ((int)threadIdx.x)) + 188160))]);\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.21, Tstamp:1637015607.89)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "data_pack auto_unroll: 1024\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,392)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,4)\n",
      "      for nu (0,4)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,4)\n",
      "      unroll nu (0,4)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,392)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,4)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,64)\n",
      "      bgemm.local auto_unroll: 64\n",
      "      for ci.0 (0,32)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,64)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)\n",
      "            placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,2)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)\n",
      "            data_pack.shared = ...\n",
      "        for ci.1 (0,8)\n",
      "          for co_c.3 (0,2)\n",
      "            bgemm.local = ...\n",
      "      for co.3 (0,2)\n",
      "        bgemm = ...\n",
      "inverse auto_unroll: 16\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,392)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,2)\n",
      "      unroll vw (0,2)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,1568)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_add = ...\n",
      "\n",
      "==================================================\n",
      "No: 173\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      "(((int)blockIdx.x) * 16)) + ((((int)threadIdx.x) & 7) * 2)) + co_inner))] = bgemm_local[((((eps_inner * 4) + (nu_inner * 2)) + co_inner))];\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.37, Tstamp:1637015607.92)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "data_pack auto_unroll: 512\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,392)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,4)\n",
      "      for nu (0,4)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,4)\n",
      "      unroll nu (0,4)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,16)\n",
      "  threadIdx.x eps.2@nu.2@p.2@co.2@ (0,784)\n",
      "    for ci.0 (0,32)\n",
      "      for ax0@ax1@ax2@ax3@.0.0 (0,3)\n",
      "        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,784)\n",
      "          placeholder.shared = ...\n",
      "      for ax0@ax1@ax2@ax3@.0.0 (0,8)\n",
      "        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,784)\n",
      "          data_pack.shared = ...\n",
      "      for ci.1 (0,8)\n",
      "        for nu_c.3 (0,2)\n",
      "          for eps_c.4 (0,4)\n",
      "            for co_c.4 (0,2)\n",
      "              bgemm.local = ...\n",
      "    for eps.3 (0,4)\n",
      "      for nu.3 (0,2)\n",
      "        for co.3 (0,2)\n",
      "          bgemm = ...\n",
      "inverse auto_unroll: 512\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,392)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,2)\n",
      "      unroll vw (0,2)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,1568)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_add = ...\n",
      "\n",
      "==================================================\n",
      "No: 174\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      " + (((int)threadIdx.x) * 2)) + co_inner))] = bgemm_local[(((((eps_inner * 196) + (nu_inner * 98)) + (p_inner * 2)) + co_inner))];\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.29, Tstamp:1637015607.95)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "data_pack auto_unroll: 16\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,392)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,4)\n",
      "      for nu (0,4)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,4)\n",
      "      unroll nu (0,4)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,8)\n",
      "  threadIdx.x eps.2@nu.2@p.2@co.2@ (0,64)\n",
      "    for ci.0 (0,32)\n",
      "      for ax0@ax1@ax2@ax3@.0.0 (0,64)\n",
      "        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)\n",
      "          placeholder.shared = ...\n",
      "      for ax0@ax1@ax2@ax3@.0.0 (0,2)\n",
      "        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)\n",
      "          vectorize ax0@ax1@ax2@ax3@.1 (0,16)\n",
      "            data_pack.shared = ...\n",
      "      for ci.1 (0,8)\n",
      "        for nu_c.3 (0,2)\n",
      "          for eps_c.4 (0,2)\n",
      "            for p_c.4 (0,49)\n",
      "              for co_c.4 (0,2)\n",
      "                bgemm.local = ...\n",
      "    for eps.3 (0,2)\n",
      "      for nu.3 (0,2)\n",
      "        for p.3 (0,49)\n",
      "          for co.3 (0,2)\n",
      "            bgemm = ...\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,392)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,2)\n",
      "      unroll vw (0,2)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,1568)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_add = ...\n",
      "\n",
      "==================================================\n",
      "No: 175\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      "Idx.x)) % 3584) >> 9) * 256)) + (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) & 255)))] + placeholder[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)))]);\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.34, Tstamp:1637015607.98)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "data_pack auto_unroll: 64\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,392)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,4)\n",
      "      for nu (0,4)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,4)\n",
      "      unroll nu (0,4)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,4)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,7)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,32)\n",
      "      bgemm.local auto_unroll: 512\n",
      "      for ci.0 (0,128)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,64)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "            placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,25)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "            data_pack.shared = ...\n",
      "        for ci.1 (0,2)\n",
      "          for p_c.3 (0,7)\n",
      "            for co_c.3 (0,2)\n",
      "              for eps_c.4 (0,2)\n",
      "                for nu_c.4 (0,2)\n",
      "                  for co_c.4 (0,4)\n",
      "                    bgemm.local = ...\n",
      "      for eps.3 (0,2)\n",
      "        for nu.3 (0,2)\n",
      "          for p.3 (0,7)\n",
      "            for co.3 (0,8)\n",
      "              bgemm = ...\n",
      "inverse auto_unroll: 512\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,196)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,64)\n",
      "    unroll vh (0,2)\n",
      "      unroll vw (0,2)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,1568)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_add = ...\n",
      "\n",
      "==================================================\n",
      "No: 176\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      ") + (((int)threadIdx.x) >> 4)) % 49) * 256) + ((((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) / 49) * 16)) + (((int)threadIdx.x) & 15)) + 188160))]);\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.27, Tstamp:1637015608.01)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "data_pack auto_unroll: 16\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,392)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,4)\n",
      "      for nu (0,4)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,4)\n",
      "      unroll nu (0,4)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,16)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,8)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,98)\n",
      "      bgemm.local auto_unroll: 1024\n",
      "      for ci.0 (0,16)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,42)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,98)\n",
      "            placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,32)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,98)\n",
      "            data_pack.shared = ...\n",
      "        for ci.1 (0,8)\n",
      "          for eps_c.3 (0,2)\n",
      "            for ci.2 (0,2)\n",
      "              for co_c.4 (0,8)\n",
      "                bgemm.local = ...\n",
      "      for eps.3 (0,2)\n",
      "        for co.3 (0,8)\n",
      "          bgemm = ...\n",
      "inverse auto_unroll: 1024\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,392)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,2)\n",
      "      unroll vw (0,2)\n",
      "        unroll r_a (0,4)\n",
      "          unroll r_b (0,4)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,1568)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_add = ...\n",
      "\n",
      "Time elapsed for measurement: 1.81 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:33:28] /home/boyuan/.anaconda3/envs/tvm-build/conda-bld/tvm-cu102-package_1636400145961/work/src/auto_scheduler/measure.cc:299: Warning: Too many errors happened during tuning. Switching to debug mode.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for training: 0.54 s\n",
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "-------------------------------------------------\n",
      "|    0 |            - |              - |      8 |\n",
      "|    1 |            - |              - |      8 |\n",
      "|    2 |            - |              - |      8 |\n",
      "|    3 |            - |              - |      8 |\n",
      "|    4 |            - |              - |      8 |\n",
      "|    5 |            - |              - |      8 |\n",
      "|    6 |            - |              - |      8 |\n",
      "|    7 |            - |              - |      8 |\n",
      "|    8 |            - |              - |      8 |\n",
      "|    9 |            - |              - |      8 |\n",
      "|   10 |            - |              - |      8 |\n",
      "|   11 |            - |              - |      8 |\n",
      "|   12 |            - |              - |      8 |\n",
      "|   13 |            - |              - |      8 |\n",
      "|   14 |            - |              - |      8 |\n",
      "|   15 |            - |              - |      8 |\n",
      "|   16 |            - |              - |      8 |\n",
      "|   17 |            - |              - |      8 |\n",
      "|   18 |            - |              - |      8 |\n",
      "|   19 |            - |              - |      8 |\n",
      "|   20 |            - |              - |      8 |\n",
      "|   21 |            - |              - |      8 |\n",
      "|   22 |            - |              - |      0 |\n",
      "|   23 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 176\tUsed time : 582 s\tNext ID: 22\t\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 1\n",
      "Sample Initial Population\t#s: 148\tfail_ct: 1900\tTime elapsed: 0.90\n",
      "GA Iter: 0\tMax score: 0.9952\tMin score: 0.8082\t#Pop: 16\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 1.0000\tMin score: 0.9970\t#Pop: 16\t#M+: 1400\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 16\tTime elapsed: 11.17\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 8 programs to measure:\n",
      ".E.E.E.E.E.E.E.E\n",
      "==================================================\n",
      "No: 177\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      " * 3584) + ((((int)threadIdx.x) >> 3) * 256)) + (((int)blockIdx.x) * 16)) + (((int)threadIdx.x) & 7)) + 25096))] = Conv2dOutput_local[((yy_inner + 21))];\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.11, Tstamp:1637015622.06)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder\n",
      "blockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,16)\n",
      "  vthread nn.1@yy.1@xx.1@ff.1@ (0,4)\n",
      "    threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,112)\n",
      "      Conv2dOutput.local auto_unroll: 64\n",
      "      for rc.0 (0,128)\n",
      "        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)\n",
      "          vectorize ax0@ax1@ax2@ax3@.1 (0,2)\n",
      "            placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,4)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)\n",
      "            vectorize ax0@ax1@ax2@ax3@.1 (0,2)\n",
      "              PaddedInput.shared = ...\n",
      "        for yy_c.3 (0,7)\n",
      "          Conv2dOutput.local = ...\n",
      "      for yy.3 (0,7)\n",
      "        Conv2dOutput = ...\n",
      "\n",
      "==================================================\n",
      "No: 178\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      "dx.x) * 512)) + (xx_inner * 256)) + ((((int)blockIdx.x) & 7) * 32)) + ff_inner) + 24))] = Conv2dOutput_local[((((xx_inner * 8) + ff_inner) + 48))];\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.12, Tstamp:1637015622.07)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder\n",
      "blockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,16)\n",
      "  vthread nn.1@yy.1@xx.1@ff.1@ (0,4)\n",
      "    threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,49)\n",
      "      Conv2dOutput.local auto_unroll: 512\n",
      "      for rc.0 (0,64)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,2)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,49)\n",
      "            placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,15)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,49)\n",
      "            PaddedInput.shared = ...\n",
      "        for xx_c.3 (0,2)\n",
      "          for ff_c.3 (0,8)\n",
      "            for rc.2 (0,2)\n",
      "              Conv2dOutput.local = ...\n",
      "      for xx.3 (0,2)\n",
      "        for ff.3 (0,8)\n",
      "          Conv2dOutput = ...\n",
      "\n",
      "==================================================\n",
      "No: 179\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      ") * 3584)) + (xx_inner * 256)) + ((((int)threadIdx.x) & 63) * 2)) + ff_inner) + 1920))] = Conv2dOutput_local[((((xx_inner * 2) + ff_inner) + 42))];\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.16, Tstamp:1637015622.08)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder\n",
      "blockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,7)\n",
      "  vthread nn.1@yy.1@xx.1@ff.1@ (0,4)\n",
      "    threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,128)\n",
      "      Conv2dOutput.local auto_unroll: 512\n",
      "      for rc.0 (0,16)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,8)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)\n",
      "            vectorize ax0@ax1@ax2@ax3@.1 (0,2)\n",
      "              placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,6)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)\n",
      "            PaddedInput.shared = ...\n",
      "        for rc.1 (0,4)\n",
      "          for ff_c.3 (0,2)\n",
      "            for rc.2 (0,2)\n",
      "              for xx_c.4 (0,7)\n",
      "                Conv2dOutput.local = ...\n",
      "      for xx.3 (0,7)\n",
      "        for ff.3 (0,2)\n",
      "          Conv2dOutput = ...\n",
      "\n",
      "==================================================\n",
      "No: 180\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      "_inner * 3584)) + ((((int)blockIdx.x) % 7) * 512)) + (((int)threadIdx.x) * 16)) + ff_inner))] = Conv2dOutput_local[(((yy_inner * 16) + ff_inner))];\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.18, Tstamp:1637015622.08)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder\n",
      "blockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,14)\n",
      "  threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,32)\n",
      "    Conv2dOutput.local auto_unroll: 1024\n",
      "    for rc.0 (0,4)\n",
      "      for ax0@ax1@ax2@ax3@.0.0 (0,64)\n",
      "        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "          vectorize ax0@ax1@ax2@ax3@.1 (0,4)\n",
      "            placeholder.shared = ...\n",
      "      for ax0@ax1@ax2@ax3@.0.0 (0,39)\n",
      "        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "          PaddedInput.shared = ...\n",
      "      for rc.1 (0,8)\n",
      "        for ff_c.3 (0,16)\n",
      "          for rc.2 (0,4)\n",
      "            for yy_c.4 (0,7)\n",
      "              Conv2dOutput.local = ...\n",
      "    for yy.3 (0,7)\n",
      "      for ff.3 (0,16)\n",
      "        Conv2dOutput = ...\n",
      "\n",
      "==================================================\n",
      "No: 181\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      "6)) + ((((int)threadIdx.x) & 7) * 8)) + ff_inner) + 192))] = Conv2dOutput_local[(((((yy_inner * 56) + (xx_inner * 8)) + ff_inner) + 336))];\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.12, Tstamp:1637015622.09)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder\n",
      "vthread nn.1@yy.1@xx.1@ff.1@ (0,4)\n",
      "  threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,112)\n",
      "    Conv2dOutput.local auto_unroll: 64\n",
      "    for rc.0 (0,64)\n",
      "      for ax0@ax1@ax2@ax3@.0.0 (0,5)\n",
      "        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)\n",
      "          placeholder.shared = ...\n",
      "      for ax0@ax1@ax2@ax3@.0.0 (0,14)\n",
      "        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)\n",
      "          PaddedInput.shared = ...\n",
      "      for rc.1 (0,2)\n",
      "        for yy_c.3 (0,2)\n",
      "          for xx_c.4 (0,7)\n",
      "            for ff_c.4 (0,8)\n",
      "              Conv2dOutput.local = ...\n",
      "    for yy.3 (0,2)\n",
      "      for xx.3 (0,7)\n",
      "        for ff.3 (0,8)\n",
      "          Conv2dOutput = ...\n",
      "\n",
      "==================================================\n",
      "No: 182\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      ")blockIdx.x) >> 1) * 3584) + (xx_inner * 256)) + ((((int)blockIdx.x) & 1) * 128)) + ((int)threadIdx.x)) + 1792))] = Conv2dOutput_local[((xx_inner + 7))];\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.11, Tstamp:1637015622.10)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder\n",
      "blockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,28)\n",
      "  vthread nn.1@yy.1@xx.1@ff.1@ (0,2)\n",
      "    threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,128)\n",
      "      Conv2dOutput.local auto_unroll: 64\n",
      "      for rc.0 (0,32)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,4)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)\n",
      "            placeholder.shared = ...\n",
      "        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)\n",
      "          PaddedInput.shared = ...\n",
      "        for rc.1 (0,4)\n",
      "          for xx_c.3 (0,7)\n",
      "            Conv2dOutput.local = ...\n",
      "      for xx.3 (0,7)\n",
      "        Conv2dOutput = ...\n",
      "\n",
      "==================================================\n",
      "No: 183\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      "x.x) >> 7) * 512)) + (xx_inner * 256)) + ((((int)blockIdx.x) & 1) * 128)) + (((int)threadIdx.x) & 127)) + 3584))] = Conv2dOutput_local[((xx_inner + 2))];\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.11, Tstamp:1637015622.11)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder\n",
      "blockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,14)\n",
      "  vthread nn.1@yy.1@xx.1@ff.1@ (0,2)\n",
      "    threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,896)\n",
      "      Conv2dOutput.local auto_unroll: 16\n",
      "      for rc.0 (0,8)\n",
      "        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,896)\n",
      "          vectorize ax0@ax1@ax2@ax3@.1 (0,4)\n",
      "            placeholder.shared = ...\n",
      "        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,896)\n",
      "          vectorize ax0@ax1@ax2@ax3@.1 (0,4)\n",
      "            PaddedInput.shared = ...\n",
      "        for rc.1 (0,2)\n",
      "          for rc.2 (0,8)\n",
      "            for xx_c.4 (0,2)\n",
      "              Conv2dOutput.local = ...\n",
      "      for xx.3 (0,2)\n",
      "        Conv2dOutput = ...\n",
      "\n",
      "==================================================\n",
      "No: 184\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      "x_inner * 256)) + (((int)blockIdx.x) * 128)) + ((((int)threadIdx.x) & 3) * 32)) + ff_inner))] = Conv2dOutput_local[(((xx_inner * 32) + ff_inner))];\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.10, Tstamp:1637015622.11)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder\n",
      "blockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,2)\n",
      "  threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,56)\n",
      "    for rc.0 (0,64)\n",
      "      for ax0@ax1@ax2@ax3@.0.0 (0,5)\n",
      "        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)\n",
      "          placeholder.shared = ...\n",
      "      for ax0@ax1@ax2@ax3@.0.0 (0,27)\n",
      "        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)\n",
      "          PaddedInput.shared = ...\n",
      "      for rc.1 (0,2)\n",
      "        for xx_c.3 (0,2)\n",
      "          for ff_c.3 (0,4)\n",
      "            for xx_c.4 (0,7)\n",
      "              for ff_c.4 (0,8)\n",
      "                Conv2dOutput.local = ...\n",
      "    for xx.3 (0,14)\n",
      "      for ff.3 (0,32)\n",
      "        Conv2dOutput = ...\n",
      "\n",
      "Time elapsed for measurement: 1.43 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:33:42] /home/boyuan/.anaconda3/envs/tvm-build/conda-bld/tvm-cu102-package_1636400145961/work/src/auto_scheduler/measure.cc:299: Warning: Too many errors happened during tuning. Switching to debug mode.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for training: 0.48 s\n",
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ -------------------------------------------------\n",
      "|    0 |            - |              - |      8 |\n",
      "|    1 |            - |              - |      8 |\n",
      "|    2 |            - |              - |      8 |\n",
      "|    3 |            - |              - |      8 |\n",
      "|    4 |            - |              - |      8 |\n",
      "|    5 |            - |              - |      8 |\n",
      "|    6 |            - |              - |      8 |\n",
      "|    7 |            - |              - |      8 |\n",
      "|    8 |            - |              - |      8 |\n",
      "|    9 |            - |              - |      8 |\n",
      "|   10 |            - |              - |      8 |\n",
      "|   11 |            - |              - |      8 |\n",
      "|   12 |            - |              - |      8 |\n",
      "|   13 |            - |              - |      8 |\n",
      "|   14 |            - |              - |      8 |\n",
      "|   15 |            - |              - |      8 |\n",
      "|   16 |            - |              - |      8 |\n",
      "|   17 |            - |              - |      8 |\n",
      "|   18 |            - |              - |      8 |\n",
      "|   19 |            - |              - |      8 |\n",
      "|   20 |            - |              - |      8 |\n",
      "|   21 |            - |              - |      8 |\n",
      "|   22 |            - |              - |      8 |\n",
      "|   23 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 184\tUsed time : 596 s\tNext ID: 23\t\n",
      "Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 1\n",
      "Sample Initial Population\t#s: 105\tfail_ct: 1943\tTime elapsed: 1.17\n",
      "GA Iter: 0\tMax score: 0.9977\tMin score: 0.8924\t#Pop: 16\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 0.9999\tMin score: 0.9985\t#Pop: 16\t#M+: 1383\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 16\tTime elapsed: 17.26\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 8 programs to measure:\n",
      ".E.E.E.E.E.E.E.E\n",
      "==================================================\n",
      "No: 185\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      "max((Conv2dOutput[((((ax1_inner * 4) + ax2_inner) + 24))] + placeholder2[((((((int)blockIdx.x) & 1) * 32) + ((int)threadIdx.x)))]), 0.000000e+00f);\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.12, Tstamp:1637015642.47)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "blockIdx.x ax0.0@ax1.0@ax2.0@ax3.0@ (0,784)\n",
      "  vthread ax0.1@ax1.1@ax2.1@ax3.1@ (0,4)\n",
      "    threadIdx.x ax0.2@ax1.2@ax2.2@ax3.2@ (0,32)\n",
      "      for ax0@ax1@ax2@ax3@.0.0 (0,147)\n",
      "        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "          placeholder.shared = ...\n",
      "      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "        vectorize ax0@ax1@ax2@ax3@.1 (0,27)\n",
      "          PaddedInput.shared = ...\n",
      "      for ry.1 (0,7)\n",
      "        for rx.1 (0,7)\n",
      "          for rc.1 (0,3)\n",
      "            for yy.4 (0,2)\n",
      "              for xx.4 (0,4)\n",
      "                Conv2dOutput = ...\n",
      "      for ax1.3 (0,2)\n",
      "        for ax2.3 (0,4)\n",
      "          T_relu = ...\n",
      "\n",
      "==================================================\n",
      "No: 186\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      "3_inner) + 64))] + placeholder2[((((((((int)blockIdx.x) & 1) * 32) + ((((int)threadIdx.x) & 1) * 8)) + ax3_inner) + 16))]), 0.000000e+00f);\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.31, Tstamp:1637015642.47)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "blockIdx.x ax0.0@ax1.0@ax2.0@ax3.0@ (0,28)\n",
      "  vthread ax0.1@ax1.1@ax2.1@ax3.1@ (0,2)\n",
      "    threadIdx.x ax0.2@ax1.2@ax2.2@ax3.2@ (0,224)\n",
      "      Conv2dOutput auto_unroll: 1024\n",
      "      for rc.0 (0,3)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,7)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,224)\n",
      "            placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,20)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,224)\n",
      "            PaddedInput.shared = ...\n",
      "        for yy.3 (0,2)\n",
      "          for ry.2 (0,7)\n",
      "            for rx.2 (0,7)\n",
      "              for xx.4 (0,4)\n",
      "                for ff.4 (0,8)\n",
      "                  Conv2dOutput = ...\n",
      "      for ax1.3 (0,2)\n",
      "        for ax2.3 (0,4)\n",
      "          for ax3.3 (0,8)\n",
      "            T_relu = ...\n",
      "\n",
      "==================================================\n",
      "No: 187\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      "utput[((((ax1_inner * 2) + ax2_inner) + 24))] + placeholder2[(((((((int)blockIdx.x) & 3) * 16) + (((int)threadIdx.x) & 7)) + 8))]), 0.000000e+00f);\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.19, Tstamp:1637015642.48)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "blockIdx.x ax0.0@ax1.0@ax2.0@ax3.0@ (0,784)\n",
      "  vthread ax0.1@ax1.1@ax2.1@ax3.1@ (0,4)\n",
      "    threadIdx.x ax0.2@ax1.2@ax2.2@ax3.2@ (0,32)\n",
      "      Conv2dOutput auto_unroll: 512\n",
      "      for ax0@ax1@ax2@ax3@.0.0 (0,74)\n",
      "        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "          placeholder.shared = ...\n",
      "      for ax0@ax1@ax2@ax3@.0.0 (0,46)\n",
      "        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "          PaddedInput.shared = ...\n",
      "      for ry.1 (0,7)\n",
      "        for rx.1 (0,7)\n",
      "          for xx.3 (0,2)\n",
      "            for rc.2 (0,3)\n",
      "              for yy.4 (0,4)\n",
      "                Conv2dOutput = ...\n",
      "      for ax1.3 (0,4)\n",
      "        for ax2.3 (0,2)\n",
      "          T_relu = ...\n",
      "\n",
      "==================================================\n",
      "No: 188\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      ".x) & 7)) + 56))] = max((Conv2dOutput[((((ax1_inner * 14) + ax2_inner) + 392))] + placeholder2[(((((int)threadIdx.x) & 7) + 56))]), 0.000000e+00f);\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.12, Tstamp:1637015642.49)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "blockIdx.x ax0.0@ax1.0@ax2.0@ax3.0@ (0,28)\n",
      "  vthread ax0.1@ax1.1@ax2.1@ax3.1@ (0,8)\n",
      "    threadIdx.x ax0.2@ax1.2@ax2.2@ax3.2@ (0,64)\n",
      "      Conv2dOutput auto_unroll: 64\n",
      "      for ry.0 (0,7)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,21)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)\n",
      "            placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,83)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)\n",
      "            PaddedInput.shared = ...\n",
      "        for rx.1 (0,7)\n",
      "          for yy.3 (0,4)\n",
      "            for rc.2 (0,3)\n",
      "              for xx.4 (0,14)\n",
      "                Conv2dOutput = ...\n",
      "      for ax1.3 (0,4)\n",
      "        for ax2.3 (0,14)\n",
      "          T_relu = ...\n",
      "\n",
      "==================================================\n",
      "No: 189\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      " = max((Conv2dOutput[((((ax1_inner * 2) + ax3_inner) + 56))] + placeholder2[(((((((int)blockIdx.x) & 15) * 4) + ax3_inner) + 2))]), 0.000000e+00f);\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.12, Tstamp:1637015642.51)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "blockIdx.x ax0.0@ax1.0@ax2.0@ax3.0@ (0,112)\n",
      "  vthread ax0.1@ax1.1@ax2.1@ax3.1@ (0,8)\n",
      "    threadIdx.x ax0.2@ax1.2@ax2.2@ax3.2@ (0,112)\n",
      "      Conv2dOutput auto_unroll: 64\n",
      "      for rx.0 (0,7)\n",
      "        for rc.0 (0,3)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)\n",
      "            placeholder.shared = ...\n",
      "          for ax0@ax1@ax2@ax3@.0.0 (0,74)\n",
      "            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)\n",
      "              PaddedInput.shared = ...\n",
      "          for ry.1 (0,7)\n",
      "            for ff.3 (0,2)\n",
      "              for yy.4 (0,4)\n",
      "                Conv2dOutput = ...\n",
      "      for ax1.3 (0,4)\n",
      "        for ax3.3 (0,2)\n",
      "          T_relu = ...\n",
      "\n",
      "==================================================\n",
      "No: 190\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      " + 43012))] = max((Conv2dOutput[((ax2_inner + 182))] + placeholder2[(((((((int)blockIdx.x) & 7) * 8) + (((int)threadIdx.x) & 3)) + 4))]), 0.000000e+00f);\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.12, Tstamp:1637015642.52)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "blockIdx.x ax0.0@ax1.0@ax2.0@ax3.0@ (0,128)\n",
      "  vthread ax0.1@ax1.1@ax2.1@ax3.1@ (0,14)\n",
      "    threadIdx.x ax0.2@ax1.2@ax2.2@ax3.2@ (0,32)\n",
      "      Conv2dOutput auto_unroll: 64\n",
      "      for ry.0 (0,7)\n",
      "        for rx.0 (0,7)\n",
      "          for rc.0 (0,3)\n",
      "            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "              vectorize ax0@ax1@ax2@ax3@.1 (0,7)\n",
      "                placeholder.shared = ...\n",
      "            for ax0@ax1@ax2@ax3@.0.0 (0,91)\n",
      "              threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "                PaddedInput.shared = ...\n",
      "            for xx.3 (0,14)\n",
      "              Conv2dOutput = ...\n",
      "      for ax2.3 (0,14)\n",
      "        T_relu = ...\n",
      "\n",
      "==================================================\n",
      "No: 191\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      ") * 64)) + ax3_inner) + 2720))] = max((Conv2dOutput[((((ax1_inner * 32) + ax3_inner) + 3136))] + placeholder2[((ax3_inner + 32))]), 0.000000e+00f);\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.12, Tstamp:1637015642.53)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "blockIdx.x ax0.0@ax1.0@ax2.0@ax3.0@ (0,8)\n",
      "  vthread ax0.1@ax1.1@ax2.1@ax3.1@ (0,8)\n",
      "    threadIdx.x ax0.2@ax1.2@ax2.2@ax3.2@ (0,28)\n",
      "      Conv2dOutput auto_unroll: 64\n",
      "      for rx.0 (0,7)\n",
      "        for rc.0 (0,3)\n",
      "          for ax0@ax1@ax2@ax3@.0.0 (0,16)\n",
      "            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,28)\n",
      "              placeholder.shared = ...\n",
      "          for ax0@ax1@ax2@ax3@.0.0 (0,242)\n",
      "            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,28)\n",
      "              PaddedInput.shared = ...\n",
      "          for ff.3 (0,8)\n",
      "            for ry.2 (0,7)\n",
      "              for yy.4 (0,14)\n",
      "                for ff.4 (0,4)\n",
      "                  Conv2dOutput = ...\n",
      "      for ax1.3 (0,14)\n",
      "        for ax3.3 (0,32)\n",
      "          T_relu = ...\n",
      "\n",
      "==================================================\n",
      "No: 192\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      "ner * 112) + (ax2_inner * 8)) + ax3_inner) + 672))] + placeholder2[(((((((int)threadIdx.x) & 3) * 8) + ax3_inner) + 32))]), 0.000000e+00f);\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.19, Tstamp:1637015642.54)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "blockIdx.x ax0.0@ax1.0@ax2.0@ax3.0@ (0,28)\n",
      "  vthread ax0.1@ax1.1@ax2.1@ax3.1@ (0,4)\n",
      "    threadIdx.x ax0.2@ax1.2@ax2.2@ax3.2@ (0,32)\n",
      "      Conv2dOutput auto_unroll: 64\n",
      "      for ry.0 (0,7)\n",
      "        for rc.0 (0,3)\n",
      "          for ax0@ax1@ax2@ax3@.0.0 (0,14)\n",
      "            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "              placeholder.shared = ...\n",
      "          for ax0@ax1@ax2@ax3@.0.0 (0,60)\n",
      "            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "              PaddedInput.shared = ...\n",
      "          for rx.1 (0,7)\n",
      "            for yy.3 (0,2)\n",
      "              for xx.3 (0,2)\n",
      "                for ff.3 (0,8)\n",
      "                  for xx.4 (0,7)\n",
      "                    Conv2dOutput = ...\n",
      "      for ax1.3 (0,2)\n",
      "        for ax2.3 (0,14)\n",
      "          for ax3.3 (0,8)\n",
      "            T_relu = ...\n",
      "\n",
      "Time elapsed for measurement: 1.46 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:34:02] /home/boyuan/.anaconda3/envs/tvm-build/conda-bld/tvm-cu102-package_1636400145961/work/src/auto_scheduler/measure.cc:299: Warning: Too many errors happened during tuning. Switching to debug mode.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for training: |  ID  | Latency (ms) | Speed (GFLOPS) | Trials |\n",
      "-------------------------------------------------\n",
      "|    0 |            - |              - |      8 |\n",
      "|    1 |            - |              - |      8 |\n",
      "|    2 |            - |              - |      8 |\n",
      "|    3 |            - |              - |      8 |\n",
      "|    4 |            - |              - |      8 |\n",
      "|    5 |            - |              - |      8 |\n",
      "|    6 |            - |              - |      8 |\n",
      "|    7 |            - |              - |      8 |\n",
      "|    8 |            - |              - |      8 |\n",
      "|    9 |            - |              - |      8 |\n",
      "|   10 |            - |              - |      8 |\n",
      "|   11 |            - |              - |      8 |\n",
      "|   12 |            - |              - |      8 |\n",
      "|   13 |            - |              - |      8 |\n",
      "|   14 |            - |              - |      8 |\n",
      "|   15 |            - |              - |      8 |\n",
      "|   16 |            - |              - |      8 |0.52 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "|   17 |            - |              - |      8 |\n",
      "|   18 |            - |              - |      8 |\n",
      "|   19 |            - |              - |      8 |\n",
      "|   20 |            - |              - |      8 |\n",
      "|   21 |            - |              - |      8 |\n",
      "|   22 |            - |              - |      8 |\n",
      "|   23 |            - |              - |      8 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 192\tUsed time : 616 s\tNext ID: 6\t\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Sample Initial Population\t#s: 82\tfail_ct: 1966\tTime elapsed: 2.17\n",
      "GA Iter: 0\tMax score: 0.9961\tMin score: 0.8369\t#Pop: 16\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 0.9998\tMin score: 0.9977\t#Pop: 16\t#M+: 1399\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 16\tTime elapsed: 30.96\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 8 programs to measure:\n",
      ".E.E.E.E.E.E.E.E\n",
      "==================================================\n",
      "No: 193\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      " * 64)) + (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) & 63)))] + placeholder[((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) & 63))]), 0.000000e+00f);\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:1.12, Tstamp:1637015679.13)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "data_pack auto_unroll: 1024\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,392)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,6)\n",
      "      for nu (0,6)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,6)\n",
      "      unroll nu (0,6)\n",
      "        unroll r_a (0,6)\n",
      "          unroll r_b (0,6)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,98)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,2)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,48)\n",
      "      bgemm.local auto_unroll: 16\n",
      "      for ci.0 (0,32)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,48)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,48)\n",
      "            placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,6)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,48)\n",
      "            data_pack.shared = ...\n",
      "        for eps_c.3 (0,2)\n",
      "          for nu_c.3 (0,3)\n",
      "            for p_c.3 (0,2)\n",
      "              for ci.2 (0,2)\n",
      "                for co_c.4 (0,4)\n",
      "                  bgemm.local = ...\n",
      "      for eps.3 (0,2)\n",
      "        for nu.3 (0,3)\n",
      "          for p.3 (0,2)\n",
      "            for co.3 (0,4)\n",
      "              bgemm = ...\n",
      "inverse auto_unroll: 1024\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,392)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,4)\n",
      "      unroll vw (0,4)\n",
      "        unroll r_a (0,6)\n",
      "          unroll r_b (0,6)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,6272)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_relu = ...\n",
      "\n",
      "==================================================\n",
      "No: 194\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      "28) + (((((int)threadIdx.x) & 3) >> 1) * 64)) + ((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) & 31) * 2)) + (((int)threadIdx.x) & 1)) + 439040))]);\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:1.36, Tstamp:1637015679.17)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "data_pack auto_unroll: 1024\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,1568)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,8)\n",
      "    for eps (0,6)\n",
      "      for nu (0,6)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,6)\n",
      "      unroll nu (0,6)\n",
      "        unroll r_a (0,6)\n",
      "          unroll r_b (0,6)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,18)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,4)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,8)\n",
      "      bgemm.local auto_unroll: 16\n",
      "      for ci.0 (0,16)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,128)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,8)\n",
      "            placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,196)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,8)\n",
      "            data_pack.shared = ...\n",
      "        for ci.1 (0,4)\n",
      "          for eps_c.3 (0,2)\n",
      "            for p_c.3 (0,7)\n",
      "              for co_c.3 (0,2)\n",
      "                for p_c.4 (0,14)\n",
      "                  for co_c.4 (0,2)\n",
      "                    bgemm.local = ...\n",
      "      for eps.3 (0,2)\n",
      "        for p.3 (0,98)\n",
      "          for co.3 (0,4)\n",
      "            bgemm = ...\n",
      "inverse auto_unroll: 1024\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,392)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,4)\n",
      "      unroll vw (0,4)\n",
      "        unroll r_a (0,6)\n",
      "          unroll r_b (0,6)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,6272)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_relu = ...\n",
      "\n",
      "==================================================\n",
      "No: 195\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      " * 64)) + (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) & 63)))] + placeholder[((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) & 63))]), 0.000000e+00f);\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:1.22, Tstamp:1637015679.21)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "data_pack auto_unroll: 64\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,392)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,6)\n",
      "      for nu (0,6)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,6)\n",
      "      unroll nu (0,6)\n",
      "        unroll r_a (0,6)\n",
      "          unroll r_b (0,6)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,28)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,2)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,96)\n",
      "      bgemm.local auto_unroll: 1024\n",
      "      for ci.0 (0,16)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,24)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,96)\n",
      "            placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,21)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,96)\n",
      "            data_pack.shared = ...\n",
      "        for ci.2 (0,4)\n",
      "          for eps_c.4 (0,3)\n",
      "            for p_c.4 (0,14)\n",
      "              for co_c.4 (0,2)\n",
      "                bgemm.local = ...\n",
      "      for eps.3 (0,3)\n",
      "        for p.3 (0,14)\n",
      "          for co.3 (0,2)\n",
      "            bgemm = ...\n",
      "inverse auto_unroll: 512\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,392)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,4)\n",
      "      unroll vw (0,4)\n",
      "        unroll r_a (0,6)\n",
      "          unroll r_b (0,6)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,6272)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_relu = ...\n",
      "\n",
      "==================================================\n",
      "No: 196\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      " >> 2)) % 7) * 64)) + (((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) % 112) / 7) * 4)) + (((int)threadIdx.x) & 3)) + 439040))] + input_tile[(35)]);\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:1.37, Tstamp:1637015679.25)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "data_pack auto_unroll: 16\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,392)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,6)\n",
      "      for nu (0,6)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,6)\n",
      "      unroll nu (0,6)\n",
      "        unroll r_a (0,6)\n",
      "          unroll r_b (0,6)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,8)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,4)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,196)\n",
      "      bgemm.local auto_unroll: 512\n",
      "      for ci.0 (0,32)\n",
      "        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,196)\n",
      "          vectorize ax0@ax1@ax2@ax3@.1 (0,12)\n",
      "            placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,9)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,196)\n",
      "            vectorize ax0@ax1@ax2@ax3@.1 (0,4)\n",
      "              data_pack.shared = ...\n",
      "        for p_c.3 (0,2)\n",
      "          for ci.2 (0,2)\n",
      "            for eps_c.4 (0,3)\n",
      "              for nu_c.4 (0,3)\n",
      "                for co_c.4 (0,4)\n",
      "                  bgemm.local = ...\n",
      "      for eps.3 (0,3)\n",
      "        for nu.3 (0,3)\n",
      "          for p.3 (0,2)\n",
      "            for co.3 (0,4)\n",
      "              bgemm = ...\n",
      "inverse auto_unroll: 64\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,392)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,4)\n",
      "      unroll vw (0,4)\n",
      "        unroll r_a (0,6)\n",
      "          unroll r_b (0,6)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,6272)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_relu = ...\n",
      "\n",
      "==================================================\n",
      "No: 197\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      "> 2)) % 49) * 64)) + (((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) % 784) / 49) * 4)) + (((int)threadIdx.x) & 3)) + 439040))] + input_tile[(35)]);\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:1.49, Tstamp:1637015679.29)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "data_pack auto_unroll: 1024\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,392)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,6)\n",
      "      for nu (0,6)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,6)\n",
      "      unroll nu (0,6)\n",
      "        unroll r_a (0,6)\n",
      "          unroll r_b (0,6)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,56)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,6)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,224)\n",
      "      bgemm.local auto_unroll: 64\n",
      "      for ci.0 (0,64)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,3)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,224)\n",
      "            placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,3)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,224)\n",
      "            data_pack.shared = ...\n",
      "        for eps_c.4 (0,2)\n",
      "          for nu_c.4 (0,3)\n",
      "            bgemm.local = ...\n",
      "      for eps.3 (0,2)\n",
      "        for nu.3 (0,3)\n",
      "          bgemm = ...\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,784)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,16)\n",
      "    unroll vh (0,4)\n",
      "      unroll vw (0,4)\n",
      "        unroll r_a (0,6)\n",
      "          unroll r_b (0,6)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,6272)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_relu = ...\n",
      "\n",
      "==================================================\n",
      "No: 198\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      " * 64)) + (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) & 63)))] + placeholder[((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) & 63))]), 0.000000e+00f);\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:1.01, Tstamp:1637015679.33)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "data_pack auto_unroll: 1024\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,392)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,6)\n",
      "      for nu (0,6)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,6)\n",
      "      unroll nu (0,6)\n",
      "        unroll r_a (0,6)\n",
      "          unroll r_b (0,6)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,64)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,7)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,12)\n",
      "      bgemm.local auto_unroll: 512\n",
      "      for ci.0 (0,64)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,3)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,12)\n",
      "            placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,588)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,12)\n",
      "            data_pack.shared = ...\n",
      "        for p_c.3 (0,4)\n",
      "          for eps_c.4 (0,3)\n",
      "            for p_c.4 (0,7)\n",
      "              bgemm.local = ...\n",
      "      for eps.3 (0,3)\n",
      "        for p.3 (0,28)\n",
      "          bgemm = ...\n",
      "inverse auto_unroll: 512\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,392)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,4)\n",
      "      unroll vw (0,4)\n",
      "        unroll r_a (0,6)\n",
      "          unroll r_b (0,6)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,6272)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_relu = ...\n",
      "\n",
      "==================================================\n",
      "No: 199\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      " * 64)) + (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) & 63)))] + placeholder[((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) & 63))]), 0.000000e+00f);\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:1.17, Tstamp:1637015679.37)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "data_pack auto_unroll: 512\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,392)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,6)\n",
      "      for nu (0,6)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,6)\n",
      "      unroll nu (0,6)\n",
      "        unroll r_a (0,6)\n",
      "          unroll r_b (0,6)\n",
      "            data_pack = ...\n",
      "vthread eps.1@nu.1@p.1@co.1@ (0,2)\n",
      "  threadIdx.x eps.2@nu.2@p.2@co.2@ (0,196)\n",
      "    bgemm.local auto_unroll: 64\n",
      "    for ci.0 (0,64)\n",
      "      for ax0@ax1@ax2@ax3@.0.0 (0,12)\n",
      "        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,196)\n",
      "          placeholder.shared = ...\n",
      "      for ax0@ax1@ax2@ax3@.0.0 (0,36)\n",
      "        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,196)\n",
      "          data_pack.shared = ...\n",
      "      for eps_c.3 (0,6)\n",
      "        for nu_c.3 (0,6)\n",
      "          for co_c.3 (0,4)\n",
      "            for p_c.4 (0,4)\n",
      "              for co_c.4 (0,2)\n",
      "                bgemm.local = ...\n",
      "    for eps.3 (0,6)\n",
      "      for nu.3 (0,6)\n",
      "        for p.3 (0,4)\n",
      "          for co.3 (0,8)\n",
      "            bgemm = ...\n",
      "inverse auto_unroll: 16\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,12544)\n",
      "  unroll vh (0,4)\n",
      "    unroll vw (0,4)\n",
      "      unroll r_a (0,6)\n",
      "        unroll r_b (0,6)\n",
      "          inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,6272)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_relu = ...\n",
      "\n",
      "==================================================\n",
      "No: 200\tGFLOPS: 0.00 / 0.00\tresults: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):\n",
      "  File \"/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\", line 631, in _local_build_worker\n",
      "    func = build_module.build(sch, args, target=task.target)\n",
      "  File \"/home/boyuan\n",
      "...\n",
      "& 15) >> 3) * 448)) + (p_inner * 64)) + ((((int)threadIdx.x) & 7) * 8)) + co_inner) + 225792))] = bgemm_local[((((p_inner * 8) + co_inner) + 56))];\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Compilation error:\n",
      "nvcc fatal   : Value 'sm_86' is not defined for option 'gpu-architecture'\n",
      "\n",
      ", all_cost:0.94, Tstamp:1637015679.40)\n",
      "==================================================\n",
      "Placeholder: placeholder, placeholder, placeholder\n",
      "data_pack auto_unroll: 1024\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,392)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,6)\n",
      "      for nu (0,6)\n",
      "        input_tile = ...\n",
      "    unroll eps (0,6)\n",
      "      unroll nu (0,6)\n",
      "        unroll r_a (0,6)\n",
      "          unroll r_b (0,6)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,42)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,2)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,96)\n",
      "      bgemm.local auto_unroll: 1024\n",
      "      for ci.0 (0,8)\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,64)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,96)\n",
      "            placeholder.shared = ...\n",
      "        for ax0@ax1@ax2@ax3@.0.0 (0,14)\n",
      "          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,96)\n",
      "            data_pack.shared = ...\n",
      "        for ci.1 (0,4)\n",
      "          for p_c.3 (0,7)\n",
      "            for co_c.3 (0,4)\n",
      "              for ci.2 (0,2)\n",
      "                for co_c.4 (0,2)\n",
      "                  bgemm.local = ...\n",
      "      for p.3 (0,7)\n",
      "        for co.3 (0,8)\n",
      "          bgemm = ...\n",
      "inverse auto_unroll: 512\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,392)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,4)\n",
      "      unroll vw (0,4)\n",
      "        unroll r_a (0,6)\n",
      "          unroll r_b (0,6)\n",
      "            inverse = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,3136)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,64)\n",
      "    T_relu = ...\n",
      "\n",
      "Time elapsed for measurement: 3.12 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:34:39] /home/boyuan/.anaconda3/envs/tvm-build/conda-bld/tvm-cu102-package_1636400145961/work/src/auto_scheduler/measure.cc:299: Warning: Too many errors happened during tuning. Switching to debug mode.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for training: 0.51 s\n"
     ]
    }
   ],
   "source": [
    "def run_tuning():\n",
    "    print(\"Begin tuning...\")\n",
    "    measure_ctx = auto_scheduler.LocalRPCMeasureContext(repeat=1, min_repeat_ms=300, timeout=10)\n",
    "\n",
    "    tuner = auto_scheduler.TaskScheduler(tasks, task_weights)\n",
    "    tune_option = auto_scheduler.TuningOptions(\n",
    "        num_measure_trials=200, # change this to 20000 to achieve the best performance\n",
    "        runner=measure_ctx.runner,\n",
    "        measure_callbacks=[auto_scheduler.RecordToFile(log_file)],\n",
    "    )\n",
    "\n",
    "    tuner.tune(tune_option)\n",
    "\n",
    "run_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compile...\n"
     ]
    },
    {
     "ename": "TVMError",
     "evalue": "Traceback (most recent call last):\n  12: TVMFuncCall\n  11: _ZNSt17_Function_handlerIFvN\n  10: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const\n  9: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::NDArray, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, tvm::runtime::NDArray> > > const&, tvm::runtime::String)\n  8: tvm::build(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)\n  7: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)\n  6: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)\n  5: tvm::transform::Pass::operator()(tvm::IRModule) const\n  4: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  3: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  2: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  1: tvm::transform::ModulePassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  0: std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::VerifyMemory()::{lambda(tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::VerifyMemory()::{lambda(tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)\n  Did you forget to bind?\n    Variable `placeholder` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n    Variable `T_add` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n    Variable `placeholder` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n    Variable `placeholder` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n  File \"/home/boyuan/.anaconda3/envs/tvm-build/conda-bld/tvm-cu102-package_1636400145961/work/src/tir/analysis/verify_memory.cc\", line 206\nRuntimeError: Memory verification failed with the following errors:\nPrimFunc([placeholder, placeholder, placeholder, T_add]) attrs={\"target\": cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, \"tir.noalias\": (bool)1, \"global_symbol\": \"tvmgen_default_fused_nn_contrib_conv2d_winograd_without_weight_transform_add_1\", \"from_legacy_te_schedule\": (bool)1} {\n  allocate data_pad[float32 * 200704], storage_scope = global\n  allocate input_tile[float32 * 200704], storage_scope = global\n  allocate B[float32 * 16], storage_scope = global\n  for (i1, 0, 16) {\n    for (i2, 0, 16) {\n      for (i3, 0, 256) {\n        data_pad[(((i1*4096) + (i2*256)) + i3)] = tir.if_then_else(((((1 <= i1) && (i1 < 15)) && (1 <= i2)) && (i2 < 15)), placeholder[((((i1*3584) + (i2*256)) + i3) - 3840)], 0f)\n      }\n    }\n  }\n  for (eps, 0, 4) {\n    for (nu, 0, 4) {\n      for (p, 0, 49) {\n        for (ci, 0, 256) {\n          input_tile[((((eps*50176) + (nu*12544)) + (p*256)) + ci)] = data_pad[(((((floordiv(p, 7)*8192) + (eps*4096)) + (floormod(p, 7)*512)) + (nu*256)) + ci)]\n        }\n      }\n    }\n  }\n  for (i, 0, 4) {\n    for (j, 0, 4) {\n      B[((i*4) + j)] = select(((i == 3) && (j == 3)), 1f, select(((i == 3) && (j == 2)), 0f, select(((i == 3) && (j == 1)), 0f, select(((i == 3) && (j == 0)), 0f, select(((i == 2) && (j == 3)), 0f, select(((i == 2) && (j == 2)), 1f, select(((i == 2) && (j == 1)), 1f, select(((i == 2) && (j == 0)), -1f, select(((i == 1) && (j == 3)), -1f, select(((i == 1) && (j == 2)), 1f, select(((i == 1) && (j == 1)), -1f, select(((i == 1) && (j == 0)), 0f, select(((i == 0) && (j == 3)), 0f, select(((i == 0) && (j == 2)), 0f, select(((i == 0) && (j == 1)), 0f, select(((i == 0) && (j == 0)), 1f, 0f))))))))))))))))\n    }\n  }\n  for (eps, 0, 4) {\n    for (nu, 0, 4) {\n      for (p, 0, 49) {\n        for (ci, 0, 256) {\n          data_pad[((((eps*50176) + (nu*12544)) + (p*256)) + ci)] = 0f\n          for (r_a, 0, 4) {\n            for (r_b, 0, 4) {\n              data_pad[((((eps*50176) + (nu*12544)) + (p*256)) + ci)] = (data_pad[((((eps*50176) + (nu*12544)) + (p*256)) + ci)] + ((input_tile[((((r_a*50176) + (r_b*12544)) + (p*256)) + ci)]*B[((r_a*4) + eps)])*B[((r_b*4) + nu)]))\n            }\n          }\n        }\n      }\n    }\n  }\n  for (eps, 0, 4) {\n    for (nu, 0, 4) {\n      for (p, 0, 49) {\n        for (co, 0, 256) {\n          input_tile[((((eps*50176) + (nu*12544)) + (p*256)) + co)] = 0f\n          for (ci, 0, 256) {\n            input_tile[((((eps*50176) + (nu*12544)) + (p*256)) + co)] = (input_tile[((((eps*50176) + (nu*12544)) + (p*256)) + co)] + (data_pad[((((eps*50176) + (nu*12544)) + (p*256)) + ci)]*placeholder[((((eps*262144) + (nu*65536)) + (co*256)) + ci)]))\n          }\n        }\n      }\n    }\n  }\n  for (i, 0, 4) {\n    for (j, 0, 2) {\n      B[((i*2) + j)] = select(((i == 3) && (j == 1)), 1f, select(((i == 3) && (j == 0)), 0f, select(((i == 2) && (j == 1)), 1f, select(((i == 2) && (j == 0)), 1f, select(((i == 1) && (j == 1)), -1f, select(((i == 1) && (j == 0)), 1f, select(((i == 0) && (j == 1)), 0f, select(((i == 0) && (j == 0)), 1f, 0f))))))))\n    }\n  }\n  for (vh, 0, 2) {\n    for (vw, 0, 2) {\n      for (p, 0, 49) {\n        for (co, 0, 256) {\n          data_pad[((((vh*25088) + (vw*12544)) + (p*256)) + co)] = 0f\n          for (r_a, 0, 4) {\n            for (r_b, 0, 4) {\n              data_pad[((((vh*25088) + (vw*12544)) + (p*256)) + co)] = (data_pad[((((vh*25088) + (vw*12544)) + (p*256)) + co)] + ((input_tile[((((r_a*50176) + (r_b*12544)) + (p*256)) + co)]*B[((r_a*2) + vh)])*B[((r_b*2) + vw)]))\n            }\n          }\n        }\n      }\n    }\n  }\n  for (h, 0, 14) {\n    for (w, 0, 14) {\n      for (co, 0, 256) {\n        input_tile[(((h*3584) + (w*256)) + co)] = data_pad[(((((floormod(h, 2)*25088) + (floormod(w, 2)*12544)) + (floordiv(h, 2)*1792)) + (floordiv(w, 2)*256)) + co)]\n      }\n    }\n  }\n  for (ax1, 0, 14) {\n    for (ax2, 0, 14) {\n      for (ax3, 0, 256) {\n        T_add[(((ax1*3584) + (ax2*256)) + ax3)] = (input_tile[(((ax1*3584) + (ax2*256)) + ax3)] + placeholder[(((ax1*3584) + (ax2*256)) + ax3)])\n      }\n    }\n  }\n}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_770641/3738357272.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mauto_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApplyHistoryBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPassContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"relay.backend.use_auto_scheduler\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mlib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Create graph executor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/relay/build_module.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(ir_mod, target, target_host, params, mod_name)\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtophub_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mbld_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBuildModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         executor_config, runtime_mod, params = bld_mod.build(\n\u001b[0m\u001b[1;32m    358\u001b[0m             \u001b[0mmod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mir_mod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         )\n",
      "\u001b[0;32m~/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/relay/build_module.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, mod, target, target_host, params, executor, mod_name)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mmod_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmangle_module_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mautotvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGLOBAL_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_autotvm_silent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtvm/_ffi/_cython/./packed_func.pxi\u001b[0m in \u001b[0;36mtvm._ffi._cy3.core.PackedFuncBase.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mtvm/_ffi/_cython/./packed_func.pxi\u001b[0m in \u001b[0;36mtvm._ffi._cy3.core.FuncCall\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mtvm/_ffi/_cython/./base.pxi\u001b[0m in \u001b[0;36mtvm._ffi._cy3.core.CALL\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  12: TVMFuncCall\n  11: _ZNSt17_Function_handlerIFvN\n  10: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const\n  9: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::NDArray, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, tvm::runtime::NDArray> > > const&, tvm::runtime::String)\n  8: tvm::build(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)\n  7: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)\n  6: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)\n  5: tvm::transform::Pass::operator()(tvm::IRModule) const\n  4: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  3: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  2: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  1: tvm::transform::ModulePassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  0: std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::VerifyMemory()::{lambda(tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::VerifyMemory()::{lambda(tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)\n  Did you forget to bind?\n    Variable `placeholder` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n    Variable `T_add` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n    Variable `placeholder` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n    Variable `placeholder` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n  File \"/home/boyuan/.anaconda3/envs/tvm-build/conda-bld/tvm-cu102-package_1636400145961/work/src/tir/analysis/verify_memory.cc\", line 206\nRuntimeError: Memory verification failed with the following errors:\nPrimFunc([placeholder, placeholder, placeholder, T_add]) attrs={\"target\": cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, \"tir.noalias\": (bool)1, \"global_symbol\": \"tvmgen_default_fused_nn_contrib_conv2d_winograd_without_weight_transform_add_1\", \"from_legacy_te_schedule\": (bool)1} {\n  allocate data_pad[float32 * 200704], storage_scope = global\n  allocate input_tile[float32 * 200704], storage_scope = global\n  allocate B[float32 * 16], storage_scope = global\n  for (i1, 0, 16) {\n    for (i2, 0, 16) {\n      for (i3, 0, 256) {\n        data_pad[(((i1*4096) + (i2*256)) + i3)] = tir.if_then_else(((((1 <= i1) && (i1 < 15)) && (1 <= i2)) && (i2 < 15)), placeholder[((((i1*3584) + (i2*256)) + i3) - 3840)], 0f)\n      }\n    }\n  }\n  for (eps, 0, 4) {\n    for (nu, 0, 4) {\n      for (p, 0, 49) {\n        for (ci, 0, 256) {\n          input_tile[((((eps*50176) + (nu*12544)) + (p*256)) + ci)] = data_pad[(((((floordiv(p, 7)*8192) + (eps*4096)) + (floormod(p, 7)*512)) + (nu*256)) + ci)]\n        }\n      }\n    }\n  }\n  for (i, 0, 4) {\n    for (j, 0, 4) {\n      B[((i*4) + j)] = select(((i == 3) && (j == 3)), 1f, select(((i == 3) && (j == 2)), 0f, select(((i == 3) && (j == 1)), 0f, select(((i == 3) && (j == 0)), 0f, select(((i == 2) && (j == 3)), 0f, select(((i == 2) && (j == 2)), 1f, select(((i == 2) && (j == 1)), 1f, select(((i == 2) && (j == 0)), -1f, select(((i == 1) && (j == 3)), -1f, select(((i == 1) && (j == 2)), 1f, select(((i == 1) && (j == 1)), -1f, select(((i == 1) && (j == 0)), 0f, select(((i == 0) && (j == 3)), 0f, select(((i == 0) && (j == 2)), 0f, select(((i == 0) && (j == 1)), 0f, select(((i == 0) && (j == 0)), 1f, 0f))))))))))))))))\n    }\n  }\n  for (eps, 0, 4) {\n    for (nu, 0, 4) {\n      for (p, 0, 49) {\n        for (ci, 0, 256) {\n          data_pad[((((eps*50176) + (nu*12544)) + (p*256)) + ci)] = 0f\n          for (r_a, 0, 4) {\n            for (r_b, 0, 4) {\n              data_pad[((((eps*50176) + (nu*12544)) + (p*256)) + ci)] = (data_pad[((((eps*50176) + (nu*12544)) + (p*256)) + ci)] + ((input_tile[((((r_a*50176) + (r_b*12544)) + (p*256)) + ci)]*B[((r_a*4) + eps)])*B[((r_b*4) + nu)]))\n            }\n          }\n        }\n      }\n    }\n  }\n  for (eps, 0, 4) {\n    for (nu, 0, 4) {\n      for (p, 0, 49) {\n        for (co, 0, 256) {\n          input_tile[((((eps*50176) + (nu*12544)) + (p*256)) + co)] = 0f\n          for (ci, 0, 256) {\n            input_tile[((((eps*50176) + (nu*12544)) + (p*256)) + co)] = (input_tile[((((eps*50176) + (nu*12544)) + (p*256)) + co)] + (data_pad[((((eps*50176) + (nu*12544)) + (p*256)) + ci)]*placeholder[((((eps*262144) + (nu*65536)) + (co*256)) + ci)]))\n          }\n        }\n      }\n    }\n  }\n  for (i, 0, 4) {\n    for (j, 0, 2) {\n      B[((i*2) + j)] = select(((i == 3) && (j == 1)), 1f, select(((i == 3) && (j == 0)), 0f, select(((i == 2) && (j == 1)), 1f, select(((i == 2) && (j == 0)), 1f, select(((i == 1) && (j == 1)), -1f, select(((i == 1) && (j == 0)), 1f, select(((i == 0) && (j == 1)), 0f, select(((i == 0) && (j == 0)), 1f, 0f))))))))\n    }\n  }\n  for (vh, 0, 2) {\n    for (vw, 0, 2) {\n      for (p, 0, 49) {\n        for (co, 0, 256) {\n          data_pad[((((vh*25088) + (vw*12544)) + (p*256)) + co)] = 0f\n          for (r_a, 0, 4) {\n            for (r_b, 0, 4) {\n              data_pad[((((vh*25088) + (vw*12544)) + (p*256)) + co)] = (data_pad[((((vh*25088) + (vw*12544)) + (p*256)) + co)] + ((input_tile[((((r_a*50176) + (r_b*12544)) + (p*256)) + co)]*B[((r_a*2) + vh)])*B[((r_b*2) + vw)]))\n            }\n          }\n        }\n      }\n    }\n  }\n  for (h, 0, 14) {\n    for (w, 0, 14) {\n      for (co, 0, 256) {\n        input_tile[(((h*3584) + (w*256)) + co)] = data_pad[(((((floormod(h, 2)*25088) + (floormod(w, 2)*12544)) + (floordiv(h, 2)*1792)) + (floordiv(w, 2)*256)) + co)]\n      }\n    }\n  }\n  for (ax1, 0, 14) {\n    for (ax2, 0, 14) {\n      for (ax3, 0, 256) {\n        T_add[(((ax1*3584) + (ax2*256)) + ax3)] = (input_tile[(((ax1*3584) + (ax2*256)) + ax3)] + placeholder[(((ax1*3584) + (ax2*256)) + ax3)])\n      }\n    }\n  }\n}\n"
     ]
    }
   ],
   "source": [
    "# Official tutorial cannot run.\n",
    "\n",
    "# Compile with the history best\n",
    "print(\"Compile...\")\n",
    "with auto_scheduler.ApplyHistoryBest(log_file):\n",
    "    with tvm.transform.PassContext(opt_level=3, config={\"relay.backend.use_auto_scheduler\": True}):\n",
    "        lib = relay.build(mod, target=target, params=params)\n",
    "\n",
    "# Create graph executor\n",
    "dev = tvm.device(str(target), 0)\n",
    "module = graph_executor.GraphModule(lib[\"default\"](dev))\n",
    "data_tvm = tvm.nd.array((np.random.uniform(size=input_shape)).astype(dtype))\n",
    "module.set_input(\"data\", data_tvm)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Evaluate inference time cost...\")\n",
    "print(module.benchmark(dev, repeat=3, min_repeat_ms=500))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b761d0240ca762903f954933761353d8903f283fea0d68128d1f9b50c03f2f84"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('tvm-build': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
