{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /home/boyuan/.cache/torch_extensions as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/boyuan/.cache/torch_extensions/c_relu_verification/build.ninja...\n",
      "Building extension module c_relu_verification...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module c_relu_verification...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math, time\n",
    "import sys\n",
    "import tvm\n",
    "from tvm import relay\n",
    "from tvm.contrib import graph_executor\n",
    "from tvm import auto_scheduler\n",
    "import numpy as np\n",
    "from tvm.contrib import graph_executor\n",
    "\n",
    "sys.path.append('/home/boyuan/verification_tianqi/')\n",
    "\n",
    "epsilon = 1e-12\n",
    "\n",
    "from HandTunedKernels.kernel_test.cnn_forward_test_bound import Bounds\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='1' # 0 for A6000 on winnie, 1 for P6000 on winnie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bounds2Tuple(x):\n",
    "    return tuple((torch.Tensor([[x.p]]), torch.Tensor([[x.eps]]), x.lw, x.lb, x.uw, x.ub))\n",
    "def Tuple2Bounds(x):\n",
    "    return Bounds(x[0], x[1], x[2], x[3], x[4], x[5])\n",
    "# def Elements2BoundsDotProduct(x1, x2, x3, x4):\n",
    "#     return BoundsDotProduct(args, p=2, eps=0.1, w=None, b=None, lw=x1, lb=x2, uw=x3, ub=x4)\n",
    "\n",
    "class BoundsReLUWrapper(nn.Module):\n",
    "    def __init__(self): #, ):\n",
    "        super(BoundsReLUWrapper, self).__init__()\n",
    "        \n",
    "    def forward(self, p, eps, lw, lb, uw, ub):\n",
    "        x = Bounds(float(p), float(eps), lw, lb, uw, ub).relu()\n",
    "        return Bounds2Tuple(x)\n",
    "\n",
    "class BoundsMatMulWrapper(nn.Module):\n",
    "    def __init__(self, W): \n",
    "        super(BoundsMatMulWrapper, self).__init__()\n",
    "        self.W = W\n",
    "\n",
    "    def forward(self, p, eps, lw, lb, uw, ub):\n",
    "        x = Bounds(float(p), float(eps), lw, lb, uw, ub).matmul(self.W)\n",
    "        return Bounds2Tuple(x) \n",
    "\n",
    "class BoundsDotProductWrapper(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BoundsDotProductWrapper, self).__init__()\n",
    "\n",
    "    def forward(self, p0, eps0, lw0, lb0, uw0, ub0, p1, eps1, lw1, lb1, uw1, ub1):\n",
    "        x = Bounds(float(p0), float(eps0), lw0, lb0, uw0, ub0)\n",
    "        y = Bounds(float(p1), float(eps1), lw1, lb1, uw1, ub1)\n",
    "        return Bounds2Tuple(x.dot_product(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1452505/3113498695.py:13: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  x = Bounds(float(p), float(eps), lw, lb, uw, ub).relu()\n",
      "/home/boyuan/.local/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "/tmp/ipykernel_1452505/3113498695.py:2: TracerWarning: torch.Tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return tuple((torch.Tensor([[x.p]]), torch.Tensor([[x.eps]]), x.lw, x.lb, x.uw, x.ub))\n",
      "/tmp/ipykernel_1452505/3113498695.py:22: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  x = Bounds(float(p), float(eps), lw, lb, uw, ub).matmul(self.W)\n",
      "/tmp/ipykernel_1452505/3113498695.py:30: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  x = Bounds(float(p0), float(eps0), lw0, lb0, uw0, ub0)\n",
      "/tmp/ipykernel_1452505/3113498695.py:31: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  y = Bounds(float(p1), float(eps1), lw1, lb1, uw1, ub1)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "p = 2\n",
    "eps = 0.5\n",
    "batch_size, length, dim_in, dim_out, dim_y_out = 8, 4, 64, 32, 32\n",
    "lb = torch.rand(batch_size,length,dim_out).to(device)\n",
    "ub = lb + torch.rand(batch_size,length,dim_out).to(device)\n",
    "lw = torch.rand(batch_size,length,dim_in,dim_out).to(device) - 0.5\n",
    "uw = torch.rand(batch_size,length,dim_in,dim_out).to(device) - 0.5\n",
    "W = torch.rand(dim_y_out, dim_out).to(device) - 0.5\n",
    "bound = Bounds(p=2,eps=0.5,lw=lw,lb=lb,uw=uw,ub=ub)\n",
    "bound1 = Bounds(p=2,eps=0.5,lw=lw,lb=lb,uw=uw,ub=ub)\n",
    "\n",
    "bound_relu_wrapper = BoundsReLUWrapper()\n",
    "bound_matmul_wrapper = BoundsMatMulWrapper(W)\n",
    "bound_dot_product_wrapper = BoundsDotProductWrapper()\n",
    "\n",
    "example_relu_inputs = Bounds2Tuple(bound)\n",
    "example_matmul_inputs = Bounds2Tuple(bound)#(p, eps, lw, lb, uw, ub)\n",
    "example_dot_product_inputs = (*Bounds2Tuple(bound), *Bounds2Tuple(bound1))\n",
    "\n",
    "\n",
    "# test1.forward(p, eps, lw, lb, uw, ub)\n",
    "# test2.forward(p, eps, lw, lb, uw, ub)\n",
    "# test3.forward(p, eps, lw, lb, uw, ub, *Bounds2Tuple(bound1))\n",
    "\n",
    "scripted_relu_model = torch.jit.trace(bound_relu_wrapper.eval(), example_relu_inputs).eval()\n",
    "# scripted_relu_model = torch.jit.script(bound_relu_wrapper)\n",
    "scripted_matmul_model = torch.jit.trace(bound_matmul_wrapper.eval(), example_matmul_inputs).eval()\n",
    "scripted_dot_product_model = torch.jit.trace(bound_dot_product_wrapper.eval(), example_dot_product_inputs).eval()\n",
    "# scripted_dot_product_model = torch.jit.script(bound_dot_product_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_bound_wrapper_diff(bound, output_tuple):\n",
    "    return torch.norm((bound.lw-output_tuple[2]).flatten())\n",
    "\n",
    "\n",
    "def check_tvm_ansor_baseline_diff(bound, output_tuple):\n",
    "    return torch.norm((bound.lw- (torch.from_numpy(output_tuple[2])).to(device)).flatten())\n",
    "\n",
    "def profile_pytorch(p, eps, lw, lb, uw, ub, w_input, other_bound, op_type='relu', num_profile=100):        \n",
    "    bound = Bounds(p, eps, lw, lb, uw, ub)\n",
    "\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    start.record()\n",
    "    for i in range(num_profile):\n",
    "        if op_type == 'relu':\n",
    "            bound_output = bound.relu()\n",
    "        elif op_type == 'matmul':\n",
    "            bound_output = bound.matmul(w_input)\n",
    "        elif op_type == 'dot_product':\n",
    "            bound_output = bound.dot_product(other_bound)\n",
    "\n",
    "    end.record()\n",
    "\n",
    "    # Waits for everything to finish running\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    average_time = start.elapsed_time(end)/num_profile # Unit: Millisecond\n",
    "    print(\"\\n\\n\\n pytorch. op_type: {}, batch_size: {}, length: {}, dim_in: {}, dim_out: {}, average_time (ms): {}\\n\\n\\n\".format(op_type, batch_size, length, dim_in, dim_out, average_time))\n",
    "    return bound_output\n",
    "    \n",
    "def profile_nn_wrapper(p, eps, lw, lb, uw, ub, w_input, other_bound, op_type='relu', num_profile=1000):    \n",
    "    if op_type == 'relu':\n",
    "        bound_nn_wrapper = BoundsReLUWrapper()\n",
    "    elif op_type == 'matmul':\n",
    "        bound_nn_wrapper = BoundsMatMulWrapper(w_input)\n",
    "    elif op_type == 'dot_product':\n",
    "        bound_nn_wrapper = BoundsDotProductWrapper()\n",
    "        \n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    if op_type == 'relu':\n",
    "        start.record()\n",
    "        for i in range(num_profile):\n",
    "            bound_output = bound_nn_wrapper.forward(p, eps, lw, lb, uw, ub)\n",
    "        end.record()\n",
    "    elif op_type == 'matmul':\n",
    "        start.record()\n",
    "        for i in range(num_profile):\n",
    "            bound_output = bound_nn_wrapper.forward(p, eps, lw, lb, uw, ub)\n",
    "        end.record()\n",
    "    elif op_type == 'dot_product':\n",
    "        start.record()\n",
    "        for i in range(num_profile):\n",
    "            bound_output = bound_nn_wrapper.forward(p, eps, lw, lb, uw, ub, *Bounds2Tuple(other_bound))\n",
    "        end.record()\n",
    "\n",
    "    # Waits for everything to finish running\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    average_time = start.elapsed_time(end)/num_profile # Unit: Millisecond\n",
    "    print(\"\\n\\n\\n nn wrapper. op_type: {}, batch_size: {}, length: {}, dim_in: {}, dim_out: {}, average_time (ms): {}\\n\\n\\n\".format(op_type, batch_size, length, dim_in, dim_out, average_time))\n",
    "    return bound_output\n",
    "\n",
    "def profile_tvm_baseline(p, eps, lw, lb, uw, ub, w_input, other_bound, op_type='relu', num_profile=1000):    \n",
    "    if op_type == 'relu':\n",
    "        bound_wrapper = BoundsReLUWrapper()\n",
    "        example_inputs = Bounds2Tuple(Bounds(p, eps, lw, lb, uw, ub))\n",
    "        scripted_model = torch.jit.trace(bound_wrapper.eval(), example_inputs).eval()\n",
    "    elif op_type == 'matmul':\n",
    "        bound_wrapper = BoundsMatMulWrapper(w_input)\n",
    "        example_inputs = Bounds2Tuple(Bounds(p, eps, lw, lb, uw, ub))\n",
    "        scripted_model = torch.jit.trace(bound_wrapper.eval(), example_inputs).eval()\n",
    "    elif op_type == 'dot_product':\n",
    "        bound_wrapper = BoundsDotProductWrapper()\n",
    "        example_inputs = (*Bounds2Tuple(Bounds(p, eps, lw, lb, uw, ub)), *Bounds2Tuple(other_bound))\n",
    "        scripted_model = torch.jit.trace(bound_wrapper.eval(), example_inputs).eval()    \n",
    "        \n",
    "    input_name = \"input%d\"\n",
    "    shape_list = []\n",
    "    if type(example_inputs) == tuple:\n",
    "        for i in range(len(example_inputs)):\n",
    "            shape_list.append((input_name%(i), example_inputs[i].shape))\n",
    "    else:\n",
    "        shape_list.append((input_name%(0), example_inputs.shape))\n",
    "    \n",
    "    mod, params = relay.frontend.from_pytorch(scripted_model, shape_list)\n",
    "\n",
    "    # target = tvm.target.Target(\"llvm\", host=\"llvm\")\n",
    "    target = tvm.target.Target('cuda')\n",
    "    dev = tvm.cuda(0)\n",
    "    with tvm.transform.PassContext(opt_level=3):\n",
    "        lib = relay.build(mod, target=target, params=params)\n",
    "        \n",
    "        \n",
    "    dtype = \"float32\"\n",
    "    m = graph_executor.GraphModule(lib[\"default\"](dev))\n",
    "    # Set inputs\n",
    "    if type(example_inputs) == tuple:\n",
    "        for i in range(2, len(example_inputs)):\n",
    "            m.set_input(input_name%(i), tvm.nd.array(example_inputs[i].cpu().numpy().astype(dtype)))\n",
    "    else:\n",
    "        m.set_input(input_name%(0), tvm.nd.array(example_inputs.numpy().cpu().astype(dtype)))\n",
    "    # Execute\n",
    "\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    start.record()\n",
    "    for i in range(num_profile):\n",
    "        m.run()\n",
    "        # Get outputs\n",
    "\n",
    "    end.record()\n",
    "    \n",
    "    # Waits for everything to finish running\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    average_time = start.elapsed_time(end)/num_profile # Unit: Millisecond\n",
    "    tvm_output = m.get_output(0)\n",
    "    print(\"\\n\\n\\ntvm. op_type: {}, batch_size: {}, length: {}, dim_in: {}, dim_out: {}, average_time (ms): {}\\n\\n\\n\".format(op_type, batch_size, length, dim_in, dim_out, average_time))\n",
    "\n",
    "    return (m.get_output(0).numpy(), m.get_output(1).numpy(), m.get_output(2).numpy(), m.get_output(3).numpy())\n",
    "    \n",
    "def profile_ansor_baseline(p, eps, lw, lb, uw, ub, w_input, other_bound, op_type=\"relu\", num_profile=1000):\n",
    "    if op_type == 'relu':\n",
    "        bound_wrapper = BoundsReLUWrapper()\n",
    "        example_inputs = Bounds2Tuple(Bounds(p, eps, lw, lb, uw, ub))\n",
    "        scripted_model = torch.jit.trace(bound_wrapper.eval(), example_inputs).eval()\n",
    "    elif op_type == 'matmul':\n",
    "        bound_wrapper = BoundsMatMulWrapper(w_input)\n",
    "        example_inputs = Bounds2Tuple(Bounds(p, eps, lw, lb, uw, ub))\n",
    "        scripted_model = torch.jit.trace(bound_wrapper.eval(), example_inputs).eval()\n",
    "    elif op_type == 'dot_product':\n",
    "        bound_wrapper = BoundsDotProductWrapper()\n",
    "        example_inputs = (*Bounds2Tuple(Bounds(p, eps, lw, lb, uw, ub)), *Bounds2Tuple(other_bound))\n",
    "        scripted_model = torch.jit.trace(bound_wrapper.eval(), example_inputs).eval()    \n",
    "        \n",
    "    input_name = \"input%d\"\n",
    "    shape_list = []\n",
    "    if type(example_inputs) == tuple:\n",
    "        for i in range(len(example_inputs)):\n",
    "            shape_list.append((input_name%(i), example_inputs[i].shape))\n",
    "    else:\n",
    "        shape_list.append((input_name%(0), example_inputs.shape))\n",
    "    \n",
    "    mod, params = relay.frontend.from_pytorch(scripted_model, shape_list)\n",
    "\n",
    "    # target = tvm.target.Target(\"llvm\", host=\"llvm\")\n",
    "    target = tvm.target.Target('cuda')\n",
    "    dev = tvm.device(str(target), 0)\n",
    "    log_file = \"ansor_autotuning_json/ansor_\"+op_type+\".json\"\n",
    "\n",
    "    tasks, task_weights = auto_scheduler.extract_tasks(mod[\"main\"], params, target, include_simple_tasks=False)\n",
    "    # Enumerate the tasks\n",
    "    # for idx, task in enumerate(tasks):\n",
    "    #     print(\"========== Task %d  (workload key: %s) ==========\" % (idx, task.workload_key))\n",
    "    #     print(task.compute_dag)\n",
    "\n",
    "    # measure_ctx launches a different process for measurement to provide isolation\n",
    "    # It protect the master process from GPU crashes\n",
    "    measure_ctx = auto_scheduler.LocalRPCMeasureContext(repeat=1, min_repeat_ms=300, timeout=10)\n",
    "\n",
    "    tuner = auto_scheduler.TaskScheduler(tasks, task_weights)\n",
    "    tune_option = auto_scheduler.TuningOptions(\n",
    "        num_measure_trials=800 * len(tasks),  # change this to 800 & #task to achieve the best performance\n",
    "        runner=measure_ctx.runner,\n",
    "        measure_callbacks=[auto_scheduler.RecordToFile(log_file)],\n",
    "    )\n",
    "\n",
    "    tuner.tune(tune_option)\n",
    "\n",
    "    # Compile with the history best\n",
    "    print(\"Compile ...\")\n",
    "    with auto_scheduler.ApplyHistoryBest(log_file):\n",
    "        with tvm.transform.PassContext(opt_level=3, config={\"relay.backend.use_auto_scheduler\": True}):\n",
    "            lib_ansor = relay.build(mod, target=target, params=params)\n",
    "\n",
    "    # Create graph executer\n",
    "    dtype = \"float32\"\n",
    "    module_ansor = graph_executor.GraphModule(lib_ansor[\"default\"](dev))\n",
    "    if type(example_inputs) == tuple:\n",
    "        for i in range(2, len(example_inputs)):\n",
    "            module_ansor.set_input(input_name%(i), tvm.nd.array(example_inputs[i].cpu().numpy().astype(dtype)))\n",
    "    else:\n",
    "        module_ansor.set_input(input_name%(0), tvm.nd.array(example_inputs.numpy().cpu().astype(dtype)))\n",
    "\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    start.record()\n",
    "    for i in range(num_profile):\n",
    "        module_ansor.run()\n",
    "        # Get outputs\n",
    "\n",
    "    end.record()\n",
    "    \n",
    "    # Waits for everything to finish running\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    average_time = start.elapsed_time(end)/num_profile # Unit: Millisecond\n",
    "    print(\"\\n\\n\\n ansor\\n\\n\\n\")\n",
    "    print(\"\\n\\n\\nansor. op_type: {}, batch_size: {}, length: {}, dim_in: {}, dim_out: {}, average_time (ms): {}\\n\\n\\n\".format(op_type, batch_size, length, dim_in, dim_out, average_time))\n",
    "\n",
    "    return (module_ansor.get_output(0).numpy(), module_ansor.get_output(1).numpy(), module_ansor.get_output(2).numpy(), module_ansor.get_output(3).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " pytorch. op_type: relu, batch_size: 1, length: 2, dim_in: 64, dim_out: 64, average_time (ms): 8.716328735351562\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " nn wrapper. op_type: relu, batch_size: 1, length: 2, dim_in: 64, dim_out: 64, average_time (ms): 1.7544358825683595\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1452505/3113498695.py:13: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  x = Bounds(float(p), float(eps), lw, lb, uw, ub).relu()\n",
      "/tmp/ipykernel_1452505/3113498695.py:2: TracerWarning: torch.Tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return tuple((torch.Tensor([[x.p]]), torch.Tensor([[x.eps]]), x.lw, x.lb, x.uw, x.ub))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "tvm. op_type: relu, batch_size: 1, length: 2, dim_in: 64, dim_out: 64, average_time (ms): 1.2006195068359375\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1452505/3113498695.py:13: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  x = Bounds(float(p), float(eps), lw, lb, uw, ub).relu()\n",
      "/tmp/ipykernel_1452505/3113498695.py:2: TracerWarning: torch.Tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return tuple((torch.Tensor([[x.p]]), torch.Tensor([[x.eps]]), x.lw, x.lb, x.uw, x.ub))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get devices for measurement successfully!\n",
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |----------------------------------------------------------------------\n",
      "------------------------------  [ \n",
      "-------------------------------------------------\n",
      "|    0 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 0\tUsed time : 0 s\tNext ID: 0\t\n",
      "Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 2\n",
      "Sample Iter: 5\t#Pop: 10\t#Target: 50\tfail_ct: 10230\tTime elapsed: 3.76\n",
      "#Target has been reduced to 25 due to too many failures or duplications\n",
      "Sample Iter: 10\t#Pop: 10\t#Target: 25\tfail_ct: 20470\tTime elapsed: 7.78\n",
      "#Target has been reduced to 12 due to too many failures or duplications\n",
      "Sample Iter: 15\t#Pop: 10\t#Target: 12\tfail_ct: 30710\tTime elapsed: 10.63\n",
      "#Target has been reduced to 6 due to too many failures or duplications\n",
      "Sample Initial Population\t#s: 10\tfail_ct: 32758\tTime elapsed: 11.22\n",
      "GA Iter: 0\tMax score: 0.9144\tMin score: 0.0733\t#Pop: 10\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 0.9796\tMin score: 0.0019\t#Pop: 70\t#M+: 1388\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 70\tTime elapsed: 1.88\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 64 programs to measure:\n",
      "................................********************************\n",
      "................................********************************\n",
      "Time elapsed for measurement: 74.01 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n",
      "Time elapsed for training: |  ID  | Latency (ms) | Speed (GFLOPS) | Trials |0.17 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "-------------------------------------------------\n",
      "|    0 |        0.002 |           3.42 |     64 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: 0.005 ms\tTrials: 64\tUsed time : 87 s\tNext ID: 0\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Sample Initial Population\t#s: 10\tfail_ct: 2038\tTime elapsed: 0.56\n",
      "GA Iter: 0\tMax score: N/A\tMin score: N/A\t#Pop: 0\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 0.8775\tMin score: 0.0753\t#Pop: 6\t#M+: 1394\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 6\tTime elapsed: 1.96\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 6 programs to measure:\n",
      "......******\n",
      "Time elapsed for measurement: 7.91 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n",
      "Time elapsed for training: 0.13 s\n",
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |----------------------------------------------------------------------\n",
      "\n",
      "-------------------------------------------------\n",
      "|    0 |        0.002 |           3.42 |    128 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: 0.005 ms\tTrials: 70\tUsed time : 98 s\tNext ID: 0\t\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Sample Initial Population\t#s: 10\tfail_ct: 2038\tTime elapsed: 0.59\n",
      "GA Iter: 0\tMax score: N/A\tMin score: N/A\t#Pop: 0\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: N/A\tMin score: N/A\t#Pop: 0\t#M+: 1393\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 0\tTime elapsed: Compile ...1.84\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 0 programs to measure:\n",
      "Time elapsed for measurement: 0.00 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n",
      "Time elapsed for training: 0.00 s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ansor\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ansor. op_type: relu, batch_size: 1, length: 2, dim_in: 64, dim_out: 64, average_time (ms): 0.9884159851074219\n",
      "\n",
      "\n",
      "\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "--------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " pytorch. op_type: relu, batch_size: 1, length: 2, dim_in: 128, dim_out: 128, average_time (ms): 5.663447265625\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " nn wrapper. op_type: relu, batch_size: 1, length: 2, dim_in: 128, dim_out: 128, average_time (ms): 4.545361938476563\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1452505/3113498695.py:13: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  x = Bounds(float(p), float(eps), lw, lb, uw, ub).relu()\n",
      "/tmp/ipykernel_1452505/3113498695.py:2: TracerWarning: torch.Tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return tuple((torch.Tensor([[x.p]]), torch.Tensor([[x.eps]]), x.lw, x.lb, x.uw, x.ub))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "tvm. op_type: relu, batch_size: 1, length: 2, dim_in: 128, dim_out: 128, average_time (ms): 0.9847090911865234\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1452505/3113498695.py:13: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  x = Bounds(float(p), float(eps), lw, lb, uw, ub).relu()\n",
      "/tmp/ipykernel_1452505/3113498695.py:2: TracerWarning: torch.Tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return tuple((torch.Tensor([[x.p]]), torch.Tensor([[x.eps]]), x.lw, x.lb, x.uw, x.ub))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get devices for measurement successfully!\n",
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |----------------------------------------------------------------------\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "-------------------------------------------------\n",
      "|    0 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 0\tUsed time : 0 s\tNext ID: 0\t\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 2\n",
      "Sample Iter: 5\t#Pop: 10\t#Target: 50\tfail_ct: 10230\tTime elapsed: 3.80\n",
      "#Target has been reduced to 25 due to too many failures or duplications\n",
      "Sample Iter: 10\t#Pop: 10\t#Target: 25\tfail_ct: 20470\tTime elapsed: 7.93\n",
      "#Target has been reduced to 12 due to too many failures or duplications\n",
      "Sample Iter: 15\t#Pop: 10\t#Target: 12\tfail_ct: 30710\tTime elapsed: 11.06\n",
      "#Target has been reduced to 6 due to too many failures or duplications\n",
      "Sample Initial Population\t#s: 10\tfail_ct: 32758\tTime elapsed: 11.63\n",
      "GA Iter: 0\tMax score: 0.8915\tMin score: 0.0063\t#Pop: 10\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 0.9947\tMin score: 0.0063\t#Pop: 70\t#M+: 1392\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 70\tTime elapsed: 1.90\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 64 programs to measure:\n",
      "................................********************************\n",
      "................................********************************\n",
      "Time elapsed for measurement: 72.60 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n",
      "Time elapsed for training: 0.14 s\n",
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |----------------------------------------------------------------------\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "-------------------------------------------------\n",
      "|    0 |        0.003 |          10.09 |     64 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: 0.006 ms\tTrials: 64\tUsed time : 86 s\tNext ID: 0\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Sample Initial Population\t#s: 10\tfail_ct: 2038\tTime elapsed: 0.55\n",
      "GA Iter: 0\tMax score: N/A\tMin score: N/A\t#Pop: 0\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 0.9201\tMin score: 0.2613\t#Pop: 6\t#M+: 1386\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 6\tTime elapsed: 1.94\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 6 programs to measure:\n",
      "......******\n",
      "Time elapsed for measurement: 8.12 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n",
      "Time elapsed for training: 0.15 s\n",
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |----------------------------------------------------------------------\n",
      "------------------------------  [ \n",
      "-------------------------------------------------\n",
      "|    0 |        0.003 |          10.09 |    128 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: 0.006 ms\tTrials: 70\tUsed time : 97 s\tNext ID: 0\t\n",
      "Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Sample Initial Population\t#s: 10\tfail_ct: 2038\tTime elapsed: 0.60\n",
      "GA Iter: 0\tMax score: N/A\tMin score: N/A\t#Pop: 0\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: N/A\tMin score: N/A\t#Pop: 0\t#M+: 1401\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 0\tTime elapsed: 1.84\n",
      "Compile ...----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 0 programs to measure:\n",
      "Time elapsed for measurement: 0.00 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n",
      "Time elapsed for training: 0.00 s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ansor\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ansor. op_type: relu, batch_size: 1, length: 2, dim_in: 128, dim_out: 128, average_time (ms): 0.9986662292480468\n",
      "\n",
      "\n",
      "\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "--------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " pytorch. op_type: relu, batch_size: 1, length: 2, dim_in: 256, dim_out: 256, average_time (ms): 7.429385986328125\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " nn wrapper. op_type: relu, batch_size: 1, length: 2, dim_in: 256, dim_out: 256, average_time (ms): 2.5079356384277345\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1452505/3113498695.py:13: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  x = Bounds(float(p), float(eps), lw, lb, uw, ub).relu()\n",
      "/tmp/ipykernel_1452505/3113498695.py:2: TracerWarning: torch.Tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return tuple((torch.Tensor([[x.p]]), torch.Tensor([[x.eps]]), x.lw, x.lb, x.uw, x.ub))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "tvm. op_type: relu, batch_size: 1, length: 2, dim_in: 256, dim_out: 256, average_time (ms): 1.0053119659423828\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1452505/3113498695.py:13: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  x = Bounds(float(p), float(eps), lw, lb, uw, ub).relu()\n",
      "/tmp/ipykernel_1452505/3113498695.py:2: TracerWarning: torch.Tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return tuple((torch.Tensor([[x.p]]), torch.Tensor([[x.eps]]), x.lw, x.lb, x.uw, x.ub))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get devices for measurement successfully!\n",
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |----------------------------------------------------------------------\n",
      "-------------------------------------------------\n",
      "|    0 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 0\tUsed time : 0 s\tNext ID: 0\t\n",
      "\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 2\n",
      "Sample Iter: 5\t#Pop: 10\t#Target: 50\tfail_ct: 10230\tTime elapsed: 3.76\n",
      "#Target has been reduced to 25 due to too many failures or duplications\n",
      "Sample Iter: 10\t#Pop: 10\t#Target: 25\tfail_ct: 20470\tTime elapsed: 7.01\n",
      "#Target has been reduced to 12 due to too many failures or duplications\n",
      "Sample Iter: 15\t#Pop: 10\t#Target: 12\tfail_ct: 30710\tTime elapsed: 9.91\n",
      "#Target has been reduced to 6 due to too many failures or duplications\n",
      "Sample Initial Population\t#s: 10\tfail_ct: 32758\tTime elapsed: 10.48\n",
      "GA Iter: 0\tMax score: 0.8986\tMin score: 0.2220\t#Pop: 10\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 0.9369\tMin score: 0.0065\t#Pop: 70\t#M+: 1395\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 70\tTime elapsed: 2.45\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 64 programs to measure:\n",
      "................................********************************\n",
      "................................********************************\n",
      "Time elapsed for measurement: 73.82 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boyuan/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for training: 0.20 s\n",
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |\n",
      "-------------------------------------------------\n",
      "|    0 |        0.007 |          20.03 |     64 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: 0.013 ms\tTrials: 64\tUsed time : 87 s\tNext ID: 0\t\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Sample Initial Population\t#s: 10\tfail_ct: 2038\tTime elapsed: 0.60\n",
      "GA Iter: 0\tMax score: N/A\tMin score: N/A\t#Pop: 0\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 0.6342\tMin score: 0.1643\t#Pop: 6\t#M+: 1394\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 6\tTime elapsed: 2.01\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 6 programs to measure:\n",
      "......******\n",
      "Time elapsed for measurement: 7.56 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n",
      "Time elapsed for training: |  ID  | Latency (ms) | Speed (GFLOPS) | Trials |0.12 s\n",
      "\n",
      "-------------------------------------------------\n",
      "|    0 |        0.007 |          20.03 |    128 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: 0.013 ms\tTrials: 70\tUsed time : 97 s\tNext ID: 0\t\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Sample Initial Population\t#s: 10\tfail_ct: 2038\tTime elapsed: 0.58\n",
      "GA Iter: 0\tMax score: N/A\tMin score: N/A\t#Pop: 0\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: N/A\tMin score: N/A\t#Pop: 0\t#M+: 1392\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 0\tTime elapsed: 1.87\n",
      "Compile ...\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 0 programs to measure:\n",
      "Time elapsed for measurement: 0.00 s\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Train cost model ]\n",
      "----------------------------------------------------------------------\n",
      "Time elapsed for training: 0.00 s\n",
      "\n",
      "\n",
      "\n",
      " ansor\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ansor. op_type: relu, batch_size: 1, length: 2, dim_in: 256, dim_out: 256, average_time (ms): 1.0492006683349608\n",
      "\n",
      "\n",
      "\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "--------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " pytorch. op_type: relu, batch_size: 1, length: 2, dim_in: 512, dim_out: 512, average_time (ms): 5.445765380859375\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " nn wrapper. op_type: relu, batch_size: 1, length: 2, dim_in: 512, dim_out: 512, average_time (ms): 3.381791076660156\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1452505/3113498695.py:13: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  x = Bounds(float(p), float(eps), lw, lb, uw, ub).relu()\n",
      "/tmp/ipykernel_1452505/3113498695.py:2: TracerWarning: torch.Tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return tuple((torch.Tensor([[x.p]]), torch.Tensor([[x.eps]]), x.lw, x.lb, x.uw, x.ub))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "tvm. op_type: relu, batch_size: 1, length: 2, dim_in: 512, dim_out: 512, average_time (ms): 1.2594483184814453\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1452505/3113498695.py:13: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  x = Bounds(float(p), float(eps), lw, lb, uw, ub).relu()\n",
      "/tmp/ipykernel_1452505/3113498695.py:2: TracerWarning: torch.Tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return tuple((torch.Tensor([[x.p]]), torch.Tensor([[x.eps]]), x.lw, x.lb, x.uw, x.ub))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get devices for measurement successfully!\n",
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |----------------------------------------------------------------------\n",
      "\n",
      "-------------------------------------------------\n",
      "|    0 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 0\tUsed time : 0 s\tNext ID: 0\t\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 2\n",
      "Sample Iter: 5\t#Pop: 10\t#Target: 50\tfail_ct: 10230\tTime elapsed: 3.11\n",
      "#Target has been reduced to 25 due to too many failures or duplications\n",
      "Sample Iter: 10\t#Pop: 10\t#Target: 25\tfail_ct: 20470\tTime elapsed: 6.39\n",
      "#Target has been reduced to 12 due to too many failures or duplications\n",
      "Sample Iter: 15\t#Pop: 10\t#Target: 12\tfail_ct: 30710\tTime elapsed: 10.49\n",
      "#Target has been reduced to 6 due to too many failures or duplications\n",
      "Sample Initial Population\t#s: 10\tfail_ct: 32758\tTime elapsed: 11.25\n",
      "GA Iter: 0\tMax score: 0.9952\tMin score: 0.1492\t#Pop: 10\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 0.9978\tMin score: 0.0170\t#Pop: 70\t#M+: 1391\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 70\tTime elapsed: 1.99\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 64 programs to measure:\n",
      "................................****"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\u001b[0m in \u001b[0;36mrpc_runner_run\u001b[0;34m(inputs, build_results, key, host, port, priority, n_parallel, timeout, number, repeat, min_repeat_ms, cooldown_interval, enable_cpu_cache_flush, verbose)\u001b[0m\n\u001b[1;32m   1296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mStatusKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOMPLETE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMeasureResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/tvm-build/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    609\u001b[0m                     \u001b[0;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/tvm-build/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/tvm-build/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "for op_type in [ 'relu']:\n",
    "    for length in [2,4,8,16,32,64,128]:\n",
    "        for dim_in in [64, 128, 256, 512, 1024]:\n",
    "            dim_out=dim_in # Just an assumption for profiling\n",
    "            dim_y_out = dim_out\n",
    "            lb = torch.rand(1,length,dim_out).to(device)\n",
    "            ub = lb + torch.rand(1,length,dim_out).to(device)\n",
    "            lw = torch.rand(1,length,dim_in,dim_out).to(device) - 0.5\n",
    "            uw = torch.rand(1,length,dim_in,dim_out).to(device) - 0.5\n",
    "\n",
    "            lb1 = torch.rand(1,length,dim_out).to(device)\n",
    "            ub1 = lb1 + torch.rand(1,length,dim_out).to(device)\n",
    "            lw1 = torch.rand(1,length,dim_in,dim_out).to(device) - 0.5\n",
    "            uw1 = torch.rand(1,length,dim_in,dim_out).to(device) - 0.5\n",
    "\n",
    "            w_input = torch.rand(dim_y_out, dim_out).to(device) - 0.5\n",
    "            other_bound = Bounds(p=2,eps=0.5,lw=lw,lb=lb,uw=uw,ub=ub)\n",
    "            \n",
    "            bound = profile_pytorch(p, eps, lw, lb, uw, ub, w_input, other_bound, op_type)\n",
    "            wrapper_outpuet_tuple = profile_nn_wrapper(p, eps, lw, lb, uw, ub, w_input, other_bound, op_type)\n",
    "            tvm_output_tuple =  profile_tvm_baseline(p, eps, lw, lb, uw, ub, w_input, other_bound, op_type)\n",
    "            ansor_output_tuple = profile_ansor_baseline(p, eps, lw, lb, uw, ub, w_input, other_bound, op_type)\n",
    "            print(check_bound_wrapper_diff(bound, wrapper_outpuet_tuple))\n",
    "            print(check_tvm_ansor_baseline_diff(bound, tvm_output_tuple))\n",
    "            print(check_tvm_ansor_baseline_diff(bound, ansor_output_tuple))\n",
    "            print('--------------\\n\\n')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch. op_type: matmul, batch_size: 1, length: 128, dim_in: 64, dim_out: 64, average_time (ms): 0.18743295669555665\n",
      "nn wrapper. op_type: matmul, batch_size: 1, length: 128, dim_in: 64, dim_out: 64, average_time (ms): 0.19392383575439454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1434907/3113498695.py:22: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  x = Bounds(float(p), float(eps), lw, lb, uw, ub).matmul(self.W)\n",
      "/tmp/ipykernel_1434907/3113498695.py:2: TracerWarning: torch.Tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return tuple((torch.Tensor([[x.p]]), torch.Tensor([[x.eps]]), x.lw, x.lb, x.uw, x.ub))\n",
      "One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tvm. op_type: matmul, batch_size: 1, length: 128, dim_in: 64, dim_out: 64, average_time (ms): 3.5975372314453127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1434907/3113498695.py:22: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  x = Bounds(float(p), float(eps), lw, lb, uw, ub).matmul(self.W)\n",
      "/tmp/ipykernel_1434907/3113498695.py:2: TracerWarning: torch.Tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return tuple((torch.Tensor([[x.p]]), torch.Tensor([[x.eps]]), x.lw, x.lb, x.uw, x.ub))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get devices for measurement successfully!\n",
      "|  ID  | Latency (ms) | Speed (GFLOPS) | Trials |----------------------------------------------------------------------\n",
      "------------------------------  [ Task Scheduler ]\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "-------------------------------------------------\n",
      "|    0 |            - |              - |      0 |\n",
      "|    1 |            - |              - |      0 |\n",
      "-------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 0\tUsed time : 0 s\tNext ID: 0\t\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Search ]\n",
      "----------------------------------------------------------------------\n",
      "Generate Sketches\t\t#s: 1\n",
      "Sample Initial Population\t#s: 294\tfail_ct: 1754\tTime elapsed: 0.75\n",
      "GA Iter: 0\tMax score: 0.9982\tMin score: 0.5667\t#Pop: 128\t#M+: 0\t#M-: 0\n",
      "GA Iter: 4\tMax score: 0.9999\tMin score: 0.9811\t#Pop: 128\t#M+: 1400\t#M-: 0\n",
      "EvolutionarySearch\t\t#s: 128\tTime elapsed: 8.12\n",
      "----------------------------------------------------------------------\n",
      "------------------------------  [ Measure ]\n",
      "----------------------------------------------------------------------\n",
      "Get 64 programs to measure:\n",
      "................................******************************"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.anaconda3/envs/tvm-build/lib/python3.8/site-packages/tvm/auto_scheduler/measure.py\u001b[0m in \u001b[0;36mrpc_runner_run\u001b[0;34m(inputs, build_results, key, host, port, priority, n_parallel, timeout, number, repeat, min_repeat_ms, cooldown_interval, enable_cpu_cache_flush, verbose)\u001b[0m\n\u001b[1;32m   1296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mStatusKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOMPLETE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMeasureResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/tvm-build/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    609\u001b[0m                     \u001b[0;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/tvm-build/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/tvm-build/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "for op_type in ['matmul']:\n",
    "    for length in [128]: # [2,4,8,16,32,64,128]:\n",
    "        for dim_in in [64, 128, 256, 512, 1024]: # [64, 128, 256, 512, 1024]:\n",
    "            dim_out=dim_in # Just an assumption for profiling\n",
    "            dim_y_out = dim_out\n",
    "            lb = torch.rand(1,length,dim_out).to(device)\n",
    "            ub = lb + torch.rand(1,length,dim_out).to(device)\n",
    "            lw = torch.rand(1,length,dim_in,dim_out).to(device) - 0.5\n",
    "            uw = torch.rand(1,length,dim_in,dim_out).to(device) - 0.5\n",
    "\n",
    "            lb1 = torch.rand(1,length,dim_out).to(device)\n",
    "            ub1 = lb1 + torch.rand(1,length,dim_out).to(device)\n",
    "            lw1 = torch.rand(1,length,dim_in,dim_out).to(device) - 0.5\n",
    "            uw1 = torch.rand(1,length,dim_in,dim_out).to(device) - 0.5\n",
    "\n",
    "            w_input = torch.rand(dim_y_out, dim_out).to(device) - 0.5\n",
    "            other_bound = Bounds(p=2,eps=0.5,lw=lw,lb=lb,uw=uw,ub=ub)\n",
    "            \n",
    "            bound = profile_pytorch(p, eps, lw, lb, uw, ub, w_input, other_bound, op_type)\n",
    "            wrapper_outpuet_tuple = profile_nn_wrapper(p, eps, lw, lb, uw, ub, w_input, other_bound, op_type)\n",
    "            tvm_output_tuple =  profile_tvm_baseline(p, eps, lw, lb, uw, ub, w_input, other_bound, op_type)\n",
    "            ansor_output_tuple = profile_ansor_baseline(p, eps, lw, lb, uw, ub, w_input, other_bound, op_type)\n",
    "            #  print(type(bound.lw), type(bound_wrapper[0]), type(tvm_output[0]))\n",
    "            print(check_bound_wrapper_diff(bound, wrapper_outpuet_tuple))\n",
    "            print(check_tvm_ansor_baseline_diff(bound, tvm_output_tuple))\n",
    "            print(check_tvm_ansor_baseline_diff(bound, ansor_output_tuple))\n",
    "            print('---')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "for op_type in ['dot_product']:\n",
    "    for length in [2,4,8,16,32,64,128]:\n",
    "        for dim_in in [64, 128, 256, 512, 1024]:\n",
    "            dim_out=dim_in # Just an assumption for profiling\n",
    "            dim_y_out = dim_out\n",
    "            lb = torch.rand(1,length,dim_out).to(device)\n",
    "            ub = lb + torch.rand(1,length,dim_out).to(device)\n",
    "            lw = torch.rand(1,length,dim_in,dim_out).to(device) - 0.5\n",
    "            uw = torch.rand(1,length,dim_in,dim_out).to(device) - 0.5\n",
    "\n",
    "            lb1 = torch.rand(1,length,dim_out).to(device)\n",
    "            ub1 = lb1 + torch.rand(1,length,dim_out).to(device)\n",
    "            lw1 = torch.rand(1,length,dim_in,dim_out).to(device) - 0.5\n",
    "            uw1 = torch.rand(1,length,dim_in,dim_out).to(device) - 0.5\n",
    "\n",
    "            w_input = torch.rand(dim_y_out, dim_out).to(device) - 0.5\n",
    "            other_bound = Bounds(p=2,eps=0.5,lw=lw,lb=lb,uw=uw,ub=ub)\n",
    "            \n",
    "            bound = profile_pytorch(p, eps, lw, lb, uw, ub, w_input, other_bound, op_type)\n",
    "            wrapper_outpuet_tuple = profile_nn_wrapper(p, eps, lw, lb, uw, ub, w_input, other_bound, op_type)\n",
    "            tvm_output_tuple =  profile_tvm_baseline(p, eps, lw, lb, uw, ub, w_input, other_bound, op_type) # May comment out this line\n",
    "            ansor_output_tuple = profile_ansor_baseline(p, eps, lw, lb, uw, ub, w_input, other_bound, op_type)\n",
    "            # print(type(bound.lw), type(bound_wrapper[0]), type(tvm_output[0]))\n",
    "            print(check_bound_wrapper_diff(bound, wrapper_outpuet_tuple))\n",
    "            print(check_tvm_ansor_baseline_diff(bound, tvm_output_tuple)) # May comment out this line\n",
    "            print(check_tvm_ansor_baseline_diff(bound, ansor_output_tuple))\n",
    "            print('---')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "016cbc0d3b18ae82325d7c409cc8f7c98214c79994741e06645a0437a4a425ec"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
